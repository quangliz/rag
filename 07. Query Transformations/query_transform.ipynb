{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query Transformations for Enhanced RAG Systems\n",
        "\n",
        "This notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Transformation Techniques\n",
        "\n",
        "- Query Rewriting: Makes queries more specific and detailed for better search precision.\n",
        "- Step-back Prompting: Generates broader queries to retrieve useful contextual information.\n",
        "- Sub-query Decomposition: Breaks complex queries into simpler components for comprehensive retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYQQYwq3Zhe_",
        "outputId": "54c20055-7340-47c4-a910-714754fa5c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q PymuPDF\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrmNeTslbMzq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForCausalLM\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook uses Llama3.2 3B Instruct generative model and BGE-base-en embedding model, so minimum GPU memory requirement is 8GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "ec0d9429948f4aadbd9e5498c874b74f",
            "7558a80506804481afbb1be287b1a736",
            "e8297b47ce6d4e16bb00e5862c7e67e5",
            "2baf656912be4aa9a33391cc3641dbde",
            "a065d7422e3143ada9254d1b2cb75792",
            "cc463d051a654c18890fe25c28065180",
            "d60a9458c6ee431183b5ee604750debd",
            "1a8b802a06774948ad3f8b5122b25be5",
            "40e789e8cfac41eab232aad05635596f",
            "96ebb61e4901470c8f0701e302258079",
            "c5ab0942310b4ca983482f5f2429ecc0",
            "1ca65c60c53a48099bfb20863c993325",
            "7ba9a060206f4ab79aa3da2061ee2d22",
            "f740b9cba4a54b8ea888d9a021f05d51",
            "918bc4ead2ca4024ad4d26b5c37f648d",
            "9597ce48d447425d9e9f535919517bb1",
            "3030b6494a2f4023951a8d387cac3df0",
            "f2f25e30b27346b0853d06f612b7dde9",
            "9555651af7d14582b791d77595e77ee3",
            "84456d28b2d44419ad13657554311a05",
            "23a908c59c824c92b402ffe2b5b6e18a",
            "64f55fcf544746f6aa0c8b4d77063141",
            "f0c6acdfd2f94bfbadb41604e50b6423",
            "ef43ec30c73a4299bdd4e2e322d71e0b",
            "3ac1e91928684a32bdd51b88601dca1f",
            "26850289f30f4c8f933c1aa6a5f06c8e",
            "5d09cfd32e094b2890108473d4af9621",
            "06826abfda0a4c78aa13af0db159eb4f",
            "b28cdce0f96d486fb97077fc33f84d76",
            "2be5b8ae984f4034bf1544181bbd9f0a",
            "d5bfc62f261544dd87ff4e57a866bb9f",
            "a4c8516c51084e469dce64b84cdee1fd",
            "7e7355c5e6ae458da1ea6f2f6969f9ca",
            "0c0d8ae6eb674d7499df43466f559fec",
            "bb7b5fdd732c4f35b52c9e7b21dd6b61",
            "9898e8056539425189b174ab16a1c831",
            "9362de6eb3b24d8eb6b791797bb0e5c3",
            "0a31140567ac48bf99adc148692dac72",
            "a6d7861fa96a451dbc21a65fc862be61",
            "f6c059a72a924e1896e31a62d859fced",
            "70814157c2f548d9a6110254461b3b5d",
            "fd9d2509c9ad47ebb875176a0b8e0451",
            "5b16551028c04da499ce6ee582c7ea43",
            "ad7080d4587e499ba3af727baa91e99a",
            "cee6a88797094dd0ad0cfa5d08b98c57",
            "51d4bea595b64386b0f1cde4464889c9",
            "8302e45456a24c9ca42395862b2b41d4",
            "608c582dfc2443f1bddd607471ba31f2",
            "b2dbbeb7cd374ef194bb5b59cf700be3",
            "a7581d5c1e7b4a47a0cfa6269ff32d3c",
            "137d832822d04540b4f86e66f82bdc6e",
            "19a30936cff64f6587eb5f3c47ed7208",
            "b6e91ed8593140eeabc5aed8ff6147c3",
            "c6f6d4a2ce684de98f8a3a56ee9f98eb",
            "51c9e81d9fde40f9871a7338fbad0587",
            "875affc2758143a3875dcc2f49984647",
            "65ea7e0fa76e4e9e86e004815c8a3d40",
            "711a5aaea7e64ed2a5e12551a2896922",
            "d2e4c0293ccd416aa0129e88509f7090",
            "c6f3b51799754fbca404791efb4dc604",
            "78d913c4d3734a43971453c7aa44d0dd",
            "62f7681b8e0b405590327be36721f7ba",
            "2d57a0186f8c4e0ea684cd638d29b09b",
            "48bc8310895d4f2bb7053102754b4e6b",
            "e12b0a225f3042a8a2364fa481fecb45",
            "703b3cc5ae714683b3376799af316353",
            "33338311c740456a852fc5151fd3d1fa",
            "c85a815d188749db9b08084ca72e6313",
            "3eef536451d448a2a6c81c9f1ca2cd85",
            "2f203f3bbf69425ebedad46a6a4efbd5",
            "ad44c8ccc0794bc88f06d058d102134f",
            "094b8cc290ee4f0fbfddee175e67f93f",
            "62d8546c1fcd4f34899b6695661fb6f0",
            "6553bd9b4e624cc9bd991c4cad19944c",
            "64d52bc2922f4bfd98b2e7767e328625",
            "282688c5f8fd47aab9368d1f230c9071",
            "e052746890b9471f909867749c5bb172",
            "9c08b795a95245b3a1732226a58b9f51",
            "9c3e1cdab10e4ff1aa4e33e25fd5716b",
            "82d466a791e7491bb265003e113ddac4",
            "dfe9413236e64b2f9faaee1cbfdfaf52",
            "c489e88334384ce5b4823644a68e4547",
            "fb18f99a8289443ba0b07a24ca7dbd91",
            "6965fd2e2a0a4c2691aa8c0aba49b902",
            "ee82a483379d44d7835281f0504e1a63",
            "13fddfcc0cf249bdab1283b93a221289",
            "4f131ea0b1c64a97aec863ff61b22d50",
            "868a2f4bf70143a197257b01834c485a",
            "0720cd4bbd254320b35be56306359bef",
            "140128bb60e4482296c224d697be8062",
            "04ba9abd7c8b407e99b8d951bf0e19a3",
            "ab79315e668c452291097d17406ee315",
            "e99c8cad11b84755bae3a860f21db3d0",
            "e88f42349e0c4b1ea63c7bad2f133fc1",
            "74ff8719aab04f9f83086780e2f6a979",
            "51eb61caa2ad415a87029f4a3818f678",
            "be7f6a9fd58a405191e77be8675bd133",
            "8a6fe7abcd0e439a8428ce06f581a06d",
            "5d918ab5a5fe42229749900adea6094a",
            "547c58041c004749b409fe039920c02e",
            "cd805d9b3f1342f9a65971accebdbab1",
            "c45586ac6efb4d3f9337942b907921f5",
            "ca295b182e984a6282595c614572ca81",
            "0c8b052109974d72882445eb98edafff",
            "971bd88cdaec4e42bbe16054b14523af",
            "60e233ed0ebf4356b62b155d42dea885",
            "eb78018351684fefb0558a9a9aa9fe50",
            "50f95e76574345198abb4dcc8398ab72",
            "83a39dd02b9e414fba47b50a22efc96d",
            "7155f1bc86294ada96c0231295927e3c",
            "3365b076b6834852bb0cb8a12a358fbb",
            "51985c539be441d5a69090ce02a2d264",
            "6bdb327be82f4fc0b24652f0c7cc9fa4",
            "00735bcbe9694b84b08493d5ad442181",
            "ad5be1defc6940c598927a717dc9d04a",
            "6fb549c3ad2b4370816be3f33d08a6ce",
            "5336b8de3f6544a5bb9565baa5f63ad0",
            "e8da8f00480145aca63e2b6bba17f64d",
            "00f3aa321ee74d03ad1f3c13be5249f5",
            "e7a78f55af8b4d71adf6229b72ba01b3",
            "e828415c03e7411895ee987a97f248ef",
            "56a635d4ca1e425d9ed0b016d29f766b",
            "cd220291df724670876fa79d81fab2b8",
            "7ddcd7fe603c404c8dc61944e0d0a189",
            "e6bf14f2253c42b9a0313828062109d9",
            "cfffc2eded6c4e6b897018cff8a27f75",
            "c35f03a0d87b40af884824ffc84a084d",
            "6240a35bba5247bd818c847534b21b7e",
            "e210b1db52c348a1bd7a9b388f722d39",
            "4dd9825b36774b1a9e8ed4d47b8a9bfa",
            "50510104a34249bfa3de9a448930f3c5",
            "85b4e8ec48234e79a9a99191181ddffa",
            "9537eef6a18d43a89dc2b30113439ee7",
            "29d9a31efb924f98a83e98fb3698fbf2",
            "4e0ac4e3c399492b8b5a6ded94d0b1db",
            "86b646a9dc4c47c694ac44f9f83d1d1b",
            "7c647e165aeb40bd909202f87d0b4459",
            "60aada0ddf124d67af148679310b1e3c",
            "858b161c217248e78da310642a29f504",
            "3d0083751d674028bd5ad48f99dec142",
            "2efa3d12943c4ea2af29f66abf7fdd32",
            "94a7b5ef4cf14276abf314e21377a8cd",
            "0ba7a0b2ae094afc99ce5663e19b6ff0"
          ]
        },
        "id": "z5uHPGQubp4j",
        "outputId": "abcb91af-8119-4272-c681-ee166b4274ed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec0d9429948f4aadbd9e5498c874b74f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ca65c60c53a48099bfb20863c993325",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0c6acdfd2f94bfbadb41604e50b6423",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c0d8ae6eb674d7499df43466f559fec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cee6a88797094dd0ad0cfa5d08b98c57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "875affc2758143a3875dcc2f49984647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33338311c740456a852fc5151fd3d1fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c08b795a95245b3a1732226a58b9f51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0720cd4bbd254320b35be56306359bef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "547c58041c004749b409fe039920c02e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3365b076b6834852bb0cb8a12a358fbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56a635d4ca1e425d9ed0b016d29f766b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9537eef6a18d43a89dc2b30113439ee7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = \"cuda\"\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"unsloth/Llama-3.2-3B-Instruct\")\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "embed_model = AutoModel.from_pretrained(\"BAAI/bge-base-en\")\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create gen function to process generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_FqTKb-kObt"
      },
      "outputs": [],
      "source": [
        "def gen(system_prompt, user_prompt): # work with unsloth/Llama-3.2-3B-Instruct\n",
        "    text = gen_tokenizer.apply_chat_template(\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            }\n",
        "        ],\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = False\n",
        "    )\n",
        "\n",
        "    model_inputs = gen_tokenizer([text], return_tensors = \"pt\").to(device)\n",
        "\n",
        "    generated_ids = gen_model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample = True\n",
        "    )\n",
        "\n",
        "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "\n",
        "    response =  gen_tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0].strip(\"assistant\\n\\n\")\n",
        "\n",
        "    # print(\"===========================================\")\n",
        "    # print(f\"resposne: \\n{response}\")\n",
        "    # print(\"===========================================\")\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Query Rewriting\n",
        "\n",
        "This technique makes queries more specific and detailed to improve precision in retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY5Y7OVYcRVP"
      },
      "outputs": [],
      "source": [
        "def rewrite_query(original_query, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
        "\n",
        "    Args:\n",
        "        original_query (str): The original user query\n",
        "        model (str): The model to use for query rewriting\n",
        "\n",
        "    Returns:\n",
        "        str: The rewritten query\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
        "\n",
        "    # Define the user prompt with the original query to be rewritten\n",
        "    user_prompt = f\"\"\"\n",
        "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information. The response MUST BE ONE rewrited text, no more additional text.\n",
        "\n",
        "    Original query: {original_query}\n",
        "\n",
        "    Rewritten query:\n",
        "    \"\"\"\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "SEHtLsCpfzvQ",
        "outputId": "f2576a95-1dc8-4bbc-97b7-c54b26ec1077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are the specific job displacement rates and economic implications of AI-driven automation on various industries and sectors, particularly in the manufacturing, transportation, and service sectors, and how do these changes impact employment opportunities, skill sets, and workforce development strategies in the United States and Europe, with a focus on the effects on low-skilled, low-wage, and gig economy workers, and what are the potential benefits and drawbacks of universal basic income, retraining programs, and social safety nets in mitigating the negative consequences of AI-driven job automation?'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rewrite_query(\"What are the impacts of AI on job automation and employment?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Step back query transformation\n",
        "\n",
        "This technique generates broader queries to retrieve contextual background information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfuPEXVLeEqr"
      },
      "outputs": [],
      "source": [
        "def generate_step_back_query(original_query, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Generates a more general 'step-back' query to retrieve broader context.\n",
        "\n",
        "    Args:\n",
        "        original_query (str): The original user query\n",
        "        model (str): The model to use for step-back query generation\n",
        "\n",
        "    Returns:\n",
        "        str: The step-back query\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
        "\n",
        "    # Define the user prompt with the original query to be generalized\n",
        "    user_prompt = f\"\"\"\n",
        "    Generate a broader, more general version of the following query that could help retrieve useful background information. DO NOT include additional unrelated text\n",
        "\n",
        "    Original query: {original_query}\n",
        "\n",
        "    Step-back query:\n",
        "    \"\"\"\n",
        "\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-J5784j9hvfb",
        "outputId": "79e1ca72-5e7d-4e28-a2e4-19b428759e8f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are the effects of technological advancements on workforce displacement and labor market dynamics?'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_step_back_query(\"What are the impacts of AI on job automation and employment?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Sub-query Composition\n",
        "\n",
        "This technique breaks down complex queries into simpler components for comprehensive retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "FS7YFy1keWv5"
      },
      "outputs": [],
      "source": [
        "def decompose_query(original_query, num_subqueries=4, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Decomposes a complex query into simpler sub-queries.\n",
        "\n",
        "    Args:\n",
        "        original_query (str): The original complex query\n",
        "        num_subqueries (int): Number of sub-queries to generate\n",
        "        model (str): The model to use for query decomposition\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of simpler sub-queries\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
        "\n",
        "    # Define the user prompt with the original query to be decomposed\n",
        "    user_prompt = f\"\"\"\n",
        "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
        "\n",
        "    Original query: {original_query}\n",
        "\n",
        "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
        "    1. [First sub-query]\n",
        "    2. [Second sub-query]\n",
        "    And so on...\n",
        "    \"\"\"\n",
        "\n",
        "    content = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Extract numbered queries using simple parsing\n",
        "    lines = content.split(\"\\n\")\n",
        "    sub_queries = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
        "            # Remove the number and leading space\n",
        "            query = line.strip()\n",
        "            query = query[query.find(\".\")+1:].strip()\n",
        "            sub_queries.append(query)\n",
        "\n",
        "    return sub_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSQlZ36yicHt",
        "outputId": "2b5a1fb3-e806-4fb6-b8c8-ae5b1011e22a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What are the primary job roles that are at risk of being automated by AI?',\n",
              " 'How do AI-driven automation changes affect the job market and the demand for new skills?',\n",
              " 'What are the potential economic and societal impacts of widespread AI-driven job automation on employment?',\n",
              " 'How can workers and industries adapt to the changing job market and mitigate the negative effects of AI-driven automation on employment?']"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decompose_query('What are the impacts of AI on job automation and employment?', num_subqueries=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWec1_cmegW4",
        "outputId": "5aa3c72e-1810-48f2-da30-66e8ae4a19d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Query: What are the impacts of AI on job automation and employment?\n",
            "\n",
            "1. Rewritten Query:\n",
            "What are the specific job displacement and creation impacts of artificial intelligence (AI) on employment, including the effects on low-skilled and high-skilled workers, and the resulting need for upskilling and reskilling programs, as well as the potential benefits of AI-driven entrepreneurship and the emergence of new industries and job roles in fields such as data science, machine learning, and cybersecurity?\n",
            "\n",
            "2. Step-back Query:\n",
            "What are the effects of automation and artificial intelligence on the modern workforce and labor market?\n",
            "\n",
            "3. Sub-queries:\n",
            "   1. What are the primary job roles that are most susceptible to automation by AI?\n",
            "   2. How does AI-driven automation affect the types of jobs that are created in new industries and sectors?\n",
            "   3. What are the potential benefits and drawbacks of AI-driven job displacement for workers who lose their jobs to automation?\n",
            "   4. How do governments and organizations plan to address and mitigate the social and economic impacts of AI-driven job automation on employment?\n"
          ]
        }
      ],
      "source": [
        "# Example query\n",
        "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
        "\n",
        "# Apply query transformations\n",
        "print(\"Original Query:\", original_query)\n",
        "\n",
        "# Query Rewriting\n",
        "rewritten_query = rewrite_query(original_query)\n",
        "print(\"\\n1. Rewritten Query:\")\n",
        "print(rewritten_query)\n",
        "\n",
        "# Step-back Prompting\n",
        "step_back_query = generate_step_back_query(original_query)\n",
        "print(\"\\n2. Step-back Query:\")\n",
        "print(step_back_query)\n",
        "\n",
        "# Sub-query Decomposition\n",
        "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
        "print(\"\\n3. Sub-queries:\")\n",
        "for i, query in enumerate(sub_queries, 1):\n",
        "    print(f\"   {i}. {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Building Simple Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8VQ5Re2ejm_"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Convert query embedding to numpy array\n",
        "        query_vector = query_embedding\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Compute cosine similarity between query vector and stored vector\n",
        "            similarity = np.dot(query_vector, vector.squeeze()) / (np.linalg.norm(query_vector) * np.linalg.norm(vector.squeeze()))\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the corresponding text\n",
        "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k similar items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create emebedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYVftw3ai32n"
      },
      "outputs": [],
      "source": [
        "embed_model.to(device)\n",
        "def embed(text):\n",
        "    if isinstance(text, str): text = [text] # if single string => convert into a list of one element\n",
        "    inputs = embed_tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device) # tokenize input\n",
        "    with torch.no_grad():\n",
        "        output = embed_model(**inputs) # running model\n",
        "        embedding = F.normalize(output.last_hidden_state[:, 0, :], p=2, dim=1) # normalize vector to L2\n",
        "    return embedding.cpu().numpy() # pass to cpu with numpy array (n, dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfOoSLG5jG9H"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    mypdf = fitz.open(pdf_path)\n",
        "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page in mypdf:\n",
        "        all_text += page.get_text(\"text\")\n",
        "    return all_text  # Return the extracted text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5xFFBd2jW9y"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, n=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "mpjF2N7qjcsh"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for RAG.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "    chunk_size (int): Size of each chunk in characters.\n",
        "    chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
        "    \"\"\"\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    # Create embeddings for all chunks at once for efficiency\n",
        "    chunk_embeddings = embed(chunks)\n",
        "\n",
        "    # Create vector store\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add chunks to vector store\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "    return store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn3nCGOtjfJA"
      },
      "outputs": [],
      "source": [
        "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
        "    \"\"\"\n",
        "    Search using a transformed query.\n",
        "\n",
        "    Args:\n",
        "        query (str): Original query\n",
        "        vector_store (SimpleVectorStore): Vector store to search\n",
        "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
        "        top_k (int): Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Search results\n",
        "    \"\"\"\n",
        "    print(f\"Transformation type: {transformation_type}\")\n",
        "    print(f\"Original query: {query}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    if transformation_type == \"rewrite\":\n",
        "        # Query rewriting\n",
        "        transformed_query = rewrite_query(query)\n",
        "        print(f\"Rewritten query: {transformed_query}\")\n",
        "\n",
        "        # Create embedding for transformed query\n",
        "        query_embedding = embed(transformed_query)\n",
        "\n",
        "        # Search with rewritten query\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "\n",
        "    elif transformation_type == \"step_back\":\n",
        "        # Step-back prompting\n",
        "        transformed_query = generate_step_back_query(query)\n",
        "        print(f\"Step-back query: {transformed_query}\")\n",
        "\n",
        "        # Create embedding for transformed query\n",
        "        query_embedding = embed(transformed_query)\n",
        "\n",
        "        # Search with step-back query\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "\n",
        "    elif transformation_type == \"decompose\":\n",
        "        # Sub-query decomposition\n",
        "        sub_queries = decompose_query(query)\n",
        "        print(\"Decomposed into sub-queries:\")\n",
        "        for i, sub_q in enumerate(sub_queries, 1):\n",
        "            print(f\"{i}. {sub_q}\")\n",
        "\n",
        "        # Create embeddings for all sub-queries\n",
        "        sub_query_embeddings = embed(sub_queries)\n",
        "\n",
        "        # Search with each sub-query and combine results\n",
        "        all_results = []\n",
        "        for i, embedding in enumerate(sub_query_embeddings):\n",
        "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
        "            all_results.extend(sub_results)\n",
        "\n",
        "        # Remove duplicates (keep highest similarity score)\n",
        "        seen_texts = {}\n",
        "        for result in all_results:\n",
        "            text = result[\"text\"]\n",
        "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
        "                seen_texts[text] = result\n",
        "\n",
        "        # Sort by similarity and take top_k\n",
        "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
        "\n",
        "    else:\n",
        "        # Regular search without transformation\n",
        "        query_embedding = embed(query)\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "FTcm79e0js3-"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Generates a response based on the query and retrieved context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Retrieved context\n",
        "        model (str): The model to use for response generation\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
        "\n",
        "    # Define the user prompt with the context and query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "    # Return the generated response, stripping any leading/trailing whitespace\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Completed pipeline for query transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7FiF5spUkBHR"
      },
      "outputs": [],
      "source": [
        "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
        "    \"\"\"\n",
        "    Run complete RAG pipeline with optional query transformation.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): User query\n",
        "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, transformed query, context, and response\n",
        "    \"\"\"\n",
        "    # Process the document to create a vector store\n",
        "    vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Apply query transformation and search\n",
        "    if transformation_type:\n",
        "        # Perform search with transformed query\n",
        "        results = transformed_search(query, vector_store, transformation_type)\n",
        "    else:\n",
        "        # Perform regular search without transformation\n",
        "        query_embedding = embed(query)\n",
        "        results = vector_store.similarity_search(query_embedding, k=3)\n",
        "\n",
        "    # Combine context from search results\n",
        "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
        "\n",
        "    # Generate response based on the query and combined context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Return the results including original query, transformation type, context, and response\n",
        "    return {\n",
        "        \"original_query\": query,\n",
        "        \"transformation_type\": transformation_type,\n",
        "        \"context\": context,\n",
        "        \"response\": response\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "5f9KjewJkG96"
      },
      "outputs": [],
      "source": [
        "def compare_responses(results, reference_answer, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Compare responses from different query transformation techniques.\n",
        "\n",
        "    Args:\n",
        "        results (Dict): Results from different transformation techniques\n",
        "        reference_answer (str): Reference answer for comparison\n",
        "        model (str): Model for evaluation\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI assistant's behavior\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
        "    Your task is to compare different responses generated using various query transformation techniques\n",
        "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
        "\n",
        "    # Prepare the comparison text with the reference answer and responses from each technique\n",
        "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
        "\n",
        "    for technique, result in results.items():\n",
        "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
        "\n",
        "    # Define the user prompt with the comparison text\n",
        "    user_prompt = f\"\"\"\n",
        "    {comparison_text}\n",
        "\n",
        "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
        "\n",
        "    For each technique (original, rewrite, step_back, decompose):\n",
        "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
        "    2. Identify strengths and weaknesses\n",
        "\n",
        "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the evaluation response using the specified model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
        "    print(response)\n",
        "    print(\"=============================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Yur526eGkKpe"
      },
      "outputs": [],
      "source": [
        "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluate different transformation techniques for the same query.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF document\n",
        "        query (str): Query to evaluate\n",
        "        reference_answer (str): Optional reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    # Define the transformation techniques to evaluate\n",
        "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
        "    results = {}\n",
        "\n",
        "    # Run RAG with each transformation technique\n",
        "    for transformation_type in transformation_types:\n",
        "        type_name = transformation_type if transformation_type else \"original\"\n",
        "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
        "\n",
        "        # Get the result for the current transformation type\n",
        "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
        "        results[type_name] = result\n",
        "\n",
        "        # Print the response for the current transformation type\n",
        "        print(f\"Response with {type_name} query:\")\n",
        "        print(result[\"response\"])\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    # Compare results if a reference answer is provided\n",
        "    if reference_answer:\n",
        "        compare_responses(results, reference_answer)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWlEz0DBl7Eg",
        "outputId": "0f2387c5-07a3-42d6-f00a-67f7f11187e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running RAG with original query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Response with original query:\n",
            "Based on the provided context, a 'cobot' refers to a robot that works alongside humans in collaborative settings. This term is mentioned in Passage 1, where it is stated that AI-powered robots can work alongside humans in \"collaborative settings (cobots)\".\n",
            "==================================================\n",
            "\n",
            "===== Running RAG with rewrite query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Transformation type: rewrite\n",
            "Original query: What is a 'cobot'?\n",
            "===========================================\n",
            "resposne: \n",
            "What are the differences between a cobot and a robot, and how do collaborative robots, also known as cobots, differ from industrial robots in terms of their design, functionality, and applications in manufacturing and logistics, and what are some examples of cobot models and industries where they are commonly used?\n",
            "===========================================\n",
            "Rewritten query: What are the differences between a cobot and a robot, and how do collaborative robots, also known as cobots, differ from industrial robots in terms of their design, functionality, and applications in manufacturing and logistics, and what are some examples of cobot models and industries where they are commonly used?\n",
            "Response with rewrite query:\n",
            "A 'cobot' is a robot that collaborates with humans in shared workspaces. According to the context, cobots are enabled by AI, which allows them to work alongside humans in collaborative settings.\n",
            "==================================================\n",
            "\n",
            "===== Running RAG with step_back query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Transformation type: step_back\n",
            "Original query: What is a 'cobot'?\n",
            "Step-back query: What is a type of robot designed for collaborative work?\n",
            "Response with step_back query:\n",
            "Based on the provided context, a 'cobot' refers to a robot that collaborates with humans in settings where they work together alongside each other. This concept is mentioned in both Passages 1 and 2, specifically in the context of industrial robots and service robots. In these passages, AI enhances the ability of robots to work alongside humans in these collaborative settings, allowing them to perform tasks more effectively and efficiently.\n",
            "==================================================\n",
            "\n",
            "===== Running RAG with decompose query =====\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "Transformation type: decompose\n",
            "Original query: What is a 'cobot'?\n",
            "Decomposed into sub-queries:\n",
            "1. What does 'cobot' stand for in the context of robotics and technology?\n",
            "2. What is the primary function or purpose of a cobot in an industrial setting?\n",
            "3. How does a cobot differ from a human worker in terms of its capabilities and limitations?\n",
            "4. What are the benefits and applications of using cobots in various industries, such as manufacturing and healthcare?\n",
            "Response with decompose query:\n",
            "Based on the provided context, a 'cobot' is short for 'cobotic robot' or 'collaborative robot'. According to Passage 1, AI enhances the precision, efficiency, and adaptability of industrial robots, allowing them to work alongside humans in collaborative settings (cobots). This implies that cobots are robots designed to work alongside humans, often in manufacturing environments, and can be enhanced by AI to perform tasks alongside humans.\n",
            "==================================================\n",
            "\n",
            "===== EVALUATION RESULTS =====\n",
            "**Comparison of Query Transformation Techniques**\n",
            "\n",
            "### Original Response\n",
            "\n",
            "* Score: 7/10\n",
            "* Strengths:\n",
            "\t+ Accurately defines 'cobot' as a robot working alongside humans\n",
            "\t+ Mentions the context of collaborative settings\n",
            "* Weaknesses:\n",
            "\t+ Lacks specific details about AI and industrial robots\n",
            "\t+ Does not provide a clear connection between 'cobot' and AI\n",
            "\n",
            "### Rewrite Response\n",
            "\n",
            "* Score: 8/10\n",
            "* Strengths:\n",
            "\t+ Clearly states that 'cobot' refers to a robot that collaborates with humans\n",
            "\t+ Provides a concise and straightforward explanation\n",
            "* Weaknesses:\n",
            "\t+ Does not explicitly mention AI\n",
            "\t+ May not be as informative as the step_back response\n",
            "\n",
            "### Step_back Response\n",
            "\n",
            "* Score: 9/10\n",
            "* Strengths:\n",
            "\t+ Provides a more detailed explanation of the concept of 'cobot'\n",
            "\t+ Mentions the context of industrial robots and service robots\n",
            "\t+ Clearly connects 'cobot' to AI\n",
            "* Weaknesses:\n",
            "\t+ May be too lengthy for some contexts\n",
            "\t+ Uses more complex sentence structure\n",
            "\n",
            "### Decompose Response\n",
            "\n",
            "* Score: 9.5/10\n",
            "* Strengths:\n",
            "\t+ Provides a clear and concise definition of 'cobot'\n",
            "\t+ Effectively connects 'cobot' to AI and industrial robots\n",
            "\t+ Offers additional context about the capabilities of cobots\n",
            "* Weaknesses:\n",
            "\t+ May be more challenging to understand for non-experts\n",
            "\n",
            "**Ranking and Explanation**\n",
            "\n",
            "Based on the scores, the decompose response performs the best overall, with a score of 9.5/10. This is because it provides a clear and concise definition of 'cobot', effectively connects it to AI and industrial robots, and offers additional context about the capabilities of cobots. The decompose response also demonstrates a good balance between accuracy, completeness, and relevance.\n",
            "\n",
            "The step_back response is a close second, with a score of 9/10. While it provides a more detailed explanation of the concept of 'cobot', it may be too lengthy for some contexts.\n",
            "\n",
            "The original response scores 7/10, as it accurately defines 'cobot' but lacks specific details about AI and industrial robots.\n",
            "\n",
            "The rewrite response scores 8/10, as it provides a clear and concise explanation but does not explicitly mention AI.\n",
            "\n",
            "In conclusion, the decompose response is the most effective query transformation technique, as it provides a clear and concise definition of 'cobot' while effectively connecting it to AI and industrial robots.\n",
            "=============================\n"
          ]
        }
      ],
      "source": [
        "# Load the validation data from a JSON file\n",
        "with open('val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[7]['question']\n",
        "\n",
        "# Extract the reference answer from the validation data\n",
        "reference_answer = data[7]['ideal_answer']\n",
        "\n",
        "# pdf_path\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
    
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
