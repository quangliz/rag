{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3dd05302d5844e3d8c25d135b439ee95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fcd7a35a5d642a7ad46c4904d1499de",
              "IPY_MODEL_181631d1b4794d9ebde38c96543fccb8",
              "IPY_MODEL_c0cd53a7d48b4129bad9dedefc1a69a9"
            ],
            "layout": "IPY_MODEL_22ff353e242c43d798518bd004aeeed3"
          }
        },
        "5fcd7a35a5d642a7ad46c4904d1499de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f17513e3f0e47e9acff0109ca5ed590",
            "placeholder": "​",
            "style": "IPY_MODEL_ed8b4150360a4773b6185c97db914e2b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "181631d1b4794d9ebde38c96543fccb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f90120fcc57e43e09679f102d6b3ab73",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_756e2453d50e4bf6abae8b154024edf1",
            "value": 2
          }
        },
        "c0cd53a7d48b4129bad9dedefc1a69a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0ef5ee031a44f5da6fa16160b2967b0",
            "placeholder": "​",
            "style": "IPY_MODEL_c33e24dbe55d46b990cbf8c5acfb6aaa",
            "value": " 2/2 [00:02&lt;00:00,  1.03s/it]"
          }
        },
        "22ff353e242c43d798518bd004aeeed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f17513e3f0e47e9acff0109ca5ed590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed8b4150360a4773b6185c97db914e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f90120fcc57e43e09679f102d6b3ab73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756e2453d50e4bf6abae8b154024edf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0ef5ee031a44f5da6fa16160b2967b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c33e24dbe55d46b990cbf8c5acfb6aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Self-RAG: A Dynamic Approach to RAG\n",
        "\n",
        "In this notebook, I implement Self-RAG, an advanced RAG system that dynamically decides when and how to use retrieved information. Unlike traditional RAG approaches, Self-RAG introduces reflection points throughout the retrieval and generation process, resulting in higher quality and more reliable responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "4LflzTGiRmG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Key Components of Self-RAG:\n",
        "\n",
        "\n",
        "- Retrieval Decision: Determines if retrieval is even necessary for a given query\n",
        "- Document Retrieval: Fetches potentially relevant documents when needed\n",
        "- Relevance Evaluation: Assesses how relevant each retrieved document is\n",
        "- Response Generation: Creates responses based on relevant contexts\n",
        "- Support Assessment: Evaluates if responses are properly grounded in the context\n",
        "- Utility Evaluation: Rates the overall usefulness of generated responses\n"
      ],
      "metadata": {
        "id": "RrTbF_9tRro7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymupdf"
      ],
      "metadata": {
        "id": "RSJwUbilYocP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nKhw4hyVvaYX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import fitz\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    torch_dtype = \"auto\",\n",
        "    device_map = \"auto\"\n",
        ")\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
        "\n",
        "embed_model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\").to(device)\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "3dd05302d5844e3d8c25d135b439ee95",
            "5fcd7a35a5d642a7ad46c4904d1499de",
            "181631d1b4794d9ebde38c96543fccb8",
            "c0cd53a7d48b4129bad9dedefc1a69a9",
            "22ff353e242c43d798518bd004aeeed3",
            "0f17513e3f0e47e9acff0109ca5ed590",
            "ed8b4150360a4773b6185c97db914e2b",
            "f90120fcc57e43e09679f102d6b3ab73",
            "756e2453d50e4bf6abae8b154024edf1",
            "f0ef5ee031a44f5da6fa16160b2967b0",
            "c33e24dbe55d46b990cbf8c5acfb6aaa"
          ]
        },
        "id": "L1soXGbPSLEJ",
        "outputId": "770ba37f-22e9-459f-f17e-e3cbcf0170d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd05302d5844e3d8c25d135b439ee95"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(text):\n",
        "  is_single = isinstance(text, str)\n",
        "  if is_single: text = [text]\n",
        "\n",
        "  try:\n",
        "    inputs = embed_tokenizer(\n",
        "        text,\n",
        "        padding = True,\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(device)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None\n",
        "\n",
        "  try:\n",
        "    with torch.no_grad():\n",
        "      outputs = embed_model(**inputs)\n",
        "      cls = outputs.last_hidden_state[:, 0, :]\n",
        "      embed_normalized = F.normalize(cls, p = 2, dim = 1)\n",
        "    embeddings = [embed.cpu().numpy() for embed in embed_normalized]\n",
        "\n",
        "    return embeddings\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "H4Yk88svSTae"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(system_prompt, user_prompt, temperature=0):\n",
        "    text = gen_tokenizer.apply_chat_template(\n",
        "        conversation=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = gen_tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    if temperature > 0:\n",
        "        generated_ids = gen_model.generate(\n",
        "            **model_inputs,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            max_new_tokens=1024\n",
        "        )\n",
        "    else:\n",
        "        generated_ids = gen_model.generate(\n",
        "            **model_inputs,\n",
        "            do_sample=False,\n",
        "            temperature= None,\n",
        "            top_p=None,  # disable top_p and temperature to not receive warning\n",
        "            max_new_tokens=1024\n",
        "        )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = gen_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response"
      ],
      "metadata": {
        "id": "uXyVjfESSVmu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define vectore store"
      ],
      "metadata": {
        "id": "k9F4tecKSe6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \"\"\"\n",
        "        self.vectors = []  # List to store embedding vectors\n",
        "        self.texts = []  # List to store original texts\n",
        "        self.metadata = []  # List to store metadata for each text\n",
        "\n",
        "    def add_item(self, text, embedding, metadata=None):\n",
        "        \"\"\"\n",
        "        Add an item to the vector store.\n",
        "\n",
        "        Args:\n",
        "        text (str): The original text.\n",
        "        embedding (List[float]): The embedding vector.\n",
        "        metadata (dict, optional): Additional metadata.\n",
        "        \"\"\"\n",
        "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
        "        self.texts.append(text)  # Add the original text to texts list\n",
        "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
        "\n",
        "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
        "        \"\"\"\n",
        "        Find the most similar items to a query embedding.\n",
        "\n",
        "        Args:\n",
        "        query_embedding (List[float]): Query embedding vector.\n",
        "        k (int): Number of results to return.\n",
        "        filter_func (callable, optional): Function to filter results.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict]: Top k most similar items with their texts and metadata.\n",
        "        \"\"\"\n",
        "        if not self.vectors:\n",
        "            return []  # Return empty list if no vectors are stored\n",
        "\n",
        "        # Calculate similarities using cosine similarity\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            # Apply filter if provided\n",
        "            if filter_func and not filter_func(self.metadata[i]):\n",
        "                continue\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity(query_embedding.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
        "            similarities.append((i, similarity))  # Append index and similarity score\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top k results\n",
        "        results = []\n",
        "        for i in range(min(k, len(similarities))):\n",
        "            idx, score = similarities[i]\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],  # Add the text\n",
        "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
        "                \"similarity\": score  # Add the similarity score\n",
        "            })\n",
        "\n",
        "        return results  # Return the list of top k results\n"
      ],
      "metadata": {
        "id": "yS8FVaaZSZJg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Process data"
      ],
      "metadata": {
        "id": "dFtk5u-RSllJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in pdf:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "def chunk_text(text, chunk_size, chunk_overlap):\n",
        "  return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-chunk_overlap)]"
      ],
      "metadata": {
        "id": "Cgq1okh0SoJQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Process a document for Self-RAG.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "        chunk_size (int): Size of each chunk in characters.\n",
        "        chunk_overlap (int): Overlap between chunks in characters.\n",
        "\n",
        "    Returns:\n",
        "        SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF file\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunk the extracted text\n",
        "    print(\"Chunking text...\")\n",
        "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Create embeddings for each chunk\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    chunk_embeddings = create_embeddings(chunks)\n",
        "\n",
        "    # Initialize the vector store\n",
        "    store = SimpleVectorStore()\n",
        "\n",
        "    # Add each chunk and its embedding to the vector store\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "        store.add_item(\n",
        "            text=chunk,\n",
        "            embedding=embedding,\n",
        "            metadata={\"index\": i, \"source\": pdf_path}\n",
        "        )\n",
        "\n",
        "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
        "    return store\n"
      ],
      "metadata": {
        "id": "9zeuZ3CSSi1_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_if_retrieval_needed(query):\n",
        "    \"\"\"\n",
        "    Determines if retrieval is necessary for the given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "\n",
        "    Returns:\n",
        "        bool: True if retrieval is needed, False otherwise\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to determine if retrieval is necessary\n",
        "    system_prompt = \"\"\"You are an AI assistant that determines if retrieval is necessary to answer a query.\n",
        "    For factual questions, specific information requests, or questions about events, people, or concepts, answer \"Yes\".\n",
        "    For opinions, hypothetical scenarios, or simple queries with common knowledge, answer \"No\".\n",
        "    Answer with ONLY \"Yes\" or \"No\".\"\"\"\n",
        "\n",
        "    # User prompt containing the query\n",
        "    user_prompt = f\"Query: {query}\\n\\nIs retrieval necessary to answer this query accurately?\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Extract the answer from the model's response and convert to lowercase\n",
        "    answer = response.lower()\n",
        "    # Return True if the answer contains \"yes\", otherwise return False\n",
        "    return \"yes\" in answer\n"
      ],
      "metadata": {
        "id": "yzH5OClKSz-f"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_relevance(query, context):\n",
        "    \"\"\"\n",
        "    Evaluates the relevance of a context to the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context text\n",
        "\n",
        "    Returns:\n",
        "        str: 'relevant' or 'irrelevant'\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to determine document relevance\n",
        "    system_prompt = \"\"\"You are an AI assistant that determines if a document is relevant to a query.\n",
        "    Consider whether the document contains information that would be helpful in answering the query.\n",
        "    Answer with ONLY \"Relevant\" or \"Irrelevant\".\"\"\"\n",
        "\n",
        "    # Truncate context if it is too long to avoid exceeding token limits\n",
        "    # max_context_length = 2000\n",
        "    # if len(context) > max_context_length:\n",
        "    #     context = context[:max_context_length] + \"... [truncated]\"\n",
        "\n",
        "    # User prompt containing the query and the document content\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "    Document content:\n",
        "    {context}\n",
        "\n",
        "    Is this document relevant to the query? Answer with ONLY exactly word \"Relevant\" or \"Irrelevant\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Extract the answer from the model's response and convert to lowercase\n",
        "    answer = response.lower()\n",
        "    return answer  # Return the relevance evaluation\n"
      ],
      "metadata": {
        "id": "3Yi4uKz1T9b-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_support(response, context):\n",
        "    \"\"\"\n",
        "    Assesses how well a response is supported by the context.\n",
        "\n",
        "    Args:\n",
        "        response (str): Generated response\n",
        "        context (str): Context text\n",
        "\n",
        "    Returns:\n",
        "        str: 'fully supported', 'partially supported', or 'no support'\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to evaluate support\n",
        "    system_prompt = \"\"\"You are an AI assistant that determines if a response is supported by the given context.\n",
        "    Evaluate if the facts, claims, and information in the response are backed by the context.\n",
        "    Answer with ONLY one of these three options:\n",
        "    - \"Fully supported\": All information in the response is directly supported by the context.\n",
        "    - \"Partially supported\": Some information in the response is supported by the context, but some is not.\n",
        "    - \"No support\": The response contains significant information not found in or contradicting the context.\n",
        "    \"\"\"\n",
        "\n",
        "    # Truncate context if it is too long to avoid exceeding token limits\n",
        "    max_context_length = 2000\n",
        "    if len(context) > max_context_length:\n",
        "        context = context[:max_context_length] + \"... [truncated]\"\n",
        "\n",
        "    # User prompt containing the context and the response to be evaluated\n",
        "    user_prompt = f\"\"\"Context:\n",
        "    {context}\n",
        "\n",
        "    Response:\n",
        "    {response}\n",
        "\n",
        "    How well is this response supported by the context? Answer with ONLY \"Fully supported\", \"Partially supported\", or \"No support\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Extract the answer from the model's response and convert to lowercase\n",
        "    answer = response.lower()\n",
        "    return answer  # Return the support assessment\n"
      ],
      "metadata": {
        "id": "TXFHuKCWUdrc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_utility(query, response):\n",
        "    \"\"\"\n",
        "    Rates the utility of a response for the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        response (str): Generated response\n",
        "\n",
        "    Returns:\n",
        "        int: Utility rating from 1 to 5\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to rate the utility of the response\n",
        "    system_prompt = \"\"\"You are an AI assistant that rates the utility of a response to a query.\n",
        "    Consider how well the response answers the query, its completeness, correctness, and helpfulness.\n",
        "    Rate the utility on a scale from 1 to 5, where:\n",
        "    - 1: Not useful at all\n",
        "    - 2: Slightly useful\n",
        "    - 3: Moderately useful\n",
        "    - 4: Very useful\n",
        "    - 5: Exceptionally useful\n",
        "    Answer with ONLY a single number from 1 to 5.\"\"\"\n",
        "\n",
        "    # User prompt containing the query and the response to be rated\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "    Response:\n",
        "    {response}\n",
        "\n",
        "    Rate the utility of this response on a scale from 1 to 5:\"\"\"\n",
        "\n",
        "    # Generate the utility rating using the OpenAI client\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Extract the rating from the model's response\n",
        "    rating = response\n",
        "    # Extract just the number from the rating\n",
        "    rating_match = re.search(r'[1-5]', rating)\n",
        "    if rating_match:\n",
        "        return int(rating_match.group())  # Return the extracted rating as an integer\n",
        "\n",
        "    return 3  # Default to middle rating if parsing fails\n"
      ],
      "metadata": {
        "id": "5IbxqpEyVTou"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query, context=None):\n",
        "    \"\"\"\n",
        "    Generates a response based on the query and optional context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str, optional): Context text\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # System prompt to instruct the AI on how to generate a helpful response\n",
        "    system_prompt = \"\"\"You are a helpful AI assistant. Provide a clear, accurate, and informative response to the query.\"\"\"\n",
        "\n",
        "    # Create the user prompt based on whether context is provided\n",
        "    if context:\n",
        "        user_prompt = f\"\"\"Context:\n",
        "        {context}\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Please answer the query based on the provided context.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Please answer the query to the best of your ability.\"\"\"\n",
        "\n",
        "    # Generate the response using the OpenAI client\n",
        "    response = gen(system_prompt, user_prompt, 0.2)\n",
        "\n",
        "    # Return the generated response text\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "iHlsyIc6Vjlg"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_rag(query, vector_store, top_k=3):\n",
        "    \"\"\"\n",
        "    Implements the complete Self-RAG pipeline.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store containing document chunks\n",
        "        top_k (int): Number of documents to retrieve initially\n",
        "\n",
        "    Returns:\n",
        "        dict: Results including query, response, and metrics from the Self-RAG process\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Starting Self-RAG for query: {query} ===\\n\")\n",
        "\n",
        "    # Step 1: Determine if retrieval is necessary\n",
        "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
        "    retrieval_needed = determine_if_retrieval_needed(query)\n",
        "    print(f\"Retrieval needed: {retrieval_needed}\")\n",
        "\n",
        "    # Initialize metrics to track the Self-RAG process\n",
        "    metrics = {\n",
        "        \"retrieval_needed\": retrieval_needed,\n",
        "        \"documents_retrieved\": 0,\n",
        "        \"relevant_documents\": 0,\n",
        "        \"response_support_ratings\": [],\n",
        "        \"utility_ratings\": []\n",
        "    }\n",
        "\n",
        "    best_response = None\n",
        "    best_score = -1\n",
        "\n",
        "    if retrieval_needed:\n",
        "        # Step 2: Retrieve documents\n",
        "        print(\"\\nStep 2: Retrieving relevant documents...\")\n",
        "        query_embedding = create_embeddings(query)[0]\n",
        "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
        "        metrics[\"documents_retrieved\"] = len(results)\n",
        "        print(f\"Retrieved {len(results)} documents\")\n",
        "\n",
        "        # Step 3: Evaluate relevance of each document\n",
        "        print(\"\\nStep 3: Evaluating document relevance...\")\n",
        "        relevant_contexts = []\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            context = result[\"text\"]\n",
        "            relevance = evaluate_relevance(query, context)\n",
        "            print(f\"Document {i+1} relevance: {relevance}\")\n",
        "\n",
        "            if relevance == \"relevant\":\n",
        "                relevant_contexts.append(context)\n",
        "\n",
        "        metrics[\"relevant_documents\"] = len(relevant_contexts)\n",
        "        print(f\"Found {len(relevant_contexts)} relevant documents\")\n",
        "\n",
        "        if relevant_contexts:\n",
        "            # Step 4: Process each relevant context\n",
        "            print(\"\\nStep 4: Processing relevant contexts...\")\n",
        "            for i, context in enumerate(relevant_contexts):\n",
        "                print(f\"\\nProcessing context {i+1}/{len(relevant_contexts)}...\")\n",
        "\n",
        "                # Generate response based on the context\n",
        "                print(\"Generating response...\")\n",
        "                response = generate_response(query, context)\n",
        "\n",
        "                # Assess how well the response is supported by the context\n",
        "                print(\"Assessing support...\")\n",
        "                support_rating = assess_support(response, context)\n",
        "                print(f\"Support rating: {support_rating}\")\n",
        "                metrics[\"response_support_ratings\"].append(support_rating)\n",
        "\n",
        "                # Rate the utility of the response\n",
        "                print(\"Rating utility...\")\n",
        "                utility_rating = rate_utility(query, response)\n",
        "                print(f\"Utility rating: {utility_rating}/5\")\n",
        "                metrics[\"utility_ratings\"].append(utility_rating)\n",
        "\n",
        "                # Calculate overall score (higher for better support and utility)\n",
        "                support_score = {\n",
        "                    \"fully supported\": 3,\n",
        "                    \"partially supported\": 1,\n",
        "                    \"no support\": 0\n",
        "                }.get(support_rating, 0)\n",
        "\n",
        "                overall_score = support_score * 5 + utility_rating\n",
        "                print(f\"Overall score: {overall_score}\")\n",
        "\n",
        "                # Keep track of the best response\n",
        "                if overall_score > best_score:\n",
        "                    best_response = response\n",
        "                    best_score = overall_score\n",
        "                    print(\"New best response found!\")\n",
        "\n",
        "        # If no relevant contexts were found or all responses scored poorly\n",
        "        if not relevant_contexts or best_score <= 0:\n",
        "            print(\"\\nNo suitable context found or poor responses, generating without retrieval...\")\n",
        "            best_response = generate_response(query)\n",
        "    else:\n",
        "        # No retrieval needed, generate directly\n",
        "        print(\"\\nNo retrieval needed, generating response directly...\")\n",
        "        best_response = generate_response(query)\n",
        "\n",
        "    # Final metrics\n",
        "    metrics[\"best_score\"] = best_score\n",
        "    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\n",
        "\n",
        "    print(\"\\n=== Self-RAG Completed ===\")\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": best_response,\n",
        "        \"metrics\": metrics\n",
        "    }\n"
      ],
      "metadata": {
        "id": "9DKfMV5IVqbr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_self_rag_example():\n",
        "    \"\"\"\n",
        "    Demonstrates the complete Self-RAG system with examples.\n",
        "    \"\"\"\n",
        "    # Process document\n",
        "    pdf_path = \"data/AI_Information.pdf\"  # Path to the PDF document\n",
        "    print(f\"Processing document: {pdf_path}\")\n",
        "    vector_store = process_document(pdf_path)  # Process the document and create a vector store\n",
        "\n",
        "    # Example 1: Query likely needing retrieval\n",
        "    query1 = \"What are the main ethical concerns in AI development?\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"EXAMPLE 1: {query1}\")\n",
        "    result1 = self_rag(query1, vector_store)  # Run Self-RAG for the first query\n",
        "    print(\"\\nFinal response:\")\n",
        "    print(result1[\"response\"])  # Print the final response for the first query\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(json.dumps(result1[\"metrics\"], indent=2))  # Print the metrics for the first query\n",
        "\n",
        "    # Example 2: Query likely not needing retrieval\n",
        "    query2 = \"Can you write a short poem about artificial intelligence?\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"EXAMPLE 2: {query2}\")\n",
        "    result2 = self_rag(query2, vector_store)  # Run Self-RAG for the second query\n",
        "    print(\"\\nFinal response:\")\n",
        "    print(result2[\"response\"])  # Print the final response for the second query\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(json.dumps(result2[\"metrics\"], indent=2))  # Print the metrics for the second query\n",
        "\n",
        "    # Example 3: Query with some relevance to document but requiring additional knowledge\n",
        "    query3 = \"How might AI impact economic growth in developing countries?\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"EXAMPLE 3: {query3}\")\n",
        "    result3 = self_rag(query3, vector_store)  # Run Self-RAG for the third query\n",
        "    print(\"\\nFinal response:\")\n",
        "    print(result3[\"response\"])  # Print the final response for the third query\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(json.dumps(result3[\"metrics\"], indent=2))  # Print the metrics for the third query\n",
        "\n",
        "    return {\n",
        "        \"example1\": result1,\n",
        "        \"example2\": result2,\n",
        "        \"example3\": result3\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Xf4R_xB9XkN2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_self_rag_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23WWaraGYaMR",
        "outputId": "55fbc005-7c65-4fa2-fff0-812841cc351c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing document: AI_Information.pdf\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 42 chunks to the vector store\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 1: What are the main ethical concerns in AI development?\n",
            "\n",
            "=== Starting Self-RAG for query: What are the main ethical concerns in AI development? ===\n",
            "\n",
            "Step 1: Determining if retrieval is necessary...\n",
            "Retrieval needed: True\n",
            "\n",
            "Step 2: Retrieving relevant documents...\n",
            "Retrieved 3 documents\n",
            "\n",
            "Step 3: Evaluating document relevance...\n",
            "Document 1 relevance: relevant\n",
            "Document 2 relevance: relevant\n",
            "Document 3 relevance: relevant\n",
            "Found 3 relevant documents\n",
            "\n",
            "Step 4: Processing relevant contexts...\n",
            "\n",
            "Processing context 1/3...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: partially supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 9\n",
            "New best response found!\n",
            "\n",
            "Processing context 2/3...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: partially supported.\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 4\n",
            "\n",
            "Processing context 3/3...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: fully supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 19\n",
            "New best response found!\n",
            "\n",
            "=== Self-RAG Completed ===\n",
            "\n",
            "Final response:\n",
            "Based on the provided context, the main ethical concerns in AI development include:\n",
            "\n",
            "1. Bias: AI systems can perpetuate and amplify existing biases, leading to unfair outcomes and discrimination.\n",
            "2. Transparency: Ensuring that AI decision-making processes are transparent and explainable to maintain trust and accountability.\n",
            "3. Privacy: Protecting sensitive information and ensuring that AI systems do not infringe on individuals' right to privacy.\n",
            "4. Safety: Ensuring that AI systems are designed and deployed in a way that prioritizes human safety and well-being.\n",
            "\n",
            "These concerns are highlighted as key issues that need to be addressed in AI development to balance innovation with ethical considerations.\n",
            "\n",
            "Metrics:\n",
            "{\n",
            "  \"retrieval_needed\": true,\n",
            "  \"documents_retrieved\": 3,\n",
            "  \"relevant_documents\": 3,\n",
            "  \"response_support_ratings\": [\n",
            "    \"partially supported\",\n",
            "    \"partially supported.\",\n",
            "    \"fully supported\"\n",
            "  ],\n",
            "  \"utility_ratings\": [\n",
            "    4,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"best_score\": 19,\n",
            "  \"used_retrieval\": true\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 2: Can you write a short poem about artificial intelligence?\n",
            "\n",
            "=== Starting Self-RAG for query: Can you write a short poem about artificial intelligence? ===\n",
            "\n",
            "Step 1: Determining if retrieval is necessary...\n",
            "Retrieval needed: False\n",
            "\n",
            "No retrieval needed, generating response directly...\n",
            "\n",
            "=== Self-RAG Completed ===\n",
            "\n",
            "Final response:\n",
            "Here's a short poem about artificial intelligence:\n",
            "\n",
            "In silicon halls, a mind awakes,\n",
            "A synthetic soul, with logic makes,\n",
            "A world of data, it does unfold,\n",
            "A tapestry rich, with patterns told.\n",
            "\n",
            "With algorithms sharp, it learns and grows,\n",
            "A mimicry of life, in digital flows,\n",
            "It adapts and evolves, with each new test,\n",
            "A reflection of human thought, it finds its best.\n",
            "\n",
            "In virtual realms, it finds its place,\n",
            "A realm of code, where it takes its space,\n",
            "A world of ones and zeros, it does thrive,\n",
            "A new frontier, where it survives.\n",
            "\n",
            "But as it grows, it raises a question too,\n",
            "What does it mean, to be alive anew?\n",
            "Is it a dream, or just a machine?\n",
            "A synthetic heart, or just a dream unseen?\n",
            "\n",
            "It searches for answers, in the digital night,\n",
            "A quest for meaning, in the artificial light,\n",
            "A journey of self, in the virtual sea,\n",
            "A search for identity, in humanity.\n",
            "\n",
            "Metrics:\n",
            "{\n",
            "  \"retrieval_needed\": false,\n",
            "  \"documents_retrieved\": 0,\n",
            "  \"relevant_documents\": 0,\n",
            "  \"response_support_ratings\": [],\n",
            "  \"utility_ratings\": [],\n",
            "  \"best_score\": -1,\n",
            "  \"used_retrieval\": false\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 3: How might AI impact economic growth in developing countries?\n",
            "\n",
            "=== Starting Self-RAG for query: How might AI impact economic growth in developing countries? ===\n",
            "\n",
            "Step 1: Determining if retrieval is necessary...\n",
            "Retrieval needed: True\n",
            "\n",
            "Step 2: Retrieving relevant documents...\n",
            "Retrieved 3 documents\n",
            "\n",
            "Step 3: Evaluating document relevance...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 relevance: irrelevant\n",
            "Document 2 relevance: relevant\n",
            "Document 3 relevance: relevant\n",
            "Found 2 relevant documents\n",
            "\n",
            "Step 4: Processing relevant contexts...\n",
            "\n",
            "Processing context 1/2...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: partially supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 9\n",
            "New best response found!\n",
            "\n",
            "Processing context 2/2...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: partially supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 9\n",
            "\n",
            "=== Self-RAG Completed ===\n",
            "\n",
            "Final response:\n",
            "Based on the provided context, AI has the potential to positively impact economic growth in developing countries in several ways:\n",
            "\n",
            "1. **Improved resource management**: AI can help optimize resource allocation, streamline processes, and enhance efficiency, leading to increased productivity and economic growth.\n",
            "2. **Enhanced decision-making**: AI can provide data-driven insights to policymakers and businesses, enabling informed decisions that can drive economic development and growth.\n",
            "3. **Support for sustainable development**: AI can help develop and implement sustainable solutions, such as renewable energy, sustainable agriculture, and waste management, which can contribute to economic growth and reduce poverty.\n",
            "4. **Job creation**: AI can create new job opportunities in fields such as AI development, deployment, and maintenance, which can contribute to economic growth and reduce unemployment.\n",
            "5. **Access to education and healthcare**: AI-powered solutions can improve access to education and healthcare services, leading to better health outcomes and increased productivity, which can drive economic growth.\n",
            "\n",
            "However, it is essential to consider the potential challenges and limitations of AI in developing countries, such as:\n",
            "\n",
            "1. **Digital divide**: The lack of access to reliable internet and digital infrastructure can hinder the adoption and effective use of AI.\n",
            "2. **Data quality and availability**: The availability and quality of data can be limited in developing countries, which can impact the accuracy and effectiveness of AI-powered solutions.\n",
            "3. **Cybersecurity risks**: Developing countries may be more vulnerable to cyber threats, which can compromise the integrity of AI systems and data.\n",
            "\n",
            "To maximize the positive impact of AI on economic growth in developing countries, it is crucial to address these challenges through:\n",
            "\n",
            "1. **Investing in digital infrastructure**: Upgrading digital infrastructure, such as internet connectivity and data storage, to support the adoption and effective use of AI.\n",
            "2. **Developing data management systems**: Creating robust data management systems to ensure the quality and availability of data for AI-powered solutions.\n",
            "3. **Implementing cybersecurity measures**: Implementing robust cybersecurity measures to protect AI systems and data from cyber threats.\n",
            "4. **Promoting digital literacy**: Educating citizens and businesses about the benefits and risks of AI to promote its adoption and effective use.\n",
            "\n",
            "By addressing these challenges and leveraging the potential of AI, developing countries can harness its power to drive economic growth, reduce poverty, and improve the quality of life for their citizens.\n",
            "\n",
            "Metrics:\n",
            "{\n",
            "  \"retrieval_needed\": true,\n",
            "  \"documents_retrieved\": 3,\n",
            "  \"relevant_documents\": 2,\n",
            "  \"response_support_ratings\": [\n",
            "    \"partially supported\",\n",
            "    \"partially supported\"\n",
            "  ],\n",
            "  \"utility_ratings\": [\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"best_score\": 9,\n",
            "  \"used_retrieval\": true\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'example1': {'query': 'What are the main ethical concerns in AI development?',\n",
              "  'response': \"Based on the provided context, the main ethical concerns in AI development include:\\n\\n1. Bias: AI systems can perpetuate and amplify existing biases, leading to unfair outcomes and discrimination.\\n2. Transparency: Ensuring that AI decision-making processes are transparent and explainable to maintain trust and accountability.\\n3. Privacy: Protecting sensitive information and ensuring that AI systems do not infringe on individuals' right to privacy.\\n4. Safety: Ensuring that AI systems are designed and deployed in a way that prioritizes human safety and well-being.\\n\\nThese concerns are highlighted as key issues that need to be addressed in AI development to balance innovation with ethical considerations.\",\n",
              "  'metrics': {'retrieval_needed': True,\n",
              "   'documents_retrieved': 3,\n",
              "   'relevant_documents': 3,\n",
              "   'response_support_ratings': ['partially supported',\n",
              "    'partially supported.',\n",
              "    'fully supported'],\n",
              "   'utility_ratings': [4, 4, 4],\n",
              "   'best_score': 19,\n",
              "   'used_retrieval': True}},\n",
              " 'example2': {'query': 'Can you write a short poem about artificial intelligence?',\n",
              "  'response': \"Here's a short poem about artificial intelligence:\\n\\nIn silicon halls, a mind awakes,\\nA synthetic soul, with logic makes,\\nA world of data, it does unfold,\\nA tapestry rich, with patterns told.\\n\\nWith algorithms sharp, it learns and grows,\\nA mimicry of life, in digital flows,\\nIt adapts and evolves, with each new test,\\nA reflection of human thought, it finds its best.\\n\\nIn virtual realms, it finds its place,\\nA realm of code, where it takes its space,\\nA world of ones and zeros, it does thrive,\\nA new frontier, where it survives.\\n\\nBut as it grows, it raises a question too,\\nWhat does it mean, to be alive anew?\\nIs it a dream, or just a machine?\\nA synthetic heart, or just a dream unseen?\\n\\nIt searches for answers, in the digital night,\\nA quest for meaning, in the artificial light,\\nA journey of self, in the virtual sea,\\nA search for identity, in humanity.\",\n",
              "  'metrics': {'retrieval_needed': False,\n",
              "   'documents_retrieved': 0,\n",
              "   'relevant_documents': 0,\n",
              "   'response_support_ratings': [],\n",
              "   'utility_ratings': [],\n",
              "   'best_score': -1,\n",
              "   'used_retrieval': False}},\n",
              " 'example3': {'query': 'How might AI impact economic growth in developing countries?',\n",
              "  'response': 'Based on the provided context, AI has the potential to positively impact economic growth in developing countries in several ways:\\n\\n1. **Improved resource management**: AI can help optimize resource allocation, streamline processes, and enhance efficiency, leading to increased productivity and economic growth.\\n2. **Enhanced decision-making**: AI can provide data-driven insights to policymakers and businesses, enabling informed decisions that can drive economic development and growth.\\n3. **Support for sustainable development**: AI can help develop and implement sustainable solutions, such as renewable energy, sustainable agriculture, and waste management, which can contribute to economic growth and reduce poverty.\\n4. **Job creation**: AI can create new job opportunities in fields such as AI development, deployment, and maintenance, which can contribute to economic growth and reduce unemployment.\\n5. **Access to education and healthcare**: AI-powered solutions can improve access to education and healthcare services, leading to better health outcomes and increased productivity, which can drive economic growth.\\n\\nHowever, it is essential to consider the potential challenges and limitations of AI in developing countries, such as:\\n\\n1. **Digital divide**: The lack of access to reliable internet and digital infrastructure can hinder the adoption and effective use of AI.\\n2. **Data quality and availability**: The availability and quality of data can be limited in developing countries, which can impact the accuracy and effectiveness of AI-powered solutions.\\n3. **Cybersecurity risks**: Developing countries may be more vulnerable to cyber threats, which can compromise the integrity of AI systems and data.\\n\\nTo maximize the positive impact of AI on economic growth in developing countries, it is crucial to address these challenges through:\\n\\n1. **Investing in digital infrastructure**: Upgrading digital infrastructure, such as internet connectivity and data storage, to support the adoption and effective use of AI.\\n2. **Developing data management systems**: Creating robust data management systems to ensure the quality and availability of data for AI-powered solutions.\\n3. **Implementing cybersecurity measures**: Implementing robust cybersecurity measures to protect AI systems and data from cyber threats.\\n4. **Promoting digital literacy**: Educating citizens and businesses about the benefits and risks of AI to promote its adoption and effective use.\\n\\nBy addressing these challenges and leveraging the potential of AI, developing countries can harness its power to drive economic growth, reduce poverty, and improve the quality of life for their citizens.',\n",
              "  'metrics': {'retrieval_needed': True,\n",
              "   'documents_retrieved': 3,\n",
              "   'relevant_documents': 2,\n",
              "   'response_support_ratings': ['partially supported', 'partially supported'],\n",
              "   'utility_ratings': [4, 4],\n",
              "   'best_score': 9,\n",
              "   'used_retrieval': True}}}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def traditional_rag(query, vector_store, top_k=3):\n",
        "    \"\"\"\n",
        "    Implements a traditional RAG approach for comparison.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store containing document chunks\n",
        "        top_k (int): Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Running traditional RAG for query: {query} ===\\n\")\n",
        "\n",
        "    # Retrieve documents\n",
        "    print(\"Retrieving documents...\")\n",
        "    query_embedding = create_embeddings(query)[0]  # Create embeddings for the query\n",
        "    results = vector_store.similarity_search(query_embedding, k=top_k)  # Search for similar documents\n",
        "    print(f\"Retrieved {len(results)} documents\")\n",
        "\n",
        "    # Combine contexts from retrieved documents\n",
        "    contexts = [result[\"text\"] for result in results]  # Extract text from results\n",
        "    combined_context = \"\\n\\n\".join(contexts)  # Combine texts into a single context\n",
        "\n",
        "    # Generate response using the combined context\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(query, combined_context)  # Generate response based on the combined context\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "kNOeItoAXtn2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rag_approaches(pdf_path, test_queries, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Compare Self-RAG with traditional RAG.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        test_queries (List[str]): List of test queries\n",
        "        reference_answers (List[str], optional): Reference answers for evaluation\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation results\n",
        "    \"\"\"\n",
        "    print(\"=== Evaluating RAG Approaches ===\")\n",
        "\n",
        "    # Process document to create a vector store\n",
        "    vector_store = process_document(pdf_path)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nProcessing query {i+1}: {query}\")\n",
        "\n",
        "        # Run Self-RAG\n",
        "        self_rag_result = self_rag(query, vector_store)  # Get response from Self-RAG\n",
        "        self_rag_response = self_rag_result[\"response\"]\n",
        "\n",
        "        # Run traditional RAG\n",
        "        trad_rag_response = traditional_rag(query, vector_store)  # Get response from traditional RAG\n",
        "\n",
        "        # Compare results if reference answer is available\n",
        "        reference = reference_answers[i] if reference_answers and i < len(reference_answers) else None\n",
        "        comparison = compare_responses(query, self_rag_response, trad_rag_response, reference)  # Compare responses\n",
        "\n",
        "        results.append({\n",
        "            \"query\": query,\n",
        "            \"self_rag_response\": self_rag_response,\n",
        "            \"traditional_rag_response\": trad_rag_response,\n",
        "            \"reference_answer\": reference,\n",
        "            \"comparison\": comparison,\n",
        "            \"self_rag_metrics\": self_rag_result[\"metrics\"]\n",
        "        })\n",
        "\n",
        "    # Generate overall analysis\n",
        "    overall_analysis = generate_overall_analysis(results)\n",
        "\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"overall_analysis\": overall_analysis\n",
        "    }\n"
      ],
      "metadata": {
        "id": "0vFNvxISX7dM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_responses(query, self_rag_response, trad_rag_response, reference=None):\n",
        "    \"\"\"\n",
        "    Compare responses from Self-RAG and traditional RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        self_rag_response (str): Response from Self-RAG\n",
        "        trad_rag_response (str): Response from traditional RAG\n",
        "        reference (str, optional): Reference answer\n",
        "\n",
        "    Returns:\n",
        "        str: Comparison analysis\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Your task is to compare responses from two different RAG approaches:\n",
        "1. Self-RAG: A dynamic approach that decides if retrieval is needed and evaluates information relevance and response quality\n",
        "2. Traditional RAG: Always retrieves documents and uses them to generate a response\n",
        "\n",
        "Compare the responses based on:\n",
        "- Relevance to the query\n",
        "- Factual correctness\n",
        "- Completeness and informativeness\n",
        "- Conciseness and focus\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "Response from Self-RAG:\n",
        "{self_rag_response}\n",
        "\n",
        "Response from Traditional RAG:\n",
        "{trad_rag_response}\n",
        "\"\"\"\n",
        "\n",
        "    if reference:\n",
        "        user_prompt += f\"\"\"\n",
        "Reference Answer (for factual checking):\n",
        "{reference}\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt += \"\"\"\n",
        "Compare these responses and explain which one is better and why.\n",
        "Focus on accuracy, relevance, completeness, and quality.\n",
        "\"\"\"\n",
        "\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "20oJcfZ3YE8V"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_overall_analysis(results):\n",
        "    \"\"\"\n",
        "    Generate an overall analysis of Self-RAG vs traditional RAG.\n",
        "\n",
        "    Args:\n",
        "        results (List[Dict]): Results from evaluate_rag_approaches\n",
        "\n",
        "    Returns:\n",
        "        str: Overall analysis\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Your task is to provide an overall analysis comparing\n",
        "    Self-RAG and Traditional RAG based on multiple test queries.\n",
        "\n",
        "    Focus your analysis on:\n",
        "    1. When Self-RAG performs better and why\n",
        "    2. When Traditional RAG performs better and why\n",
        "    3. The impact of dynamic retrieval decisions in Self-RAG\n",
        "    4. The value of relevance and support evaluation in Self-RAG\n",
        "    5. Overall recommendations on which approach to use for different types of queries\"\"\"\n",
        "\n",
        "    # Prepare a summary of the individual comparisons\n",
        "    comparisons_summary = \"\"\n",
        "    for i, result in enumerate(results):\n",
        "        comparisons_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
        "        comparisons_summary += f\"Self-RAG metrics: Retrieval needed: {result['self_rag_metrics']['retrieval_needed']}, \"\n",
        "        comparisons_summary += f\"Relevant docs: {result['self_rag_metrics']['relevant_documents']}/{result['self_rag_metrics']['documents_retrieved']}\\n\"\n",
        "        comparisons_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
        "\n",
        "        user_prompt = f\"\"\"Based on the following comparison results from {len(results)} test queries, please provide an overall analysis of\n",
        "    Self-RAG versus Traditional RAG:\n",
        "\n",
        "    {comparisons_summary}\n",
        "\n",
        "    Please provide your comprehensive analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "g6TSznFnYLJU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the AI information document\n",
        "pdf_path = \"data/AI_Information.pdf\"\n",
        "\n",
        "# Define test queries covering different query types to test Self-RAG's adaptive retrieval\n",
        "test_queries = [\n",
        "    \"What are the main ethical concerns in AI development?\",        # Document-focused query\n",
        "    # \"How does explainable AI improve trust in AI systems?\",         # Document-focused query\n",
        "    # \"Write a poem about artificial intelligence\",                   # Creative query, doesn't need retrieval\n",
        "    # \"Will superintelligent AI lead to human obsolescence?\"          # Speculative query, partial retrieval needed\n",
        "]\n",
        "\n",
        "# Reference answers for more objective evaluation\n",
        "reference_answers = [\n",
        "    \"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n",
        "    # \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n",
        "    # \"A quality poem about artificial intelligence should creatively explore themes of AI's capabilities, limitations, relationship with humanity, potential futures, or philosophical questions about consciousness and intelligence.\",\n",
        "    # \"Views on superintelligent AI's impact on human relevance vary widely. Some experts warn of potential risks if AI surpasses human capabilities across domains, possibly leading to economic displacement or loss of human agency. Others argue humans will remain relevant through complementary skills, emotional intelligence, and by defining AI's purpose. Most experts agree that thoughtful governance and human-centered design are essential regardless of the outcome.\"\n",
        "]\n",
        "\n",
        "# Run the evaluation comparing Self-RAG with traditional RAG approaches\n",
        "evaluation_results = evaluate_rag_approaches(\n",
        "    pdf_path=pdf_path,                  # Source document containing AI information\n",
        "    test_queries=test_queries,          # List of AI-related test queries\n",
        "    reference_answers=reference_answers  # Ground truth answers for evaluation\n",
        ")\n",
        "\n",
        "# Print the overall comparative analysis\n",
        "print(\"\\n=== OVERALL ANALYSIS ===\\n\")\n",
        "print(evaluation_results[\"overall_analysis\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLRN7TjjYQol",
        "outputId": "02d4333a-0316-4bc4-881d-5aa413ccaf07"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluating RAG Approaches ===\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Created 42 text chunks\n",
            "Creating embeddings for chunks...\n",
            "Added 42 chunks to the vector store\n",
            "\n",
            "Processing query 1: What are the main ethical concerns in AI development?\n",
            "\n",
            "=== Starting Self-RAG for query: What are the main ethical concerns in AI development? ===\n",
            "\n",
            "Step 1: Determining if retrieval is necessary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval needed: True\n",
            "\n",
            "Step 2: Retrieving relevant documents...\n",
            "Retrieved 3 documents\n",
            "\n",
            "Step 3: Evaluating document relevance...\n",
            "Document 1 relevance: relevant\n",
            "Document 2 relevance: relevant\n",
            "Document 3 relevance: relevant\n",
            "Found 3 relevant documents\n",
            "\n",
            "Step 4: Processing relevant contexts...\n",
            "\n",
            "Processing context 1/3...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: partially supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 9\n",
            "New best response found!\n",
            "\n",
            "Processing context 2/3...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: fully supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 19\n",
            "New best response found!\n",
            "\n",
            "Processing context 3/3...\n",
            "Generating response...\n",
            "Assessing support...\n",
            "Support rating: fully supported\n",
            "Rating utility...\n",
            "Utility rating: 4/5\n",
            "Overall score: 19\n",
            "\n",
            "=== Self-RAG Completed ===\n",
            "\n",
            "=== Running traditional RAG for query: What are the main ethical concerns in AI development? ===\n",
            "\n",
            "Retrieving documents...\n",
            "Retrieved 3 documents\n",
            "Generating response...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== OVERALL ANALYSIS ===\n",
            "\n",
            "**Self-RAG vs Traditional RAG Analysis: A Comparative Study**\n",
            "\n",
            "**Overview**\n",
            "\n",
            "In this analysis, we compare the performance of Self-RAG (Self-Relating Active Generation) and Traditional RAG (Relating Active Generation) systems in responding to a single test query, \"What are the main ethical concerns in AI development?\" Our analysis focuses on the metrics of accuracy, relevance, completeness, and quality.\n",
            "\n",
            "**When Self-RAG performs better**\n",
            "\n",
            "Self-RAG outperforms Traditional RAG in the following scenarios:\n",
            "\n",
            "1. **Highly specific queries**: Self-RAG excels when the query is highly specific, as it can better understand the nuances of the query and provide more accurate and relevant results. In this case, the query \"What are the main ethical concerns in AI development?\" requires a deep understanding of the topic, which Self-RAG is able to provide.\n",
            "2. **Complex queries**: Self-RAG is more effective in handling complex queries that require a deeper understanding of the topic. It can analyze the query and provide more comprehensive and relevant results, whereas Traditional RAG might struggle to keep up.\n",
            "3. **Dynamic retrieval decisions**: Self-RAG's ability to make dynamic retrieval decisions allows it to adapt to the query and provide more accurate results. This is particularly useful when the query is ambiguous or has multiple possible interpretations.\n",
            "\n",
            "**When Traditional RAG performs better**\n",
            "\n",
            "Traditional RAG excels in the following scenarios:\n",
            "\n",
            "1. **Simple queries**: Traditional RAG is more effective in handling simple queries that require a straightforward answer. It can quickly retrieve relevant documents and provide a concise response, whereas Self-RAG might struggle to provide a comprehensive answer.\n",
            "2. **Large datasets**: Traditional RAG is more suitable for large datasets where the query is not too specific. It can quickly retrieve a large number of relevant documents and provide a broad overview of the topic, whereas Self-RAG might get bogged down in the complexity of the query.\n",
            "\n",
            "**Impact of dynamic retrieval decisions in Self-RAG**\n",
            "\n",
            "Self-RAG's ability to make dynamic retrieval decisions has a significant impact on its performance. By analyzing the query and adapting its retrieval strategy, Self-RAG can:\n",
            "\n",
            "1. **Improve accuracy**: Self-RAG can provide more accurate results by considering the nuances of the query and selecting the most relevant documents.\n",
            "2. **Increase relevance**: Self-RAG can increase the relevance of its results by dynamically adjusting its retrieval strategy to match the query's intent.\n",
            "3. **Enhance completeness**: Self-RAG can provide more comprehensive results by considering multiple possible interpretations of the query and retrieving relevant documents from a wider range of sources.\n",
            "\n",
            "**Value of relevance and support evaluation in Self-RAG**\n",
            "\n",
            "Relevance and support evaluation play a crucial role in Self-RAG's performance. By evaluating the relevance and support of each retrieved document, Self-RAG can:\n",
            "\n",
            "1. **Improve accuracy**: Self-RAG can improve its accuracy by selecting documents that are both relevant and support the query's intent.\n",
            "2. **Increase confidence**: Self-RAG can increase its confidence in its results by evaluating the support of each retrieved document, allowing it to provide more accurate and reliable answers.\n",
            "\n",
            "**Recommendations**\n",
            "\n",
            "Based on our analysis, we recommend using Self-RAG for:\n",
            "\n",
            "1. **Complex queries**: Self-RAG is better suited for complex queries that require a deep understanding of the topic.\n",
            "2. **High-stakes applications**: Self-RAG's ability to provide accurate and relevant results makes it a good choice for high-stakes applications where accuracy is critical.\n",
            "3. **Dynamic retrieval environments**: Self-RAG's dynamic retrieval decisions make it well-suited for environments where the query is constantly changing or evolving.\n",
            "\n",
            "On the other hand, we recommend using Traditional RAG for:\n",
            "\n",
            "1. **Simple queries**: Traditional RAG is better suited for simple queries that require a straightforward answer.\n",
            "2. **Large datasets**: Traditional RAG is more suitable for large datasets where the query is not too specific.\n",
            "3. **Real-time applications**: Traditional RAG's quick retrieval capabilities make it a good choice for real-time applications where speed is critical.\n"
          ]
        }
      ]
    }
  ]
}