{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4fcf685",
   "metadata": {},
   "source": [
    "In a simple RAG setup, we follow these steps:\n",
    "1. **Data Ingestion**: Load and preprocess text data\n",
    "2. **Chunking**: Break data into smaller chunks to improve retrieval performance\n",
    "3. **Embedding**: Convert the text chunks into numerical representations using an emdedding model\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on user query\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieval text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f8784",
   "metadata": {},
   "source": [
    "### 1. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "92793e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "aeff84f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../conf.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70686f0d",
   "metadata": {},
   "source": [
    "### 2. Ingest data(PDF for this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e0c485ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f0393",
   "metadata": {},
   "source": [
    "### 3. Chunk the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "71906188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ea5d9",
   "metadata": {},
   "source": [
    "#### Set up Google GenAI Client\n",
    "Access [this](https://aistudio.google.com/apikey) to get your free API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4b66984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dbbdb",
   "metadata": {},
   "source": [
    "now load pdf, extract text and split it into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "71d19b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 42\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"../data/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e7abb",
   "metadata": {},
   "source": [
    "### 4. Emdedding\n",
    "Convert text into numerical vectors, which allow for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8b340ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00805921,  0.00604082,  0.01368964, ..., -0.00378978,\n",
       "        -0.00951595, -0.00788247], shape=(3072,)),\n",
       " array([-0.00541291, -0.00768946,  0.00840041, ..., -0.00555763,\n",
       "        -0.00453059, -0.00575172], shape=(3072,)),\n",
       " array([-0.01510609, -0.00370133, -0.01130207, ..., -0.01065998,\n",
       "         0.0138528 , -0.00942225], shape=(3072,)),\n",
       " array([-0.01703906, -0.00132903, -0.01255116, ..., -0.01783094,\n",
       "         0.00196428, -0.01987451], shape=(3072,)),\n",
       " array([ 0.00496949,  0.00703917,  0.00059291, ..., -0.01075656,\n",
       "        -0.00148964, -0.00847893], shape=(3072,)),\n",
       " array([ 0.00363039, -0.00663219,  0.00391538, ...,  0.00074985,\n",
       "        -0.00317055, -0.01557343], shape=(3072,)),\n",
       " array([-0.00655024, -0.00751401,  0.00925364, ..., -0.01057247,\n",
       "         0.00491098, -0.00145009], shape=(3072,)),\n",
       " array([-0.01434592,  0.00530639,  0.0129325 , ..., -0.01475381,\n",
       "         0.00640732,  0.00202144], shape=(3072,)),\n",
       " array([-0.0254624 , -0.0206279 , -0.0010777 , ..., -0.01374053,\n",
       "         0.00585974, -0.00667698], shape=(3072,)),\n",
       " array([-0.03328557, -0.00644612,  0.00576207, ..., -0.02736133,\n",
       "         0.01593894, -0.00025464], shape=(3072,)),\n",
       " array([-0.01463111, -0.01560566,  0.0058773 , ..., -0.01866096,\n",
       "         0.00784983,  0.00040406], shape=(3072,)),\n",
       " array([-0.00269015, -0.00624502,  0.00938848, ..., -0.02300911,\n",
       "         0.01303634,  0.00484036], shape=(3072,)),\n",
       " array([-0.01721786, -0.00919008,  0.01158605, ..., -0.01529638,\n",
       "        -0.00687852,  0.00320557], shape=(3072,)),\n",
       " array([-0.01235604, -0.01481227, -0.00359418, ..., -0.00980268,\n",
       "        -0.00402873, -0.00467687], shape=(3072,)),\n",
       " array([-0.01825543, -0.00223613, -0.00090202, ..., -0.00711819,\n",
       "         0.00790995, -0.00166702], shape=(3072,)),\n",
       " array([-0.02527447,  0.0081723 ,  0.00371846, ..., -0.00311524,\n",
       "         0.01264383, -0.00994038], shape=(3072,)),\n",
       " array([-0.0138795 , -0.00506325,  0.00448836, ..., -0.01179077,\n",
       "         0.00085376, -0.00335279], shape=(3072,)),\n",
       " array([-0.01591422,  0.00322442,  0.01382688, ..., -0.00599834,\n",
       "         0.00080812,  0.00494562], shape=(3072,)),\n",
       " array([-0.01062397, -0.00852746, -0.00294843, ..., -0.01081158,\n",
       "         0.00438706, -0.00158646], shape=(3072,)),\n",
       " array([-0.0158192 , -0.009129  ,  0.00666875, ..., -0.0111726 ,\n",
       "         0.00398373,  0.00087468], shape=(3072,)),\n",
       " array([ 0.00375695,  0.00844429,  0.00213977, ..., -0.00687256,\n",
       "         0.00207524, -0.00663738], shape=(3072,)),\n",
       " array([-0.00407079,  0.0056626 ,  0.00222136, ..., -0.00719033,\n",
       "         0.00531494, -0.01527148], shape=(3072,)),\n",
       " array([-0.00580376,  0.00476425,  0.00116879, ..., -0.00831368,\n",
       "        -0.00266314, -0.01171167], shape=(3072,)),\n",
       " array([-0.01309432,  0.00092666, -0.0012393 , ..., -0.01430836,\n",
       "         0.00091593, -0.00489943], shape=(3072,)),\n",
       " array([-0.01111439, -0.00466343,  0.00999298, ..., -0.00983951,\n",
       "        -0.00206729, -0.00391654], shape=(3072,)),\n",
       " array([-0.00164436, -0.00653233,  0.00785642, ...,  0.00141089,\n",
       "        -0.00940122, -0.01091332], shape=(3072,)),\n",
       " array([-0.01337521, -0.01875661,  0.00663016, ..., -0.02029369,\n",
       "         0.00394488, -0.01083988], shape=(3072,)),\n",
       " array([-0.02769485, -0.02029362,  0.00465149, ..., -0.01567821,\n",
       "         0.00693873, -0.00142724], shape=(3072,)),\n",
       " array([-0.02152857, -0.01828739,  0.02300829, ..., -0.00465369,\n",
       "        -0.00453249, -0.00092839], shape=(3072,)),\n",
       " array([-0.00309648,  0.00320951,  0.00667757, ..., -0.00143322,\n",
       "         0.00095242, -0.00845621], shape=(3072,)),\n",
       " array([-0.01210851, -0.00921584,  0.00013013, ..., -0.00471665,\n",
       "         0.01213457, -0.00374884], shape=(3072,)),\n",
       " array([-0.01509468,  0.01660672,  0.0001523 , ..., -0.0165257 ,\n",
       "         0.00526193,  0.00628905], shape=(3072,)),\n",
       " array([-0.00606456, -0.01481275, -0.00130978, ..., -0.00615658,\n",
       "        -0.01035849, -0.0116022 ], shape=(3072,)),\n",
       " array([-0.0077748 , -0.00499397,  0.01762595, ..., -0.01053174,\n",
       "        -0.00687297, -0.0065242 ], shape=(3072,)),\n",
       " array([-0.02124875, -0.01549251,  0.01616276, ..., -0.0158121 ,\n",
       "        -0.01048285, -0.00353676], shape=(3072,)),\n",
       " array([-0.02830753, -0.02268894,  0.01628986, ..., -0.03037773,\n",
       "         0.01344095,  0.00177008], shape=(3072,)),\n",
       " array([-0.02336927, -0.0193176 ,  0.0079946 , ..., -0.01978766,\n",
       "         0.01328553,  0.00230965], shape=(3072,)),\n",
       " array([-0.02436159, -0.02404325,  0.01032423, ..., -0.01807353,\n",
       "         0.01372153, -0.004266  ], shape=(3072,)),\n",
       " array([-0.03250303, -0.01030781,  0.00963771, ..., -0.03386123,\n",
       "         0.01696975,  0.00599309], shape=(3072,)),\n",
       " array([-0.03523463, -0.02026717,  0.00716095, ..., -0.02619512,\n",
       "         0.00457122,  0.00059559], shape=(3072,)),\n",
       " array([-0.03190796, -0.01443162,  0.02226692, ..., -0.0160346 ,\n",
       "         0.0127761 ,  0.01258855], shape=(3072,)),\n",
       " array([-0.01270343,  0.00383352,  0.01577535, ..., -0.01024577,\n",
       "         0.00519634,  0.00069179], shape=(3072,))]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_embeddings(text, model=\"gemini-embedding-001\"):\n",
    "    \"\"\"\n",
    "    Generates embedding vectors for a list of input texts using the Gemini embedding API.\n",
    "\n",
    "    Args:\n",
    "        text (List[str]): A list of input strings to embed.\n",
    "        model (str): Name of the Gemini embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: A list of embedding vectors, each corresponding to an input string.\n",
    "    \"\"\"\n",
    "\n",
    "    result = [\n",
    "        np.array(e.values) for e in client.models.embed_content(\n",
    "            model=model,\n",
    "            contents=text, \n",
    "            config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")).embeddings\n",
    "    ]\n",
    "    return result # list[np.array(x,)]\n",
    "response = create_embeddings(text_chunks)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a8c54a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between '0' and '1': 0.9061\n",
      "Similarity between '0' and '2': 0.8539\n",
      "Similarity between '0' and '3': 0.8081\n",
      "Similarity between '0' and '4': 0.8296\n",
      "Similarity between '1' and '2': 0.8759\n",
      "Similarity between '1' and '3': 0.8110\n",
      "Similarity between '1' and '4': 0.8435\n",
      "Similarity between '2' and '3': 0.8729\n",
      "Similarity between '2' and '4': 0.8194\n",
      "Similarity between '3' and '4': 0.8738\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = response # np.array(x,)\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# you can pass parameter of a list[NDArray(x,)] or np.stack(list[NDArray(x,)]) => NDArray(n, x), but not pair of NDArray(x,)\n",
    "\n",
    "for i, text1 in enumerate(text_chunks):\n",
    "    for j in range(i + 1, 5):\n",
    "        text2 = text_chunks[j]\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"Similarity between '{i}' and '{j}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab540f83",
   "metadata": {},
   "source": [
    "implement semantic search to find contextual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query) # :list[NDArray(x,)] # one NDArray(x,) element\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        # np.array(query_embedding) # :(x,)\n",
    "        # np.array(chunk_embedding) # :(x,)\n",
    "        similarity_score = cosine_similarity(query_embedding[0].reshape(1, -1), np.array(chunk_embedding).reshape(1, -1)) # cos(list[NDArray(x,)][0].reshape(1, -1) => (1, x), (x,1).reshape(1, -1) => (1,x))\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a6644",
   "metadata": {},
   "source": [
    "running a query on extracted chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4cb926b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00176254 -0.00547624  0.010617   ... -0.02954735  0.00507351\n",
      " -0.00670528]\n",
      "Query: How does AI contribute to personalized medicine?\n",
      "Context 1:\n",
      "ent by analyzing medical images, predicting \n",
      "patient outcomes, and assisting in treatment planning. AI-powered tools enhance accuracy, \n",
      "efficiency, and patient care. \n",
      "Drug Discovery and Development \n",
      "AI accelerates drug discovery and development by analyzing biological data, predicting drug \n",
      "efficacy, and identifying potential drug candidates. AI-powered systems reduce the time and cost \n",
      "of bringing new treatments to market. \n",
      "Personalized Medicine \n",
      "AI enables personalized medicine by analyzing individual patient data, predicting treatment \n",
      "responses, and tailoring interventions. Personalized medicine enhances treatment effectiveness \n",
      "and reduces adverse effects. \n",
      "Robotic Surgery \n",
      "AI-powered robotic surgery systems assist surgeons in performing complex procedures with \n",
      "greater precision and control. These systems enhance dexterity, reduce invasiveness, and \n",
      "improve patient outcomes. \n",
      "Healthcare Administration \n",
      "AI streamlines healthcare administration by automating tasks, managing patient\n",
      "=====================================\n",
      "Context 2:\n",
      " applications such as medical diagnosis, drug discovery, \n",
      "personalized medicine, and robotic surgery. AI-powered tools can analyze medical images, \n",
      "predict patient outcomes, and assist in treatment planning. \n",
      "Finance \n",
      "In finance, AI is used for fraud detection, algorithmic trading, risk management, and customer \n",
      "service. AI algorithms can analyze large datasets to identify patterns, predict market trends, and \n",
      "automate financial processes. \n",
      " \n",
      "Transportation \n",
      "AI is revolutionizing transportation with the development of self-driving cars, traffic optimization \n",
      "systems, and logistics management. Autonomous vehicles use AI to perceive their surroundings, \n",
      "make driving decisions, and navigate safely. \n",
      "Retail \n",
      "The retail industry uses AI for personalized recommendations, inventory management, customer \n",
      "service chatbots, and supply chain optimization. AI-powered systems can analyze customer data \n",
      "to predict demand, personalize offers, and improve the shopping experience. \n",
      "Manufacturing \n",
      "AI is\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('../data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[3]['question']\n",
    "# print(create_embeddings(query)[0]) # :list[one NDArray(x,)]\n",
    "# Perform semantic search to find the top 2 most relevant text chunks for the query\n",
    "top_chunks = semantic_search(query, text_chunks, response, k=2)\n",
    "\n",
    "# Print the query\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Print the top 2 most relevant text chunks\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed0883",
   "metadata": {},
   "source": [
    "### 5. Generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bddab884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This approach enhances treatment effectiveness and reduces adverse effects.\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I don't have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gemini-2.5-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    GenerateContentResponse: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_prompt,\n",
    "            temperature=1\n",
    "            ),\n",
    "        contents=user_message\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)\n",
    "print(ai_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd60a8",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5cf5b4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.text}\\nTrue Response: {data[3]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7a1d8ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99075595]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_gen = create_embeddings(ai_response) # list[NDArray(x,)]\n",
    "ideal_re = create_embeddings(data[3]['ideal_answer']) #list[NDArray(x,)]\n",
    "\n",
    "# ai_gen, ideal_re\n",
    "cosine_similarity(ai_gen, ideal_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4d8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
