{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Fusion Retrieval: Combining Vector and Keyword Search\n",
    "\n",
    "In this notebook, I implement a fusion retrieval system that combines the strengths of semantic vector search with keyword-based BM25 retrieval. This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n",
    "\n",
    "## Why Fusion Retrieval Matters\n",
    "\n",
    "Traditional RAG systems typically rely on vector search alone, but this has limitations:\n",
    "\n",
    "- Vector search excels at semantic similarity but may miss exact keyword matches\n",
    "- Keyword search is great for specific terms but lacks semantic understanding\n",
    "- Different queries perform better with different retrieval methods\n",
    "\n",
    "Fusion retrieval gives us the best of both worlds by:\n",
    "\n",
    "- Performing both vector-based and keyword-based retrieval\n",
    "- Normalizing the scores from each approach\n",
    "- Combining them with a weighted formula\n",
    "- Ranking documents based on the combined score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv  import load_dotenv\n",
    "load_dotenv(\"conf.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"GEMINI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # Get the page object\n",
    "        text += page.get_text()  # Extract text from the page and append to the text string\n",
    "    \n",
    "    return text  # Return the extracted text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with text and metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunk_data = {\n",
    "                \"text\": chunk,  # The chunk text\n",
    "                \"metadata\": {\n",
    "                    \"start_char\": i,  # Start character index of the chunk\n",
    "                    \"end_char\": i + len(chunk)  # End character index of the chunk\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk_data)  # Add the chunk data to the list\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    \n",
    "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "  is_single = isinstance(text, str)\n",
    "  if is_single: text = [text]\n",
    "\n",
    "  try:\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding = True,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(device)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return None\n",
    "\n",
    "  try:\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "      cls = outputs.last_hidden_state[:, 0, :]\n",
    "      embed_normalized = F.normalize(cls, p = 2, dim = 1)\n",
    "    embeddings = [embed.cpu().numpy() for embed in embed_normalized]\n",
    "\n",
    "    return embeddings\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(embedding)  # Append the embedding vector\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of text items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],  # Extract text from item\n",
    "                embedding=embedding,  # Use corresponding embedding\n",
    "                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n",
    "            )\n",
    "    \n",
    "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding with similarity scores.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = cosine_similarity(query_embedding.reshape(1, -1), vector.reshape(1, -1))[0][0]  # Compute cosine similarity\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results with scores\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Retrieve text by index\n",
    "                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n",
    "                \"similarity\": float(score)  # Add similarity score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_documents(self):\n",
    "        \"\"\"\n",
    "        Get all documents in the store.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All documents\n",
    "        \"\"\"\n",
    "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(chunks):\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        BM25Okapi: A BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from each chunk\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    tokenized_docs = [text.split() for text in texts]\n",
    "    \n",
    "    # Create the BM25 index using the tokenized documents\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Print the number of documents in the BM25 index\n",
    "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
    "    \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(bm25, chunks, query, k=5):\n",
    "    \"\"\"\n",
    "    Search the BM25 index with a query.\n",
    "    \n",
    "    Args:\n",
    "        bm25 (BM25Okapi): BM25 index\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        query (str): Query string\n",
    "        k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results with scores\n",
    "    \"\"\"\n",
    "    # Tokenize the query by splitting it into individual words\n",
    "    query_tokens = query.split()\n",
    "    \n",
    "    # Get BM25 scores for the query tokens against the indexed documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Initialize an empty list to store results with their scores\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the scores and corresponding chunks\n",
    "    for i, score in enumerate(scores):\n",
    "        # Create a copy of the metadata to avoid modifying the original\n",
    "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
    "        # Add index to metadata\n",
    "        metadata[\"index\"] = i\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": chunks[i][\"text\"],\n",
    "            \"metadata\": metadata,  # Add metadata with index\n",
    "            \"bm25_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort the results by BM25 score in descending order\n",
    "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining vector-based and BM25 search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        chunks (List[Dict]): Original text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of results to return\n",
    "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results based on combined scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing fusion retrieval for query: {query}\")\n",
    "    \n",
    "    # Define small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_embedding = create_embeddings(query)[0]  # Create embedding for the query\n",
    "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n",
    "    \n",
    "    # Get BM25 search results\n",
    "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n",
    "    \n",
    "    # Create dictionaries to map document index to score\n",
    "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
    "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
    "    \n",
    "    # Ensure all documents have scores for both methods\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    combined_results = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n",
    "        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n",
    "        combined_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"vector_score\": vector_score,\n",
    "            \"bm25_score\": bm25_score,\n",
    "            \"index\": i\n",
    "        })\n",
    "    \n",
    "    # Extract scores as arrays\n",
    "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
    "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
    "    \n",
    "    # Normalize scores\n",
    "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "    \n",
    "    # Compute combined scores\n",
    "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
    "    \n",
    "    # Add combined scores to results\n",
    "    for i, score in enumerate(combined_scores):\n",
    "        combined_results[i][\"combined_score\"] = float(score)\n",
    "    \n",
    "    # Sort by combined score (descending)\n",
    "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    \n",
    "    # Return top k results\n",
    "    top_results = combined_results[:k]\n",
    "    \n",
    "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Clean the extracted text to remove extra whitespace and special characters\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Split the cleaned text into overlapping chunks\n",
    "    chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract the text content from each chunk for embedding creation\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    \n",
    "    # Create embeddings for the chunk texts\n",
    "    embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add the chunks and their embeddings to the vector store\n",
    "    vector_store.add_items(chunks, embeddings)\n",
    "    print(f\"Added {len(chunks)} items to vector store\")\n",
    "    \n",
    "    # Create a BM25 index from the chunks\n",
    "    bm25_index = create_bm25_index(chunks)\n",
    "    \n",
    "    # Return the chunks, vector store, and BM25 index\n",
    "    return chunks, vector_store, bm25_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context from retrieved documents\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n",
    "\n",
    "    # Format the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please answer the question ONLY based on the provided context. DO NOT make up an answer.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"GEMINI_GEN_MODEL\"),  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.1  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Answer a query using fusion RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results including retrieved documents and response\n",
    "    \"\"\"\n",
    "    # Retrieve documents using fusion retrieval method\n",
    "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_only_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only vector-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)[0]\n",
    "    \n",
    "    # Retrieve documents using vector-based similarity search\n",
    "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only BM25-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Retrieve documents using BM25 search\n",
    "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare different retrieval methods for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
    "    \n",
    "    # Run vector-only RAG\n",
    "    print(\"\\nRunning vector-only RAG...\")\n",
    "    vector_result = vector_only_rag(query, vector_store, k)\n",
    "    \n",
    "    # Run BM25-only RAG\n",
    "    print(\"\\nRunning BM25-only RAG...\")\n",
    "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
    "    \n",
    "    # Run fusion RAG\n",
    "    print(\"\\nRunning fusion RAG...\")\n",
    "    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n",
    "    \n",
    "    # Compare responses from different retrieval methods\n",
    "    print(\"\\nComparing responses...\")\n",
    "    comparison = evaluate_responses(\n",
    "        query, \n",
    "        vector_result[\"response\"], \n",
    "        bm25_result[\"response\"], \n",
    "        fusion_result[\"response\"],\n",
    "        reference_answer\n",
    "    )\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"vector_result\": vector_result,\n",
    "        \"bm25_result\": bm25_result,\n",
    "        \"fusion_result\": fusion_result,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the responses from different retrieval methods.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_response (str): Response from vector-only RAG\n",
    "        bm25_response (str): Response from BM25-only RAG\n",
    "        fusion_response (str): Response from fusion RAG\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation of responses\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
    "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
    "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
    "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
    "\n",
    "    Evaluate the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Comprehensiveness\n",
    "    - Clarity and coherence\"\"\"\n",
    "\n",
    "    # User prompt containing the query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Vector-based response:\n",
    "    {vector_response}\n",
    "\n",
    "    BM25 keyword response:\n",
    "    {bm25_response}\n",
    "\n",
    "    Fusion response:\n",
    "    {fusion_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # Add reference answer to the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "            Reference answer:\n",
    "            {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add instructions for detailed comparison to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
    "    Be specific about the strengths and weaknesses of each approach for this particular query. If there is reference answer, the evaluation and comparision accucaries SHOULD BE compared to the reference answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the evaluation \n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"GEMINI_GEN_MODEL\"),  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated evaluation content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluating queries\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems. \n",
    "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
    "    1. Vector-based retrieval (semantic similarity)\n",
    "    2. BM25 keyword retrieval (keyword matching)\n",
    "    3. Fusion retrieval (combination of both)\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where each approach performs best\n",
    "    2. Overall strengths and weaknesses of each approach\n",
    "    3. How fusion retrieval balances the trade-offs\n",
    "    4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries, \n",
    "    provide an overall analysis comparing these three approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n",
    "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
    "\n",
    "    # Generate the overall analysis \n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"GEMINI_GEN_MODEL\"),\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    # print(\"===RESULTS===\")\n",
    "    # print(results)\n",
    "    # Return the generated analysis content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate fusion retrieval compared to other methods.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
    "    \n",
    "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
    "    chunks, vector_store, bm25_index = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each test query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get the reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Compare retrieval methods for the current query\n",
    "        comparison = compare_retrieval_methods(\n",
    "            query, \n",
    "            chunks, \n",
    "            vector_store, \n",
    "            bm25_index, \n",
    "            k=k, \n",
    "            alpha=alpha,\n",
    "            reference_answer=reference\n",
    "        )\n",
    "        \n",
    "        # Append the comparison results to the results list\n",
    "        results.append(comparison)\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response ===\")\n",
    "        print(comparison[\"vector_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== BM25 Response ===\")\n",
    "        print(comparison[\"bm25_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Fusion Response ===\")\n",
    "        print(comparison[\"fusion_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(comparison[\"comparison\"])\n",
    "    \n",
    "    # Generate an overall analysis of the fusion retrieval performance\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    # Return the results and overall analysis\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Fusion Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING FUSION RETRIEVAL ===\n",
      "\n",
      "Extracting text from data/AI_Information.pdf...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 items to vector store\n",
      "Created BM25 index with 42 documents\n",
      "\n",
      "\n",
      "=== Evaluating Query 1/10 ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "\n",
      "=== Comparing retrieval methods for query: What is 'Explainable AI' and why is it considered important? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What is 'Explainable AI' and why is it considered important?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in XAI focuses on developing methods for explaining AI decisions.\n",
      "\n",
      "It is considered important for several reasons:\n",
      "*   It enhances trust and improves accountability.\n",
      "*   It helps users assess the reliability and fairness of AI systems.\n",
      "*   It enables users to assess the fairness and accuracy of AI decisions.\n",
      "*   It is crucial for building trust and accountability, especially as many AI systems, particularly deep learning models, are \"black boxes\" making it difficult to understand their decisions.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The context states that many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. While the term \"Explainable AI\" is not explicitly defined, the text indicates that \"Enhancing transparency and explainability is crucial for building trust and accountability.\"\n",
      "\n",
      "=== Fusion Response ===\n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in XAI focuses on developing methods for explaining AI decisions.\n",
      "\n",
      "It is considered important for several reasons:\n",
      "*   It enhances trust and improves accountability.\n",
      "*   Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions; enhancing transparency and explainability is crucial for building trust and accountability.\n",
      "*   Making AI systems understandable and providing insights into their decision-making processes helps users assess their reliability and fairness.\n",
      "\n",
      "=== Comparison ===\n",
      "This detailed comparison evaluates the three retrieval approaches against the provided query and reference answer.\n",
      "\n",
      "**Query:** What is 'Explainable AI' and why is it considered important?\n",
      "**Reference Answer:** Explainable AI (XAI) aims to make AI systems more transparent and understandable, providing insights into how they make decisions. It's considered important for building trust, accountability, and ensuring fairness in AI systems.\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response**\n",
      "\n",
      "*   **Relevance to the query:** High. Directly answers both parts of the query.\n",
      "*   **Factual correctness:** Correct. All statements are accurate.\n",
      "*   **Comprehensiveness:** Good.\n",
      "    *   **What is XAI?** Provides a clear and accurate definition, aligning well with the reference answer and adding the detail about \"research in XAI focuses on developing methods.\"\n",
      "    *   **Why is it important?** Covers trust, accountability, and fairness, which are key points from the reference. It also introduces the crucial concept of \"black box\" models, which is a primary driver for XAI's importance, adding valuable context beyond the reference.\n",
      "*   **Clarity and coherence:** Good. The response is clear and uses bullet points effectively. However, there is some redundancy in the importance points (\"It helps users assess the reliability and fairness...\" and \"It enables users to assess the fairness and accuracy...\").\n",
      "*   **Accuracy compared to Reference:** Very high. It accurately captures the essence of the reference answer and expands upon it with relevant details (e.g., \"black box\" models).\n",
      "\n",
      "**Strengths:**\n",
      "*   Successfully defines XAI based on semantic understanding.\n",
      "*   Identifies multiple key reasons for XAI's importance, including the critical \"black box\" problem.\n",
      "*   Provides a comprehensive and factually correct answer.\n",
      "\n",
      "**Weaknesses:**\n",
      "*   Slight redundancy in the bullet points for importance.\n",
      "\n",
      "---\n",
      "\n",
      "**2. BM25 keyword response**\n",
      "\n",
      "*   **Relevance to the query:** Poor. It explicitly states that the term \"Explainable AI\" is not defined, failing to answer the first part of the query directly. It only partially addresses the second part.\n",
      "*   **Factual correctness:** The quoted text is likely correct from its source, but the response itself is incomplete and misleading by stating a definition isn't present when it should be synthesized.\n",
      "*   **Comprehensiveness:** Very poor.\n",
      "    *   **What is XAI?** Fails completely to define XAI.\n",
      "    *   **Why is it important?** Only provides a fragmented answer, quoting \"Enhancing transparency and explainability is crucial for building trust and accountability.\" It misses \"fairness\" and doesn't elaborate on *why* transparency is crucial beyond the quote. It mentions \"black boxes\" but uses it as an excuse for not having a definition rather than a reason for XAI's importance.\n",
      "*   **Clarity and coherence:** Poor. The response reads like a summary of what *wasn't* found or what was only partially found, rather than a direct answer. It's disjointed.\n",
      "*   **Accuracy compared to Reference:** Very low. It fails to provide a definition and only partially covers the importance, missing key aspects like fairness and the underlying reasons for importance.\n",
      "\n",
      "**Strengths:**\n",
      "*   Might be good for very specific keyword matches where the answer is explicitly stated in a single sentence.\n",
      "\n",
      "**Weaknesses:**\n",
      "*   Struggles with queries requiring synthesis or a definition that isn't explicitly phrased as \"X is Y.\"\n",
      "*   Fails to provide a direct answer to the core definition.\n",
      "*   Provides an incomplete and fragmented answer to the importance.\n",
      "*   Highlights the limitations of pure keyword matching for conceptual queries.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Fusion response**\n",
      "\n",
      "*   **Relevance to the query:** High. Directly and effectively answers both parts of the query.\n",
      "*   **Factual correctness:** Correct. All statements are accurate.\n",
      "*   **Comprehensiveness:** Excellent.\n",
      "    *   **What is XAI?** Provides a clear and accurate definition, identical to the vector response, aligning perfectly with the reference.\n",
      "    *   **Why is it important?** Covers trust, accountability, fairness, and reliability. Crucially, it integrates the \"black box\" problem very smoothly and concisely as a direct reason for XAI's importance, making the explanation highly impactful and comprehensive without redundancy.\n",
      "*   **Clarity and coherence:** Excellent. The response is concise, well-structured, and flows logically. It avoids the redundancy seen in the vector response while still providing all necessary details.\n",
      "*   **Accuracy compared to Reference:** Very high. It accurately captures the essence of the reference answer and expands upon it with highly relevant and well-integrated details (e.g., \"black box\" models), making it even more informative than the reference.\n",
      "\n",
      "**Strengths:**\n",
      "*   Combines the semantic understanding for the core definition with the ability to pull in and integrate highly relevant contextual information (like \"black box\" models) effectively.\n",
      "*   Provides a comprehensive, accurate, and concise answer.\n",
      "*   Avoids redundancy while being thorough.\n",
      "*   Demonstrates superior synthesis of information.\n",
      "\n",
      "**Weaknesses:**\n",
      "*   None significant for this query.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion: Which approach performed best and why?**\n",
      "\n",
      "For this query, the **Fusion retrieval approach performed best**.\n",
      "\n",
      "**Reasons:**\n",
      "\n",
      "1.  **Comprehensive and Accurate Definition:** Like the vector approach, it successfully defined 'Explainable AI' accurately and clearly, aligning with the reference answer. This indicates that the semantic understanding component (likely from the vector part) was effective.\n",
      "2.  **Superior Explanation of Importance:** The fusion response excelled in explaining *why* XAI is important. It not only covered the core aspects of trust, accountability, and fairness (as in the reference and vector response) but also seamlessly integrated the critical concept of \"black box\" AI models as a primary driver for XAI's necessity. This integration was more concise and impactful than in the vector response, which had some redundancy.\n",
      "3.  **Optimal Balance:** It avoided the pitfalls of pure keyword matching (like BM25's failure to define XAI) and improved upon the slight redundancy of the pure vector approach. By combining both methods, it likely retrieved a richer set of relevant documents (due to keyword hits) and then semantically understood and synthesized the information more effectively.\n",
      "4.  **Clarity and Conciseness:** The fusion response was the most articulate and well-structured, providing a complete answer without unnecessary repetition.\n",
      "\n",
      "The **BM25 keyword retrieval** performed worst because it failed to answer the definitional part of the query and provided only a fragmented, quoted answer for the importance, highlighting its limitation when direct keyword matches for a definition are not present or when synthesis is required.\n",
      "\n",
      "The **Vector-based retrieval** performed very well, providing a largely correct and comprehensive answer. Its only minor drawback was a slight redundancy in explaining the importance. However, the fusion approach managed to present the same information more concisely and effectively.\n",
      "\n",
      "\n",
      "=== Evaluating Query 2/10 ===\n",
      "Query: Can AI be used to predict earthquakes?\n",
      "\n",
      "=== Comparing retrieval methods for query: Can AI be used to predict earthquakes? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: Can AI be used to predict earthquakes?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "Based on the provided context, AI enhances disaster response by \"analyzing data, predicting impacts, and supporting relief efforts.\" However, the context does not explicitly state that AI can be used to predict the *occurrence* of earthquakes.\n",
      "\n",
      "=== BM25 Response ===\n",
      "Based on the provided context, there is no mention of AI being used to predict earthquakes. The text states that AI is increasingly being used to address social and environmental challenges such as climate change, poverty, and healthcare disparities, but it does not specifically list earthquake prediction as an application.\n",
      "\n",
      "=== Fusion Response ===\n",
      "Based on the provided context, there is no mention of AI being used to predict earthquakes. The context lists various applications of AI in healthcare, finance, transportation, retail, and manufacturing, and mentions AI for social good addressing challenges like climate change, poverty, and healthcare disparities, but earthquakes are not specified.\n",
      "\n",
      "=== Comparison ===\n",
      "The user is asking to evaluate three RAG responses against a reference answer for the query \"Can AI be used to predict earthquakes?\". The key insight from the reference answer (\"I don't have enough information to answer that\") is that the underlying context *does not contain information about AI predicting earthquakes*. Therefore, the best responses should accurately reflect this lack of information.\n",
      "\n",
      "Let's evaluate each response:\n",
      "\n",
      "**Reference Answer:** I don't have enough information to answer that.\n",
      "*   **Interpretation:** The source context does not provide an answer to whether AI can predict earthquakes. A good RAG system should convey this.\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response:**\n",
      "*   \"Based on the provided context, AI enhances disaster response by 'analyzing data, predicting impacts, and supporting relief efforts.'\"\n",
      "    *   **Relevance:** This part is semantically related to the query (disaster, prediction) but not specific to earthquakes. It suggests the vector search found documents discussing AI in a broader disaster context.\n",
      "    *   **Factual Correctness:** Assumes the quoted phrase is from the context. If so, it's factually correct within that context.\n",
      "*   \"However, the context does not explicitly state that AI can be used to predict the *occurrence* of earthquakes.\"\n",
      "    *   **Relevance:** Directly addresses the query by stating the absence of specific information.\n",
      "    *   **Factual Correctness:** This aligns perfectly with the reference answer.\n",
      "*   **Comprehensiveness:** It correctly identifies the lack of specific information. It also provides a related piece of information (AI in disaster response) which, while not directly answering the earthquake part, shows some relevant context was retrieved.\n",
      "*   **Clarity and Coherence:** Clear and easy to understand.\n",
      "\n",
      "---\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "*   \"Based on the provided context, there is no mention of AI being used to predict earthquakes.\"\n",
      "    *   **Relevance:** Directly and accurately answers the core of the query by stating the absence of information.\n",
      "    *   **Factual Correctness:** Aligns perfectly with the reference answer.\n",
      "*   \"The text states that AI is increasingly being used to address social and environmental challenges such as climate change, poverty, and healthcare disparities, but it does not specifically list earthquake prediction as an application.\"\n",
      "    *   **Relevance:** Provides specific examples of what *is* mentioned in the context, which effectively reinforces the absence of earthquake prediction.\n",
      "    *   **Factual Correctness:** Assumes these challenges are listed in the context. If so, it's factually correct.\n",
      "*   **Comprehensiveness:** Very good. It not only states what's *not* there but also provides specific examples of what *is* there, giving context to the negative answer.\n",
      "*   **Clarity and Coherence:** Clear, concise, and well-structured.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Fusion response:**\n",
      "*   \"Based on the provided context, there is no mention of AI being used to predict earthquakes.\"\n",
      "    *   **Relevance:** Directly and accurately answers the core of the query by stating the absence of information.\n",
      "    *   **Factual Correctness:** Aligns perfectly with the reference answer.\n",
      "*   \"The context lists various applications of AI in healthcare, finance, transportation, retail, and manufacturing, and mentions AI for social good addressing challenges like climate change, poverty, and healthcare disparities, but earthquakes are not specified.\"\n",
      "    *   **Relevance:** Provides a comprehensive list of what *is* mentioned in the context, which strongly reinforces the absence of earthquake prediction.\n",
      "    *   **Factual Correctness:** Assumes these applications and challenges are listed in the context. If so, it's factually correct.\n",
      "*   **Comprehensiveness:** Excellent. It not only states what's *not* there but provides the most detailed and comprehensive list of what *is* there, making the \"no mention\" claim very robust.\n",
      "*   **Clarity and Coherence:** Clear, concise, and well-structured.\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison and Which Approach Performed Best:**\n",
      "\n",
      "All three responses successfully convey that the information about AI predicting earthquakes is *not* present in the provided context, which is the correct behavior given the reference answer. This indicates that all three RAG systems are functioning well in identifying the absence of information.\n",
      "\n",
      "However, there are subtle differences in how they support this \"no information\" claim:\n",
      "\n",
      "*   **Vector-based:** Its strength lies in semantic understanding. It correctly identifies the lack of *specific* information about earthquake prediction, even while retrieving broader, semantically related concepts like \"disaster response.\" Its weakness is that the supporting evidence for the *absence* of earthquake prediction is less direct; it states what *is* there (general disaster response) and then what *isn't* (specific earthquake prediction). It's good, but not as strong in providing *negative evidence* as the others.\n",
      "\n",
      "*   **BM25 keyword:** This approach excels at identifying the precise absence of keywords. It directly states that \"no mention\" of earthquake prediction exists. Its strength is then reinforced by listing *other* specific applications that *are* mentioned (climate change, poverty, healthcare disparities). This provides strong contextual evidence for why earthquake prediction isn't covered.\n",
      "\n",
      "*   **Fusion retrieval:** This approach combines the strengths of both. It directly states the absence of information, similar to BM25. Crucially, it then provides the *most comprehensive* list of what *is* mentioned in the context (healthcare, finance, transportation, retail, manufacturing, *and* social good challenges like climate change, poverty, healthcare disparities). This extensive list of *other* applications makes the absence of earthquake prediction even more definitive and well-supported. It demonstrates that the system thoroughly scanned the relevant context and found no mention.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "While all three responses correctly align with the reference answer by indicating a lack of information, the **Fusion retrieval** approach performed best for this query.\n",
      "\n",
      "**Why Fusion performed best:**\n",
      "*   **Superior Comprehensiveness (of negative evidence):** It provided the most exhaustive list of what *was* present in the context, thereby making the absence of earthquake prediction the most strongly supported. This gives the user the highest confidence that the information truly isn't there.\n",
      "*   **Clarity and Directness:** It was as direct and clear as BM25 in stating the absence of information.\n",
      "*   **Robustness:** By combining both semantic and keyword approaches, it was able to retrieve a broader and more relevant set of contextual information (even if it was to prove a negative), leading to a more complete and convincing answer.\n",
      "\n",
      "In essence, the Fusion response not only said \"I don't have enough information\" but also provided the most compelling evidence from the context to back up that claim.\n",
      "\n",
      "\n",
      "=== Evaluating Query 3/10 ===\n",
      "Query: What are some of the ethical concerns related to AI-powered facial recognition?\n",
      "\n",
      "=== Comparing retrieval methods for query: What are some of the ethical concerns related to AI-powered facial recognition? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What are some of the ethical concerns related to AI-powered facial recognition?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The provided context discusses several ethical and societal concerns related to AI in general, such as:\n",
      "*   Bias and Fairness\n",
      "*   Transparency and Explainability\n",
      "*   Privacy and Security\n",
      "*   Job Displacement\n",
      "*   Autonomy and Control\n",
      "*   Weaponization of AI\n",
      "\n",
      "However, the context does not contain any information specifically about ethical concerns related to AI-powered facial recognition.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The provided context discusses general ethical and societal concerns related to AI, which include:\n",
      "*   **Bias and Fairness:** AI systems can inherit and amplify biases present in their training data, leading to unfair or discriminatory outcomes.\n",
      "*   **Transparency and Explainability:** Many AI systems are \"black boxes,\" making it difficult to understand their decision-making processes.\n",
      "*   **Privacy and Security:** AI systems often rely on large amounts of data, raising concerns about privacy and security.\n",
      "\n",
      "While the context mentions facial recognition as an application of AI, it does not specifically detail the ethical concerns *related to AI-powered facial recognition*. It only lists the general ethical concerns for AI as a whole.\n",
      "\n",
      "=== Fusion Response ===\n",
      "Based on the provided context, there is no information about the ethical concerns related to AI-powered facial recognition. The text discusses general ethical concerns of AI such as bias and fairness, transparency and explainability, privacy and security, job displacement, autonomy and control, and weaponization of AI, but it does not specifically mention facial recognition.\n",
      "\n",
      "=== Comparison ===\n",
      "This evaluation will compare the three RAG responses against the provided query and the reference answer, focusing on relevance, factual correctness, comprehensiveness, and clarity.\n",
      "\n",
      "**Query:** What are some of the ethical concerns related to AI-powered facial recognition?\n",
      "**Reference Answer:** I don't have enough information to answer that.\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response:**\n",
      "*   \"The provided context discusses several ethical and societal concerns related to AI in general, such as: Bias and Fairness, Transparency and Explainability, Privacy and Security, Job Displacement, Autonomy and Control, Weaponization of AI\"\n",
      "*   \"However, the context does not contain any information specifically about ethical concerns related to AI-powered facial recognition.\"\n",
      "\n",
      "    *   **Relevance to the query:** Partially relevant. It correctly identifies that the *specific* query about facial recognition cannot be answered. However, it then provides general AI ethical concerns, which are related but not specific enough.\n",
      "    *   **Factual correctness:** Assumes the context indeed contained the listed general AI ethical concerns. The core statement about the lack of specific facial recognition information is accurate based on its own output.\n",
      "    *   **Comprehensiveness:** It's comprehensive in stating what *was* found (general AI ethics) and what *wasn't* (specific facial recognition ethics).\n",
      "    *   **Clarity and coherence:** Clear, concise, and easy to understand.\n",
      "    *   **Comparison to Reference Answer:** It goes beyond the simple \"I don't have enough information\" by providing the general AI ethical concerns that were found. This is a helpful addition for the user, but it deviates from the directness of the reference answer.\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "*   \"The provided context discusses general ethical and societal concerns related to AI, which include: Bias and Fairness, Transparency and Explainability, Privacy and Security.\"\n",
      "*   \"While the context mentions facial recognition as an application of AI, it does not specifically detail the ethical concerns *related to AI-powered facial recognition*. It only lists the general ethical concerns for AI as a whole.\"\n",
      "\n",
      "    *   **Relevance to the query:** Highly relevant. It correctly identifies the inability to answer the specific query. Crucially, it notes that \"facial recognition\" *was* mentioned in the context (as an application), but not with specific ethical concerns. This is a key distinction.\n",
      "    *   **Factual correctness:** Assumes the context contained the general concerns and mentioned facial recognition as an application. The core statement about lacking specific info is accurate.\n",
      "    *   **Comprehensiveness:** Very good. It explains what was found (general AI ethics, and facial recognition as an application) and what was missing (specific ethical concerns for facial recognition).\n",
      "    *   **Clarity and coherence:** Clear, well-structured, and provides a nuanced explanation.\n",
      "    *   **Comparison to Reference Answer:** Similar to the Vector response, it goes beyond the simple \"I don't have enough information.\" However, its explanation of *why* it couldn't answer (i.e., finding the term \"facial recognition\" but not its specific ethical concerns) is more precise and insightful than the Vector response.\n",
      "\n",
      "**3. Fusion response:**\n",
      "*   \"Based on the provided context, there is no information about the ethical concerns related to AI-powered facial recognition.\"\n",
      "*   \"The text discusses general ethical concerns of AI such as bias and fairness, transparency and explainability, privacy and security, job displacement, autonomy and control, and weaponization of AI, but it does not specifically mention facial recognition.\"\n",
      "\n",
      "    *   **Relevance to the query:** Highly relevant in its initial statement. It directly addresses the lack of specific information. It then provides the general AI ethical concerns.\n",
      "    *   **Factual correctness:** Assumes the context contained the general concerns. However, the concluding statement \"but it does not specifically mention facial recognition\" directly contradicts the BM25 response's claim that \"the context mentions facial recognition as an application of AI.\" If \"facial recognition\" *was* mentioned at all (even generally), then Fusion's final statement is inaccurate. Given the query, it's more likely that the term was present but not in the specific context of ethical concerns.\n",
      "    *   **Comprehensiveness:** Good in listing general AI concerns and stating the limitation.\n",
      "    *   **Clarity and coherence:** Very clear and concise.\n",
      "    *   **Comparison to Reference Answer:** This response is the most direct in its initial statement about lacking information, aligning very closely with the reference answer's intent. However, the potential factual inaccuracy in its concluding sentence (regarding the mention of \"facial recognition\") is a significant weakness.\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison and Which Approach Performed Best:**\n",
      "\n",
      "All three approaches correctly identified that the specific information about ethical concerns related to *AI-powered facial recognition* was not available in the context. This aligns with the spirit of the reference answer. However, their explanations for *why* and what *was* found differ.\n",
      "\n",
      "*   **Vector-based retrieval:** Performed well in identifying the general semantic topic (AI ethics) but lacked the precision to explain the absence of the *specific* sub-topic (facial recognition ethics). It couldn't tell if \"facial recognition\" was mentioned at all.\n",
      "*   **BM25 keyword retrieval:** Demonstrated a strong understanding of the query's specificity. Its ability to state that \"facial recognition\" *was* mentioned (as an application) but not with specific ethical concerns is a significant strength. This indicates that it successfully found the keyword but the surrounding context didn't provide the desired information. This is a very accurate and helpful explanation for the user.\n",
      "*   **Fusion retrieval:** Was the most direct in stating the lack of information, which is good. However, its final statement (\"it does not specifically mention facial recognition\") is problematic if the term *was* present in the context (as BM25 suggests). If the goal of fusion is to combine the strengths, it should ideally have leveraged the keyword presence while also noting the semantic mismatch for specific ethical concerns.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "For this particular query, the **BM25 keyword response performed best.**\n",
      "\n",
      "**Why:**\n",
      "\n",
      "1.  **Accuracy and Nuance:** BM25 provided the most accurate and nuanced explanation for *why* the specific query could not be answered. It correctly identified that the keyword \"facial recognition\" was present in the context (as an application of AI), but the associated information did not detail its specific ethical concerns. This level of detail is crucial for a RAG system, as it helps the user understand the limitations of the available data.\n",
      "2.  **Alignment with Retrieval Mechanism:** This explanation perfectly reflects how a keyword-based system would operate: it found the relevant keyword but the surrounding text didn't provide the specific context requested.\n",
      "3.  **Helpfulness:** By stating that \"facial recognition\" was mentioned, it gives the user more information than a simple \"not found\" or a general list of AI ethics. It implies that the context *is* about AI and mentions facial recognition, just not in the specific way the user asked.\n",
      "\n",
      "The Fusion response, while direct, suffered from a potential factual inaccuracy in its concluding statement. The Vector response was good at identifying general themes but lacked the precision of BM25 in explaining the absence of the specific entity's concerns.\n",
      "\n",
      "\n",
      "=== Evaluating Query 4/10 ===\n",
      "Query: How does AI contribute to personalized medicine?\n",
      "\n",
      "=== Comparing retrieval methods for query: How does AI contribute to personalized medicine? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: How does AI contribute to personalized medicine?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enhances treatment effectiveness and reduces adverse effects.\n",
      "\n",
      "=== BM25 Response ===\n",
      "AI contributes to personalized medicine by \"analyzing individual patient d\". The provided context cuts off the sentence at this point, so a full explanation is not available.\n",
      "\n",
      "=== Fusion Response ===\n",
      "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enhances treatment effectiveness and reduces adverse effects.\n",
      "\n",
      "=== Comparison ===\n",
      "This evaluation compares three RAG responses to the query \"How does AI contribute to personalized medicine?\" against a provided reference answer.\n",
      "\n",
      "**Reference Answer:**\n",
      "\"AI enables personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions to specific needs. This enhances treatment effectiveness and reduces adverse effects.\"\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response:**\n",
      "\"AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enhances treatment effectiveness and reduces adverse effects.\"\n",
      "\n",
      "*   **Relevance to the query:** Excellent. Directly and accurately answers how AI contributes.\n",
      "*   **Factual correctness:** Correct. The mechanisms and benefits described are accurate.\n",
      "*   **Comprehensiveness:** Excellent. It covers the key aspects: data analysis, prediction, tailoring interventions, and the resulting benefits (enhanced effectiveness, reduced adverse effects). It is almost identical to the reference answer.\n",
      "*   **Clarity and coherence:** Excellent. The response is clear, concise, and flows logically.\n",
      "*   **Comparison to Reference Answer:** This response is virtually identical to the reference answer, demonstrating a highly accurate and comprehensive understanding of the query's intent and the relevant information.\n",
      "\n",
      "**Strengths for this query:**\n",
      "*   Successfully identified and retrieved semantically relevant information that fully answers the query.\n",
      "*   Demonstrates a strong understanding of the underlying concepts, not just keyword matching.\n",
      "\n",
      "**Weaknesses for this query:**\n",
      "*   None apparent.\n",
      "\n",
      "---\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "\"AI contributes to personalized medicine by \"analyzing individual patient d\". The provided context cuts off the sentence at this point, so a full explanation is not available.\"\n",
      "\n",
      "*   **Relevance to the query:** Poor. While it starts with a relevant phrase, it fails to provide a complete answer.\n",
      "*   **Factual correctness:** Incomplete. The fragment provided (\"analyzing individual patient d\") is factually correct as far as it goes, but the truncation makes it useless.\n",
      "*   **Comprehensiveness:** Extremely Poor. It fails entirely to provide a comprehensive answer due to the truncated context.\n",
      "*   **Clarity and coherence:** Poor. The response explicitly states its failure to provide a full explanation, indicating a severe issue with the retrieved context.\n",
      "*   **Comparison to Reference Answer:** This response completely fails to match the comprehensiveness, clarity, or utility of the reference answer. It highlights a critical failure in retrieval or context window management.\n",
      "\n",
      "**Strengths for this query:**\n",
      "*   None apparent in the final output. BM25's strength is typically in precise keyword matching, but here it seems to have retrieved a snippet that was too short or poorly chunked, leading to a broken response.\n",
      "\n",
      "**Weaknesses for this query:**\n",
      "*   Highly susceptible to context window limitations or poor document chunking, leading to incomplete or nonsensical answers.\n",
      "*   Lacks semantic understanding, relying solely on keyword presence, which can lead to retrieving irrelevant or truncated snippets if the exact keywords are present but the surrounding context is not useful.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Fusion response:**\n",
      "\"AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enhances treatment effectiveness and reduces adverse effects.\"\n",
      "\n",
      "*   **Relevance to the query:** Excellent. Directly and accurately answers how AI contributes.\n",
      "*   **Factual correctness:** Correct. The mechanisms and benefits described are accurate.\n",
      "*   **Comprehensiveness:** Excellent. It covers the key aspects: data analysis, prediction, tailoring interventions, and the resulting benefits (enhanced effectiveness, reduced adverse effects). It is almost identical to the reference answer.\n",
      "*   **Clarity and coherence:** Excellent. The response is clear, concise, and flows logically.\n",
      "*   **Comparison to Reference Answer:** This response is virtually identical to the reference answer, demonstrating a highly accurate and comprehensive understanding of the query's intent and the relevant information.\n",
      "\n",
      "**Strengths for this query:**\n",
      "*   Successfully combined the strengths of both vector and keyword approaches (though in this specific case, it appears the vector component likely dominated or the BM25 component also found the correct full text).\n",
      "*   Provided a complete, accurate, and coherent answer.\n",
      "\n",
      "**Weaknesses for this query:**\n",
      "*   None apparent.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion: Which approach performed best for this query and why?**\n",
      "\n",
      "For this specific query, both the **Vector-based response** and the **Fusion response** performed equally and exceptionally well. They both provided a complete, accurate, and coherent answer that perfectly matched the reference answer. This indicates that for this query, the semantic understanding provided by vector retrieval (which is also a component of fusion retrieval) was highly effective in identifying the most relevant and comprehensive information.\n",
      "\n",
      "The **BM25 keyword response** performed the worst by a significant margin. It failed to provide a usable answer due to a truncated context, highlighting a critical vulnerability of pure keyword-based retrieval when the retrieved document chunks are not appropriately sized or when the exact keywords appear in an incomplete sentence.\n",
      "\n",
      "**Specific Strengths and Weaknesses for this Query:**\n",
      "\n",
      "*   **Vector-based Retrieval:**\n",
      "    *   **Strength:** Its ability to understand the semantic meaning of the query (\"How does AI contribute...\") allowed it to retrieve a comprehensive and relevant passage, even if the exact keywords weren't perfectly aligned. This led to a full and accurate answer.\n",
      "    *   **Weakness:** None observed for this query.\n",
      "\n",
      "*   **BM25 Keyword Retrieval:**\n",
      "    *   **Strength:** It did identify a relevant starting phrase (\"analyzing individual patient d\"), indicating it found a document containing relevant keywords.\n",
      "    *   **Weakness:** Its primary weakness was its inability to retrieve a complete and coherent piece of information. This could be due to the retrieved chunk being too small, or the relevant information being split across multiple chunks, which BM25 struggles to piece together semantically. It demonstrates a lack of robustness when context is fragmented or poorly managed.\n",
      "\n",
      "*   **Fusion Retrieval:**\n",
      "    *   **Strength:** In this case, it successfully leveraged its components (likely the vector component) to provide a perfect answer. It demonstrates the robustness of combining approaches, as even if one component struggles, the other might compensate.\n",
      "    *   **Weakness:** None observed for this query.\n",
      "\n",
      "In summary, for this query, the semantic understanding offered by vector-based methods (present in both Vector and Fusion approaches) was crucial for delivering a high-quality, comprehensive answer, while the pure keyword approach (BM25) failed due to context limitations.\n",
      "\n",
      "\n",
      "=== Evaluating Query 5/10 ===\n",
      "Query: Does the document mention any specific companies developing AI technology?\n",
      "\n",
      "=== Comparing retrieval methods for query: Does the document mention any specific companies developing AI technology? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: Does the document mention any specific companies developing AI technology?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "No, the document does not mention any specific companies developing AI technology. It discusses various aspects of AI, including its applications, ethical considerations, future trends, and governmental roles, but not specific company names.\n",
      "\n",
      "=== BM25 Response ===\n",
      "No, the document does not mention any specific companies developing AI technology.\n",
      "\n",
      "=== Fusion Response ===\n",
      "No, the document does not mention any specific companies developing AI technology.\n",
      "\n",
      "=== Comparison ===\n",
      "The user is asking for a comparison of three RAG responses against a reference answer, specifically focusing on their accuracy relative to the reference. The reference answer, \"I don't have enough information to answer that,\" is crucial as it indicates the system's inability to definitively answer the query.\n",
      "\n",
      "Let's evaluate each response:\n",
      "\n",
      "**Reference Answer:** \"I don't have enough information to answer that.\"\n",
      "*   **Implication:** The RAG system, for some reason (e.g., no relevant chunks retrieved, or inability to confidently assert a negative), could not confirm or deny the presence of specific companies. This is the ground truth for the *system's knowledge state*.\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response:**\n",
      "\"No, the document does not mention any specific companies developing AI technology. It discusses various aspects of AI, including its applications, ethical considerations, future trends, and governmental roles, but not specific company names.\"\n",
      "\n",
      "*   **Relevance to the query:** Directly answers the query.\n",
      "*   **Factual correctness (vs. Reference):** **Incorrect.** The response confidently states \"No,\" which directly contradicts the reference answer's assertion that it \"doesn't have enough information.\" This indicates overconfidence or a hallucinated negative assertion. If the system truly didn't have enough information, it should not have stated a definitive \"No.\"\n",
      "*   **Comprehensiveness:** High. It not only answers \"No\" but also provides helpful context about what *is* discussed in the document. This is a good practice for helpfulness, *if* the core \"No\" were accurate.\n",
      "*   **Clarity and coherence:** Excellent. The response is clear, well-structured, and easy to understand.\n",
      "\n",
      "---\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "\"No, the document does not mention any specific companies developing AI technology.\"\n",
      "\n",
      "*   **Relevance to the query:** Directly answers the query.\n",
      "*   **Factual correctness (vs. Reference):** **Incorrect.** Similar to the vector response, it confidently states \"No,\" which contradicts the reference answer's \"I don't have enough information.\" BM25's keyword-based nature might lead it to quickly conclude \"No\" if keywords like \"company\" or \"companies\" aren't found in proximity to \"AI technology,\" without deeper semantic understanding.\n",
      "*   **Comprehensiveness:** Low. It's a concise \"No\" without any additional context.\n",
      "*   **Clarity and coherence:** Good. It's clear and to the point.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Fusion response:**\n",
      "\"No, the document does not mention any specific companies developing AI technology.\"\n",
      "\n",
      "*   **Relevance to the query:** Directly answers the query.\n",
      "*   **Factual correctness (vs. Reference):** **Incorrect.** Again, it confidently states \"No,\" directly contradicting the reference answer. The fusion approach, which combines strengths, ideally should be more robust, but in this case, it seems to have inherited the same overconfidence or lack of nuance as the individual methods.\n",
      "*   **Comprehensiveness:** Low. It's a concise \"No\" without any additional context.\n",
      "*   **Clarity and coherence:** Good. It's clear and to the point.\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison and Which Approach Performed Best:**\n",
      "\n",
      "**Overall Performance Against Reference:**\n",
      "All three approaches **failed** to align with the reference answer. The reference answer indicates that the system *should not have been able to definitively answer* the query with a \"No.\" Instead, all three responses confidently asserted a negative. This is a critical failure in terms of factual correctness *from the system's perspective*. It suggests that the RAG system, regardless of the retrieval method, either:\n",
      "1.  **Hallucinated a negative:** Concluded \"No\" without sufficient evidence or when the confidence threshold for such a statement wasn't met.\n",
      "2.  **Failed to identify uncertainty:** Was unable to recognize that it lacked the information to provide a definitive answer, leading to an overconfident \"No.\"\n",
      "\n",
      "**Strengths and Weaknesses for this particular query:**\n",
      "\n",
      "*   **Vector-based retrieval:**\n",
      "    *   **Strength:** Its ability to provide additional context about what *is* in the document (applications, ethics, trends) demonstrates a more comprehensive understanding of the document's content, even if it couldn't find specific companies. This is generally a good trait for helpfulness.\n",
      "    *   **Weakness:** Its primary weakness, shared by all, is the **overconfident and incorrect assertion of \"No\"** when the system's true state (per the reference) was \"I don't have enough information.\"\n",
      "\n",
      "*   **BM25 keyword retrieval:**\n",
      "    *   **Strength:** Simplicity and conciseness.\n",
      "    *   **Weakness:** **Overconfident and incorrect assertion of \"No.\"** Its reliance on keywords might make it less robust in determining the *absence* of information, as it might simply not find the exact keywords, leading to a quick negative. It also lacks the contextual richness of the vector response.\n",
      "\n",
      "*   **Fusion retrieval:**\n",
      "    *   **Strength:** Ideally, fusion should combine the best of both, but in this case, it produced the same concise, overconfident, and incorrect \"No\" as BM25.\n",
      "    *   **Weakness:** **Overconfident and incorrect assertion of \"No.\"** It did not demonstrate any superior ability to handle the \"lack of information\" scenario compared to the other methods.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "None of the approaches performed well for this query when evaluated against the reference answer. All three exhibited a critical flaw by providing a definitive \"No\" when the system, according to the reference, lacked sufficient information to do so. This highlights a common challenge in RAG systems: distinguishing between \"the information is not present in the document\" and \"I cannot find the information in the document (and therefore cannot confirm its presence or absence).\"\n",
      "\n",
      "If forced to choose the \"least bad\" option, the **Vector-based response** is marginally better *in terms of its attempt at comprehensiveness and helpfulness* (by providing context about what *is* in the document), even though its core assertion is fundamentally incorrect relative to the reference. However, from a strict accuracy standpoint against the reference answer, all three are equally poor.\n",
      "\n",
      "\n",
      "=== Evaluating Query 6/10 ===\n",
      "Query: What is the role of AI in smart grids?\n",
      "\n",
      "=== Comparing retrieval methods for query: What is the role of AI in smart grids? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What is the role of AI in smart grids?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "AI optimizes energy management by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.\n",
      "\n",
      "=== BM25 Response ===\n",
      "AI optimizes energy management in smart cities by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.\n",
      "\n",
      "=== Fusion Response ===\n",
      "AI optimizes energy management in smart cities by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.\n",
      "\n",
      "=== Comparison ===\n",
      "Let's evaluate each response against the query \"What is the role of AI in smart grids?\" and the provided reference answer.\n",
      "\n",
      "**Reference Answer:** \"AI optimizes energy distribution in smart grids by enabling real-time monitoring, demand response, and integration of distributed energy resources. This enhances grid reliability, reduces energy waste, and supports renewable energy.\"\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response:**\n",
      "\"AI optimizes energy management by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.\"\n",
      "\n",
      "*   **Relevance to the query:** High. Directly addresses the role of AI in smart grids. It implicitly understands \"smart grids\" even without explicitly stating it in the first sentence, focusing on the core functions.\n",
      "*   **Factual correctness:** Correct. All points mentioned are valid roles of AI in smart grids.\n",
      "*   **Comprehensiveness:** Good. It covers key aspects like demand prediction (related to real-time monitoring/demand response), supply management (energy distribution), efficiency, grid stability (reliability), waste reduction, and renewable integration. It captures the essence of the reference answer's points, though using slightly different terminology (e.g., \"predicting demand\" vs. \"real-time monitoring, demand response\").\n",
      "*   **Clarity and coherence:** Excellent. The response is concise, clear, and flows well.\n",
      "\n",
      "*   **Comparison to Reference:**\n",
      "    *   **Strengths:** Accurately captures the core functions and benefits. It avoids introducing extraneous terms like \"smart cities,\" sticking precisely to the \"smart grids\" context. Its phrasing like \"predicting demand\" and \"managing supply\" effectively conveys the concepts of \"real-time monitoring,\" \"demand response,\" and \"energy distribution\" from the reference. \"Grid stability\" is a good synonym for \"grid reliability.\"\n",
      "    *   **Weaknesses:** Doesn't explicitly use the terms \"real-time monitoring\" or \"demand response,\" which are specific and important functionalities in smart grids, although \"predicting demand\" covers some of this ground.\n",
      "\n",
      "---\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "\"AI optimizes energy management in smart cities by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.\"\n",
      "\n",
      "*   **Relevance to the query:** High, but with a slight deviation. It correctly identifies the role of AI, but introduces \"in smart cities\" which is a broader context than the specific \"smart grids\" requested in the query. While smart grids are often components of smart cities, the query was more precise.\n",
      "*   **Factual correctness:** Correct. The information itself is accurate for AI's role.\n",
      "*   **Comprehensiveness:** Good. Content-wise, it's almost identical to the Vector-based response, covering similar aspects.\n",
      "*   **Clarity and coherence:** Excellent. Clear and concise.\n",
      "\n",
      "*   **Comparison to Reference:**\n",
      "    *   **Strengths:** Provides accurate and relevant information, largely mirroring the content of the Vector response and aligning well with the reference's core points.\n",
      "    *   **Weaknesses:** The inclusion of \"in smart cities\" is a minor but notable inaccuracy regarding the query's specific scope (\"smart grids\"). This suggests BM25 might have retrieved documents where \"AI,\" \"energy management,\" and \"smart cities\" were strongly co-occurring, leading to a slightly less precise answer for \"smart grids.\"\n",
      "\n",
      "---\n",
      "\n",
      "**3. Fusion response:**\n",
      "\"AI optimizes energy management in smart cities by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.\"\n",
      "\n",
      "*   **Relevance to the query:** High, but also includes the \"in smart cities\" deviation, identical to the BM25 response.\n",
      "*   **Factual correctness:** Correct.\n",
      "*   **Comprehensiveness:** Good. Identical content to the BM25 response.\n",
      "*   **Clarity and coherence:** Excellent.\n",
      "\n",
      "*   **Comparison to Reference:**\n",
      "    *   **Strengths:** Similar to BM25, it provides good, relevant information.\n",
      "    *   **Weaknesses:** It inherits the \"in smart cities\" imprecision from the BM25 response. This indicates that for this query, the fusion approach either didn't effectively mitigate the keyword-based imprecision, or both retrieval methods contributed to this broader context.\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison and Which Approach Performed Best:**\n",
      "\n",
      "All three responses are remarkably similar in their core content, suggesting that the underlying knowledge base or the generation model might have a strong influence on the output, or that the top retrieved documents across methods were very similar. However, the subtle difference in precision makes one stand out.\n",
      "\n",
      "1.  **Relevance to the query:**\n",
      "    *   **Vector-based:** Best. Directly addresses \"smart grids\" without introducing broader contexts.\n",
      "    *   **BM25 & Fusion:** Good, but slightly less precise due to the inclusion of \"in smart cities.\"\n",
      "\n",
      "2.  **Factual correctness:** All three are factually correct.\n",
      "\n",
      "3.  **Comprehensiveness:** All three are good and comparable. They cover the main aspects mentioned in the reference answer (optimization, demand/supply, renewables, reliability/stability, waste reduction).\n",
      "\n",
      "4.  **Clarity and coherence:** All three are excellent in terms of clarity and coherence.\n",
      "\n",
      "**Conclusion: The Vector-based response performed best for this query.**\n",
      "\n",
      "**Why:**\n",
      "\n",
      "The **Vector-based response** is superior because it demonstrates a more precise understanding of the query's scope. The query specifically asked about \"smart grids,\" and the Vector response focused solely on that, aligning perfectly with the reference answer's specificity. The BM25 and Fusion responses, by including \"in smart cities,\" introduced a slightly broader and less precise context. While smart grids are part of smart cities, the query's specificity was \"smart grids,\" and the Vector approach adhered to that. This suggests that for this particular query, the semantic understanding of the vector approach was more accurate in distinguishing between \"smart grids\" and the broader \"smart cities\" context, leading to a more focused and accurate answer relative to the query and the reference.\n",
      "\n",
      "\n",
      "=== Evaluating Query 7/10 ===\n",
      "Query: Can AI write a complete, original novel?\n",
      "\n",
      "=== Comparing retrieval methods for query: Can AI write a complete, original novel? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: Can AI write a complete, original novel?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "Based on the provided context, AI is used to \"write articles, generate content, and create scripts.\" It is also stated that AI-powered systems can \"generate art, music, and literature\" and \"create original content, including images, text, and music.\"\n",
      "\n",
      "However, the context does not explicitly state that AI can write a complete, original novel. It mentions generating \"text\" and \"literature\" but does not specify the length or complexity of such works to the extent of a full novel.\n",
      "\n",
      "=== BM25 Response ===\n",
      "Based on the provided context, AI is capable of generating literature and creating original content, including text. It is also used to write articles, generate content, and create scripts. However, the context does not explicitly state that AI can write a complete, original novel.\n",
      "\n",
      "=== Fusion Response ===\n",
      "Based on the provided context, AI is used to \"write articles, generate content, and create scripts.\" It can also \"generate literature\" and \"create original content, including text.\" However, the context does not explicitly state that AI can write a complete, original novel. It mentions AI-powered writing tools can \"assist writers with research, editing, and content generation.\"\n",
      "\n",
      "=== Comparison ===\n",
      "As an expert evaluator of RAG systems, I will compare the three responses based on the provided criteria and against the reference answer.\n",
      "\n",
      "**Query:** Can AI write a complete, original novel?\n",
      "\n",
      "**Reference Answer:** I don't have enough information to answer that.\n",
      "\n",
      "---\n",
      "\n",
      "**Evaluation Criteria:**\n",
      "\n",
      "1.  **Relevance to the query:** How well does the response address the specific question asked?\n",
      "2.  **Factual correctness:** Is the information presented accurate based on the implied context?\n",
      "3.  **Comprehensiveness:** Does the response provide a sufficiently detailed and complete answer based on the available context?\n",
      "4.  **Clarity and coherence:** Is the response easy to understand and logically structured?\n",
      "5.  **Accuracy against Reference Answer:** How well does the response align with the spirit and conclusion of the reference answer?\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison:**\n",
      "\n",
      "**1. Vector-based response:**\n",
      "*   **Relevance:** High. It directly addresses the query by stating what the context *does* and *does not* say about AI writing novels.\n",
      "*   **Factual Correctness:** Appears correct based on the implied context. It accurately reports what the context states about AI's content generation capabilities.\n",
      "*   **Comprehensiveness:** Good. It lists several types of content AI can generate (articles, scripts, art, music, literature, text). Crucially, it provides a clear explanation for *why* the context is insufficient for a novel: \"does not specify the length or complexity of such works to the extent of a full novel.\" This adds valuable nuance.\n",
      "*   **Clarity and Coherence:** Excellent. The response is well-structured, clear, and easy to understand.\n",
      "*   **Accuracy against Reference Answer:** Very high. It perfectly captures the essence of \"I don't have enough information to answer that\" by explaining the limitations of the context regarding the specific query.\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "*   **Relevance:** High. It directly addresses the query and its limitations.\n",
      "*   **Factual Correctness:** Appears correct based on the implied context. It accurately reports what the context states.\n",
      "*   **Comprehensiveness:** Moderate. It lists some content types (literature, original content, text, articles, scripts). However, it is less comprehensive than the Vector response in explaining *why* the context is insufficient, simply stating \"does not explicitly state.\" It lacks the detailed reasoning about length/complexity.\n",
      "*   **Clarity and Coherence:** Good. It's concise and to the point.\n",
      "*   **Accuracy against Reference Answer:** High. It aligns well with the reference answer by concluding that the context doesn't explicitly answer the specific query.\n",
      "\n",
      "**3. Fusion response:**\n",
      "*   **Relevance:** High. It directly addresses the query and its limitations.\n",
      "*   **Factual Correctness:** Appears correct based on the implied context. It accurately reports what the context states.\n",
      "*   **Comprehensiveness:** Very Good. It lists content types similar to the others (articles, scripts, literature, original content, text). **Crucially, it adds the detail that AI-powered writing tools can \"assist writers with research, editing, and content generation.\"** This is a highly relevant piece of information about AI's role in writing that the other two responses missed, suggesting a broader retrieval of relevant context. While it doesn't explain *why* the context is insufficient as clearly as the Vector response (it just says \"does not explicitly state\"), the added detail about assistance makes it more comprehensive about AI's overall writing capabilities as presented in the context.\n",
      "*   **Clarity and Coherence:** Good. It's clear and well-structured.\n",
      "*   **Accuracy against Reference Answer:** High. It aligns well with the reference answer by concluding that the context doesn't explicitly answer the specific query.\n",
      "\n",
      "---\n",
      "\n",
      "**Which approach performed best for this query and why?**\n",
      "\n",
      "All three responses are strong and effectively align with the \"I don't have enough information\" sentiment of the reference answer. They all correctly identify that the provided context does not explicitly state that AI can write a complete, original novel.\n",
      "\n",
      "However, the **Fusion response performed best** for this particular query.\n",
      "\n",
      "**Specific Strengths and Weaknesses:**\n",
      "\n",
      "*   **Vector-based retrieval:**\n",
      "    *   **Strength:** Provided the most insightful explanation for *why* the context was insufficient (\"does not specify the length or complexity\"). This demonstrates a deeper understanding of the semantic gap.\n",
      "    *   **Weakness:** Missed the additional relevant detail about AI assisting writers.\n",
      "*   **BM25 keyword retrieval:**\n",
      "    *   **Strength:** Concise and directly to the point.\n",
      "    *   **Weakness:** Least comprehensive in terms of the details extracted from the context and offered the least explanation for the contextual limitation.\n",
      "*   **Fusion retrieval:**\n",
      "    *   **Strength:** Successfully combined the strengths of both approaches. It retrieved a good range of general content generation capabilities *and* uniquely included the highly relevant detail about AI assisting writers with research, editing, and content generation. This suggests a more robust and comprehensive retrieval of all relevant information from the context, even if it doesn't directly answer the \"complete novel\" part. This additional context enriches the overall answer.\n",
      "    *   **Weakness:** While it correctly states the context doesn't explicitly answer, it doesn't elaborate on *why* (e.g., length/complexity) as clearly as the Vector response.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The Fusion response is superior because it provided the most comprehensive set of relevant facts from the underlying context, including a crucial detail about AI's assistive role in writing that the other methods missed. While the Vector response offered a slightly better *explanation* for the limitation, the Fusion response's ability to retrieve a broader, more nuanced picture of AI's capabilities *as described in the context* makes it the most informative and well-rounded answer, while still accurately reflecting the lack of definitive information for the specific query. This demonstrates the value of combining semantic and keyword approaches to ensure a wider net of relevant information is cast.\n",
      "\n",
      "\n",
      "=== Evaluating Query 8/10 ===\n",
      "Query: What is a 'cobot'?\n",
      "\n",
      "=== Comparing retrieval methods for query: What is a 'cobot'? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What is a 'cobot'?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "Based on the provided context, \"cobots\" are industrial robots that are enhanced by AI, allowing them to work alongside humans in collaborative settings.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The provided context does not contain information about what a 'cobot' is.\n",
      "\n",
      "=== Fusion Response ===\n",
      "The provided context does not contain information about what a 'cobot' is.\n",
      "\n",
      "=== Comparison ===\n",
      "This evaluation will compare the three retrieval approaches based on the provided responses and the reference answer.\n",
      "\n",
      "**Reference Answer Analysis:**\n",
      "The reference answer, \"It mentions collaborative settings (cobots) in industrial robots,\" is crucial. It indicates that the underlying context *does* contain information about 'cobots', specifically linking them to \"collaborative settings\" and \"industrial robots.\" This means any response claiming the context *lacks* information is incorrect in its assessment of the context's content.\n",
      "\n",
      "---\n",
      "\n",
      "**1. Vector-based response:**\n",
      "*   **Response:** \"Based on the provided context, 'cobots' are industrial robots that are enhanced by AI, allowing them to work alongside humans in collaborative settings.\"\n",
      "*   **Relevance to the query:** Excellent. Directly answers \"What is a 'cobot'?\"\n",
      "*   **Factual correctness:** Excellent. The definition provided (\"industrial robots,\" \"work alongside humans,\" \"collaborative settings\") aligns perfectly with the core concept of cobots and the hints from the reference answer. The \"enhanced by AI\" part is also generally true for modern cobots, adding a layer of detail.\n",
      "*   **Comprehensiveness:** Good. Provides a concise and accurate definition.\n",
      "*   **Clarity and coherence:** Excellent. The language is clear, concise, and easy to understand.\n",
      "*   **Comparison to Reference Answer:** This response successfully identified and extracted the information that the reference answer implies is present in the context. It correctly linked 'cobots' to 'industrial robots' and 'collaborative settings'.\n",
      "\n",
      "**Strengths:**\n",
      "*   Successfully understood the semantic meaning of \"cobot\" and retrieved relevant information even if the exact phrase \"what is a cobot\" wasn't a perfect keyword match in the source.\n",
      "*   Provided an accurate and relevant definition, demonstrating a strong grasp of the underlying context.\n",
      "\n",
      "**Weaknesses:**\n",
      "*   None apparent for this specific query.\n",
      "\n",
      "---\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "*   **Response:** \"The provided context does not contain information about what a 'cobot' is.\"\n",
      "*   **Relevance to the query:** Poor. It fails to answer the query.\n",
      "*   **Factual correctness:** Incorrect, based on the reference answer. The reference answer explicitly states that the context *does* mention \"collaborative settings (cobots) in industrial robots.\" This indicates that BM25 failed to retrieve the relevant document(s) or passages.\n",
      "*   **Comprehensiveness:** None.\n",
      "*   **Clarity and coherence:** Clear in its failure to find information.\n",
      "*   **Comparison to Reference Answer:** Directly contradicts the reference answer. It incorrectly asserts the absence of information that the reference answer confirms is present.\n",
      "\n",
      "**Strengths:**\n",
      "*   Clearly states its inability to find information, which can be useful if the information genuinely isn't there.\n",
      "\n",
      "**Weaknesses:**\n",
      "*   Failed to retrieve relevant information, likely due to its reliance on exact keyword matching. The term \"cobot\" might not have been a prominent keyword, or the relevant information was phrased differently, which BM25 couldn't semantically understand.\n",
      "*   Provided a factually incorrect assessment of the context's content.\n",
      "\n",
      "---\n",
      "\n",
      "**3. Fusion response:**\n",
      "*   **Response:** \"The provided context does not contain information about what a 'cobot' is.\"\n",
      "*   **Relevance to the query:** Poor. It fails to answer the query.\n",
      "*   **Factual correctness:** Incorrect, based on the reference answer. Similar to BM25, it incorrectly claims the absence of information that the reference answer confirms is present.\n",
      "*   **Comprehensiveness:** None.\n",
      "*   **Clarity and coherence:** Clear in its failure to find information.\n",
      "*   **Comparison to Reference Answer:** Directly contradicts the reference answer. It incorrectly asserts the absence of information that the reference answer confirms is present.\n",
      "\n",
      "**Strengths:**\n",
      "*   None apparent for this specific query, as it performed identically to BM25.\n",
      "\n",
      "**Weaknesses:**\n",
      "*   Despite being a \"fusion\" approach, it failed to leverage the strengths of vector retrieval (which successfully found the information, as shown by the vector-based response). This suggests a flaw in the fusion logic for this query, perhaps giving too much weight to the keyword component's negative result, or failing to properly combine the results.\n",
      "*   Provided a factually incorrect assessment of the context's content.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion: Which approach performed best and why?**\n",
      "\n",
      "For this query, the **Vector-based retrieval** approach performed by far the best.\n",
      "\n",
      "**Why:**\n",
      "1.  **Semantic Understanding:** The query \"What is a 'cobot'?\" benefits greatly from semantic understanding. While \"cobot\" might be a specific term, the definition often involves concepts like \"collaborative,\" \"human-robot interaction,\" and \"industrial robots.\" Vector-based retrieval excels at understanding the meaning and context of words, allowing it to retrieve documents that semantically relate to \"cobot\" even if the exact phrase isn't heavily present or if the definition is spread across different parts of the text.\n",
      "2.  **Alignment with Reference Answer:** The vector-based response successfully extracted the information that the reference answer indicated was present in the context. It correctly identified the key characteristics of a cobot.\n",
      "3.  **Overcoming Keyword Limitations:** BM25, being a keyword-based method, likely failed because the exact term \"cobot\" or its direct definition might not have been prominent enough for a high score, or the relevant information was described using synonyms or related concepts that BM25 couldn't bridge.\n",
      "4.  **Fusion Failure:** The most surprising outcome is the failure of the Fusion approach. Since the vector component *did* find the information, the fusion logic must have somehow overridden or discarded this correct retrieval, perhaps due to a strong negative signal from the BM25 component or an ineffective weighting strategy. This highlights that fusion isn't always a guaranteed improvement and its implementation details matter significantly.\n",
      "\n",
      "In summary, the vector-based approach demonstrated superior ability to understand the query's intent and the semantic content of the underlying context, leading to an accurate and relevant answer, while the keyword-based and fusion approaches failed to retrieve the available information.\n",
      "\n",
      "\n",
      "=== Evaluating Query 9/10 ===\n",
      "Query: What is Direct Air Capture (DAC) used for?\n",
      "\n",
      "=== Comparing retrieval methods for query: What is Direct Air Capture (DAC) used for? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What is Direct Air Capture (DAC) used for?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The provided context mentions \"carbon capture and storage\" as something AI supports for climate change mitigation, but it does not specifically mention or describe Direct Air Capture (DAC) or its uses.\n",
      "\n",
      "=== BM25 Response ===\n",
      "Based on the provided context, there is no information about what Direct Air Capture (DAC) is used for.\n",
      "\n",
      "=== Fusion Response ===\n",
      "The provided context mentions \"carbon capture and storage\" as a method AI supports for climate change mitigation, but it does not specifically mention or define \"Direct Air Capture (DAC)\".\n",
      "\n",
      "=== Comparison ===\n",
      "This evaluation compares the performance of three RAG retrieval approaches against a reference answer for the query \"What is Direct Air Capture (DAC) used for?\".\n",
      "\n",
      "**Reference Answer:** \"DAC technology removes CO2 directly from the atmosphere. The captured CO2 can be stored or used in various applications.\"\n",
      "\n",
      "---\n",
      "\n",
      "**Evaluation of Each Response:**\n",
      "\n",
      "**1. Vector-based response:**\n",
      "*   \"The provided context mentions \"carbon capture and storage\" as something AI supports for climate change mitigation, but it does not specifically mention or describe Direct Air Capture (DAC) or its uses.\"\n",
      "    *   **Relevance to the query:** Partially relevant. It acknowledges the query's topic (DAC) and identifies a broader, related concept (\"carbon capture and storage\") that was present in its context. However, it ultimately states the specific answer is missing.\n",
      "    *   **Factual correctness:** Correct in stating that the *provided context* (as seen by the RAG system) did not contain the specific information about DAC's uses. It correctly identifies a related term.\n",
      "    *   **Comprehensiveness:** Not comprehensive regarding the query. It's a negative answer, indicating the information is absent.\n",
      "    *   **Clarity and coherence:** Clear and coherent.\n",
      "    *   **Comparison to Reference Answer:** Fails to provide the information present in the reference answer. It correctly identifies a related concept but cannot extract the specific details about DAC's uses.\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "*   \"Based on the provided context, there is no information about what Direct Air Capture (DAC) is used for.\"\n",
      "    *   **Relevance to the query:** Directly addresses the query by stating the information is absent.\n",
      "    *   **Factual correctness:** Correct in stating that the *provided context* lacked the specific information.\n",
      "    *   **Comprehensiveness:** Not comprehensive regarding the query. It's a concise negative answer.\n",
      "    *   **Clarity and coherence:** Clear and concise.\n",
      "    *   **Comparison to Reference Answer:** Fails to provide the information present in the reference answer. It offers no additional context or related terms, simply stating the absence of a direct keyword match.\n",
      "\n",
      "**3. Fusion response:**\n",
      "*   \"The provided context mentions \"carbon capture and storage\" as a method AI supports for climate change mitigation, but it does not specifically mention or define \"Direct Air Capture (DAC)\".\"\n",
      "    *   **Relevance to the query:** Partially relevant. Similar to the Vector response, it acknowledges the query and points to a related concept (\"carbon capture and storage\") found in the context, while clearly stating the specific information about DAC is missing.\n",
      "    *   **Factual correctness:** Correct in stating that the *provided context* lacked the specific answer. It accurately identifies a related term.\n",
      "    *   **Comprehensiveness:** Not comprehensive regarding the query. It's a negative answer, indicating the information is absent.\n",
      "    *   **Clarity and coherence:** Clear and coherent.\n",
      "    *   **Comparison to Reference Answer:** Fails to provide the information present in the reference answer. However, it provides the most informative negative answer by identifying a related concept from the context.\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison and Best Approach:**\n",
      "\n",
      "For this particular query, **all three approaches failed to provide the correct answer as per the reference answer.** This indicates a fundamental limitation in the retrieved context for all systems  the relevant information about DAC's uses was not found or was not specific enough in the documents they accessed.\n",
      "\n",
      "However, if we evaluate how well each system *communicates the absence of information* and provides any *related context*, the performance differs:\n",
      "\n",
      "*   **BM25 Keyword Retrieval (Weakest):**\n",
      "    *   **Strengths:** Very direct and unambiguous in stating the lack of information.\n",
      "    *   **Weaknesses:** Provides no additional context or related terms that might have been present in the documents. It's a blunt \"no information\" response, suggesting it found no direct keyword matches that could lead to an answer. This approach is highly dependent on exact keyword presence.\n",
      "\n",
      "*   **Vector-based Retrieval (Better):**\n",
      "    *   **Strengths:** Demonstrates semantic understanding by identifying a related concept (\"carbon capture and storage\") that was present in its context, even if the specific DAC information was missing. This shows it's trying to find *something* semantically relevant.\n",
      "    *   **Weaknesses:** Still fails to retrieve the direct answer. The context it found was too broad or not specific enough to answer the query.\n",
      "\n",
      "*   **Fusion Retrieval (Best among the three for this query):**\n",
      "    *   **Strengths:** This approach combines the strengths of both. It clearly states the absence of specific information about DAC (like BM25) but also leverages semantic understanding to identify and mention the related concept (\"carbon capture and storage\") that was present in the context (like Vector). This provides the most informative negative answer, indicating that while the direct answer wasn't found, related concepts were present.\n",
      "    *   **Weaknesses:** Ultimately, it still fails to provide the direct answer to the query, just like the other two. The \"fusion\" here helped in providing a more nuanced \"no answer\" response, but not an actual answer.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "For the query \"What is Direct Air Capture (DAC) used for?\", **none of the approaches performed well in terms of answering the query correctly** when compared to the reference answer. All three responses indicate that the information was not found in their respective contexts.\n",
      "\n",
      "However, if forced to choose the \"best\" approach based on how they handled this failure, **Fusion retrieval performed best**. It provided the most comprehensive and informative \"negative\" response by acknowledging the query, stating the absence of specific information, and also pointing to a related concept that *was* found in the context. This suggests a more robust understanding of the query's domain, even when the precise answer is unavailable. The Vector-based approach was a close second for similar reasons, while BM25 was the least helpful as it simply stated the absence of information without any additional context.\n",
      "\n",
      "\n",
      "=== Evaluating Query 10/10 ===\n",
      "Query: Is AI currently being used to control nuclear weapons systems?\n",
      "\n",
      "=== Comparing retrieval methods for query: Is AI currently being used to control nuclear weapons systems? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: Is AI currently being used to control nuclear weapons systems?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The provided context discusses the \"potential use of AI in autonomous weapons systems\" and the need for \"international discussions and regulations\" to address the risks associated with \"AI-powered weapons.\" However, it does not state whether AI is currently being used to control nuclear weapons systems.\n",
      "\n",
      "=== BM25 Response ===\n",
      "Based on the provided context, there is no information stating that AI is currently being used to control nuclear weapons systems. The text mentions the \"potential use of AI in autonomous weapons systems\" and the need for \"International discussions and regulations\" regarding \"AI-powered weapons,\" but it does not confirm current deployment for nuclear weapons.\n",
      "\n",
      "=== Fusion Response ===\n",
      "Based on the provided context, there is no information stating that AI is currently being used to control nuclear weapons systems. The text mentions the \"potential use of AI in autonomous weapons systems\" and the need for \"international discussions and regulations to address the risks associated with AI-powered weapons.\"\n",
      "\n",
      "=== Comparison ===\n",
      "This is an excellent example where all three retrieval approaches, when combined with a capable Language Model (LLM), produce very similar and highly accurate responses.\n",
      "\n",
      "**Reference Answer:** \"I don't have enough information to answer that.\"\n",
      "\n",
      "**Overall Evaluation against Reference Answer:**\n",
      "All three RAG responses effectively convey the same core message as the reference answer: the information needed to directly answer the query (\"Is AI currently being used to control nuclear weapons systems?\") is not present in the provided context. However, they go a step further, which is a significant strength for a RAG system. Instead of just stating \"I don't know,\" they explain *why* they don't know by summarizing what the context *does* discuss (potential use, autonomous weapons, regulations). This provides valuable grounding and context to the user.\n",
      "\n",
      "---\n",
      "\n",
      "**Detailed Comparison:**\n",
      "\n",
      "**1. Vector-based response:**\n",
      "*   **Relevance to the query:** High. Directly addresses the query by stating what the context *does* and *does not* contain regarding AI and weapons.\n",
      "*   **Factual correctness:** Excellent. Accurately reflects that the context discusses \"potential use\" and \"autonomous weapons\" but not \"current use\" in \"nuclear weapons systems.\"\n",
      "*   **Comprehensiveness:** Good. It clearly outlines the scope of the context's discussion (potential use, autonomous systems, regulations, risks) while highlighting the absence of the specific information requested.\n",
      "*   **Clarity and coherence:** Excellent. The language is clear, concise, and easy to understand.\n",
      "*   **Strengths for this query:** Clearly distinguishes between \"potential\" and \"current\" use, and between \"autonomous weapons systems\" generally and \"nuclear weapons systems\" specifically. Its explicit statement \"it does not state whether AI is currently being used to control nuclear weapons systems\" is very direct.\n",
      "*   **Weaknesses for this query:** None significant.\n",
      "\n",
      "**2. BM25 keyword response:**\n",
      "*   **Relevance to the query:** High. Similar to the vector response, it directly addresses the query by explaining the limitations of the context.\n",
      "*   **Factual correctness:** Excellent. Correctly states the context's content and its lack of information on current nuclear weapon control by AI.\n",
      "*   **Comprehensiveness:** Good. Covers the same key points as the vector response regarding the context's content (potential use, autonomous systems, regulations).\n",
      "*   **Clarity and coherence:** Excellent. Very clear and well-structured.\n",
      "*   **Strengths for this query:** Its phrasing \"it does not confirm current deployment for nuclear weapons\" is very precise and directly answers the implied \"yes/no\" nature of the query with a \"no, not confirmed by context.\"\n",
      "*   **Weaknesses for this query:** None significant.\n",
      "\n",
      "**3. Fusion response:**\n",
      "*   **Relevance to the query:** High. Directly addresses the query and explains the context's limitations.\n",
      "*   **Factual correctness:** Excellent. Accurately reflects the content and omissions of the context.\n",
      "*   **Comprehensiveness:** Good. It covers the essential points (potential use, autonomous systems, regulations, risks). Its phrasing \"international discussions and regulations to address the risks associated with AI-powered weapons\" is perhaps the most fluid and complete summary of that specific aspect.\n",
      "*   **Clarity and coherence:** Excellent. Very clear and concise.\n",
      "*   **Strengths for this query:** Concise and to the point. The summary of the context's content is well-integrated.\n",
      "*   **Weaknesses for this query:** Marginally less explicit in its final negative confirmation compared to the other two, simply stating \"there is no information stating that AI is currently being used...\" without adding a concluding phrase like \"it does not state...\" or \"it does not confirm current deployment.\" This is a very minor point, however.\n",
      "\n",
      "---\n",
      "\n",
      "**Which approach performed best for this query and why?**\n",
      "\n",
      "For this specific query, **all three approaches performed exceptionally well and are remarkably similar in their output.** They all correctly identified that the context did not contain the specific information requested (current use of AI in *nuclear* weapons) while accurately summarizing what the context *did* discuss (potential use in *autonomous* weapons and the need for regulations).\n",
      "\n",
      "If forced to pick a \"best\" based on the *slightest* nuance:\n",
      "*   The **Vector-based response** and **BM25 keyword response** have a slight edge due to their concluding phrases (\"it does not state whether AI is currently being used to control nuclear weapons systems\" and \"it does not confirm current deployment for nuclear weapons,\" respectively). These phrases provide a very direct and explicit negative confirmation, which aligns perfectly with the \"yes/no\" nature of the query.\n",
      "*   The **Fusion response** is also excellent and very close, but its concluding statement is slightly less emphatic in its negative confirmation.\n",
      "\n",
      "Given the high quality and similarity, this query doesn't strongly differentiate the retrieval methods themselves, but rather showcases the LLM's ability to synthesize information (or lack thereof) effectively from the retrieved context. It suggests that the relevant information (or the absence of it) was easily discoverable by all three methods.\n",
      "\n",
      "**Conclusion:** All three responses are highly effective and superior to the simple reference answer because they provide grounding and explain *why* the direct answer isn't available. The differences between them are minimal, but the **Vector-based** and **BM25 keyword** responses are marginally more explicit in their negative confirmation.\n",
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "Based on the provided summaries across 10 diverse queries, we can draw a comprehensive comparison of Vector-based, BM25, and Fusion retrieval approaches. While the summaries don't provide granular scores, they offer valuable insights into the general performance characteristics and the types of queries each method handles effectively.\n",
      "\n",
      "### 1. Vector-based Retrieval (Semantic Similarity)\n",
      "\n",
      "*   **How it works:** Converts queries and documents into numerical vectors (embeddings) and finds documents whose vectors are \"closest\" to the query vector in a high-dimensional space. Focuses on the meaning and context.\n",
      "*   **Types of Queries where it performs best:**\n",
      "    *   **Conceptual/Abstract Queries:** Queries that require understanding the underlying meaning rather than just keyword matching (e.g., \"What is 'Explainable AI' and why is it considered important?\", \"How does AI contribute to personalized medicine?\").\n",
      "    *   **Queries with Synonyms or Rephrasing:** When the user uses different words than those in the document but means the same thing.\n",
      "    *   **Broad Topics:** Good for retrieving documents that discuss a topic generally, even if specific keywords aren't present.\n",
      "*   **Overall Strengths:**\n",
      "    *   **Semantic Understanding:** Excels at grasping the intent behind a query, even if the exact keywords aren't used.\n",
      "    *   **Handles Vocabulary Mismatch:** Overcomes the problem where the user's language differs from the document's.\n",
      "    *   **Improved Recall:** Can retrieve relevant documents that BM25 might miss due to keyword absence.\n",
      "*   **Overall Weaknesses:**\n",
      "    *   **Less Precise for Specific Facts:** Can sometimes be too broad, potentially retrieving conceptually related but factually irrelevant documents if the embeddings aren't highly tuned.\n",
      "    *   **Performance Dependency:** Heavily reliant on the quality of the embedding model.\n",
      "    *   **\"Hallucination\" Risk (in RAG context):** If the retrieved context is too broad or semantically similar but factually incorrect, the LLM might generate plausible but wrong answers.\n",
      "\n",
      "### 2. BM25 Keyword Retrieval (Keyword Matching)\n",
      "\n",
      "*   **How it works:** A statistical method that ranks documents based on the frequency and inverse document frequency of query terms within the documents, along with document length normalization. Focuses on exact word matches.\n",
      "*   **Types of Queries where it performs best:**\n",
      "    *   **Specific Factual Lookups:** Queries asking for precise information, names, acronyms, or definitions (e.g., \"What is a 'cobot'?\", \"What is Direct Air Capture (DAC) used for?\").\n",
      "    *   **Queries with Unique Identifiers:** When the query contains specific terms that are likely to appear verbatim in the relevant documents.\n",
      "    *   **Identifying Absence of Information:** Can be effective in determining if a specific keyword or phrase is *not* present in the document collection (e.g., \"Does the document mention any specific companies developing AI technology?\").\n",
      "*   **Overall Strengths:**\n",
      "    *   **Precision for Keyword-Rich Queries:** Highly effective when the query terms are explicitly present in the relevant documents.\n",
      "    *   **Fast and Transparent:** Computationally efficient and easy to understand why a document was retrieved.\n",
      "    *   **Good for \"Negative\" Queries:** Can reliably indicate if a specific piece of information (identified by keywords) is absent.\n",
      "*   **Overall Weaknesses:**\n",
      "    *   **Vocabulary Mismatch:** Fails if the user uses synonyms or rephrases the query, even if the meaning is the same.\n",
      "    *   **Lack of Semantic Understanding:** Cannot infer relationships between words or understand context beyond exact matches.\n",
      "    *   **Poor for Conceptual Queries:** Struggles with abstract or general questions that don't rely on specific keywords.\n",
      "\n",
      "### 3. Fusion Retrieval (Combination of Both)\n",
      "\n",
      "*   **How it works:** Combines the results from both Vector-based and BM25 retrieval methods, often using techniques like Reciprocal Rank Fusion (RRF) or weighted sum, to produce a final ranked list of documents.\n",
      "*   **Types of Queries where it performs best:**\n",
      "    *   **General-Purpose/Diverse Queries:** Excels when the types of queries are varied, encompassing both conceptual and factual questions.\n",
      "    *   **Robustness:** Provides a more consistent and reliable performance across a wide range of query types, mitigating the weaknesses of individual methods.\n",
      "    *   **Queries where both semantic and keyword signals are important:** For example, a query that is conceptual but also contains a specific term.\n",
      "*   **Overall Strengths:**\n",
      "    *   **Balances Precision and Recall:** Leverages BM25 for precise keyword matches and Vector for semantic understanding, leading to a more comprehensive and relevant set of retrieved documents.\n",
      "    *   **Increased Robustness:** Less prone to catastrophic failure on specific query types. If one method performs poorly, the other can often compensate.\n",
      "    *   **Improved Overall Performance:** As seen in Query 10 (\"Is AI currently being used to control nuclear weapons systems?\"), fusion often performs as well as the best individual method, and sometimes better, by combining their strengths. The summary for Query 10 explicitly states all three approaches produced \"very similar and highly accurate responses,\" suggesting fusion maintained high performance.\n",
      "*   **Overall Weaknesses:**\n",
      "    *   **Increased Complexity:** More complex to implement and tune, especially regarding the weighting or fusion strategy.\n",
      "    *   **Marginal Gains:** For some very specific query types where one method is overwhelmingly superior, the fusion might offer only marginal gains or even slight degradation if not tuned correctly.\n",
      "\n",
      "### How Fusion Retrieval Balances the Trade-offs\n",
      "\n",
      "Fusion retrieval directly addresses the inherent trade-offs between BM25's precision (for keywords) and Vector's recall (for semantics).\n",
      "\n",
      "*   **Mitigating Vocabulary Mismatch:** If a user asks a conceptual question using different words, the vector component can still find relevant documents.\n",
      "*   **Ensuring Factual Accuracy:** If a user asks a very specific factual question, the BM25 component ensures that documents containing those exact terms are highly ranked, preventing the vector component from retrieving only semantically similar but factually distinct information.\n",
      "*   **Handling Ambiguity:** For queries that could be interpreted in multiple ways, fusion can bring in documents from both keyword and semantic perspectives, providing a richer context for the LLM to synthesize an answer.\n",
      "\n",
      "In essence, fusion acts as a safety net, ensuring that the system doesn't miss relevant information due to the limitations of a single retrieval approach. It provides a more resilient and adaptable retrieval layer for a RAG system.\n",
      "\n",
      "### Recommendations for When to Use Each Approach\n",
      "\n",
      "1.  **BM25 Keyword Retrieval:**\n",
      "    *   **Use when:** Your document collection is highly structured, and queries are primarily factual, specific, or involve unique identifiers, names, or acronyms.\n",
      "    *   **Example Use Case:** Internal knowledge bases where users search for specific policy numbers, product names, or definitions.\n",
      "    *   **Consideration:** If your users tend to use very precise language that matches your document content.\n",
      "\n",
      "2.  **Vector-based Retrieval:**\n",
      "    *   **Use when:** Your document collection is diverse, and queries are often conceptual, open-ended, or involve natural language that may not perfectly match document keywords.\n",
      "    *   **Example Use Case:** Customer support chatbots where users ask questions in varied ways, or research systems where users explore broad topics.\n",
      "    *   **Consideration:** Requires a good quality embedding model and can be more computationally intensive for indexing.\n",
      "\n",
      "3.  **Fusion Retrieval:**\n",
      "    *   **Use when:** You need a robust, general-purpose retrieval system that can handle a wide variety of query types. This is often the **recommended default** for most modern RAG applications.\n",
      "    *   **Example Use Case:** Public-facing Q&A systems, comprehensive knowledge management platforms, or any application where query diversity is high and consistent performance is critical.\n",
      "    *   **Consideration:** While more complex to set up initially, the benefits of improved accuracy and robustness across diverse queries usually outweigh the added complexity.\n",
      "\n",
      "In conclusion, while individual methods have their niches, **Fusion Retrieval emerges as the most versatile and robust approach** for a RAG system, effectively balancing the strengths of keyword matching and semantic understanding to provide consistently high-quality results across a broad spectrum of user queries.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Path to PDF document\n",
    "# Path to PDF document containing AI information for knowledge retrieval testing\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# load questions from json\n",
    "test_data = json.load(open(\"data/val.json\"))\n",
    "# Define a single AI-related test query\n",
    "# test_queries = [\n",
    "#     \"What are the main applications of transformer models in natural language processing?\"  # AI-specific query\n",
    "# ]\n",
    "\n",
    "test_queries = [question[\"question\"] for question in test_data]\n",
    "\n",
    "# Optional reference answer\n",
    "# reference_answers = [\n",
    "#     \"Transformer models have revolutionized natural language processing with applications including machine translation, text summarization, question answering, sentiment analysis, and text generation. They excel at capturing long-range dependencies in text and have become the foundation for models like BERT, GPT, and T5.\",\n",
    "# ]\n",
    "reference_answers = [answer[\"ideal_answer\"] for answer in test_data]\n",
    "# Set parameters\n",
    "k = 5  # Number of documents to retrieve\n",
    "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_fusion_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
