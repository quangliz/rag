{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import tempfile\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831ccad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"conf.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0081eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20609a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract both text and images from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_dir (str, optional): Directory to save extracted images\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Text data and image data\n",
    "    \"\"\"\n",
    "    # Create a temporary directory for images if not provided\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    text_data = []  # List to store extracted text data\n",
    "    image_paths = []  # List to store paths of extracted images\n",
    "    \n",
    "    print(f\"Extracting content from {pdf_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # Loop through every page in the PDF\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "                \n",
    "                # Extract text from the page\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                # Extract images from the page\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # XREF of the image\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "                    \n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # Save the image to the output directory\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "                        \n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "        \n",
    "        print(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\n",
    "        return text_data, image_paths\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc357cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text data into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text_data (List[Dict]): Text data extracted from PDF\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Chunked text data\n",
    "    \"\"\"\n",
    "    chunked_data = []  # Initialize an empty list to store chunked data\n",
    "    \n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # Extract the text content\n",
    "        metadata = item[\"metadata\"]  # Extract the metadata\n",
    "        \n",
    "        # Skip if text is too short\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Create chunks with overlap\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "            if chunk:  # Ensure we don't add empty chunks\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # Add each chunk with updated metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # Copy the original metadata\n",
    "            chunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,  # The chunk text\n",
    "                \"metadata\": chunk_metadata  # The updated metadata\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\n",
    "    return chunked_data  # Return the list of chunked data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca903bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image file as base64.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded image\n",
    "    \"\"\"\n",
    "    # Open the image file in binary read mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the image file and encode it to base64\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # Decode the base64 bytes to a string and return\n",
    "        return encoded_image.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f501a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using OpenAI's vision capabilities.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is an image\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"Error: Image file not found\"\n",
    "    \n",
    "    try:\n",
    "        # Open and validate the image\n",
    "        Image.open(image_path)\n",
    "        \n",
    "        # Encode the image to base64\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        # Create the API request to generate the caption\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gemini-2.5-flash\", # Use the llava-1.5-7b model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "You are an assistant specialized in describing images from academic papers.\n",
    "Provide detailed captions for the image that capture key information.\n",
    "If the image contains charts, tables, or diagrams, describe their content and purpose clearly.\n",
    "Your caption should be optimized for future retrieval when people ask questions about this content.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image in detail with UNDER 300 WORDS, focusing on its academic content:\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Extract the caption from the response\n",
    "        caption = response.choices[0].message.content\n",
    "        return caption\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Return an error message if an exception occurs\n",
    "        return f\"Error generating caption: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a583f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    Process all images and generate captions.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (List[Dict]): Paths to extracted images\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Image data with captions\n",
    "    \"\"\"\n",
    "    image_data = []  # Initialize an empty list to store image data with captions\n",
    "    \n",
    "    print(f\"Generating captions for {len(image_paths)} images...\")  # Print the number of images to process\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}...\")  # Print the current image being processed\n",
    "        img_path = img_item[\"path\"]  # Get the image path\n",
    "        metadata = img_item[\"metadata\"]  # Get the image metadata\n",
    "        \n",
    "        # Generate caption for the image\n",
    "        caption = generate_image_caption(img_path)\n",
    "        \n",
    "        # Add the image data with caption to the list\n",
    "        image_data.append({\n",
    "            \"content\": caption,  # The generated caption\n",
    "            \"metadata\": metadata,  # The image metadata\n",
    "            \"image_path\": img_path  # The path to the image\n",
    "        })\n",
    "    \n",
    "    return image_data  # Return the list of image data with captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "996547dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation for multi-modal content.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, contents, and metadata\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            content (str): The content (text or image caption)\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding vector, content, and metadata to their respective lists\n",
    "        self.vectors.append(embedding)\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of content items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        print(type(items), type(embeddings))\n",
    "        # Loop through items and embeddings and add each to the vector store\n",
    "        for item, embedding in zip(items, embeddings):\n",
    "            self.add_item(\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = cosine_similarity(query_embedding.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
    "            })\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f558df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b460e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a34189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "  is_single = isinstance(text, str)\n",
    "  if is_single: text = [text]\n",
    "\n",
    "  try:\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding = True,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(device)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return None\n",
    "\n",
    "  try:\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "      cls = outputs.last_hidden_state[:, 0, :]\n",
    "      embed_normalized = F.normalize(cls, p = 2, dim = 1)\n",
    "    embeddings = [embed.cpu().numpy() for embed in embed_normalized]\n",
    "\n",
    "    return embeddings\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db54c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract both text and images from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_dir (str, optional): Directory to save extracted images\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Text data and image data\n",
    "    \"\"\"\n",
    "    # Create a temporary directory for images if not provided\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    text_data = []  # List to store extracted text data\n",
    "    image_paths = []  # List to store paths of extracted images\n",
    "\n",
    "    print(f\"Extracting content from {pdf_path}...\")\n",
    "\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # Loop through every page in the PDF\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "\n",
    "                # Extract text from the page\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "                # Extract images from the page\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # XREF of the image\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "\n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "\n",
    "                        # Save the image to the output directory\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "\n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "\n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "        print(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\n",
    "        return text_data, image_paths\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6128c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text data into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text_data (List[Dict]): Text data extracted from PDF\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Chunked text data\n",
    "    \"\"\"\n",
    "    chunked_data = []  # Initialize an empty list to store chunked data\n",
    "\n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # Extract the text content\n",
    "        metadata = item[\"metadata\"]  # Extract the metadata\n",
    "\n",
    "        # Skip if text is too short\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Create chunks with overlap\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "            if chunk:  # Ensure we don't add empty chunks\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        # Add each chunk with updated metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # Copy the original metadata\n",
    "            chunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\n",
    "\n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,  # The chunk text\n",
    "                \"metadata\": chunk_metadata  # The updated metadata\n",
    "            })\n",
    "\n",
    "    print(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\n",
    "    return chunked_data  # Return the list of chunked data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2005819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image file as base64.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "\n",
    "    Returns:\n",
    "        str: Base64 encoded image\n",
    "    \"\"\"\n",
    "    # Open the image file in binary read mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the image file and encode it to base64\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # Decode the base64 bytes to a string and return\n",
    "        return encoded_image.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23731b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using OpenAI's vision capabilities.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "\n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is an image\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"Error: Image file not found\"\n",
    "\n",
    "    try:\n",
    "        # Open and validate the image\n",
    "        Image.open(image_path)\n",
    "\n",
    "        # Encode the image to base64\n",
    "        base64_image = encode_image(image_path)\n",
    "\n",
    "        # Create the API request to generate the caption\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gemini-2.5-flash\", # Use the llava-1.5-7b model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "You are an assistant specialized in describing images from academic papers.\n",
    "Provide detailed captions for the image that capture key information.\n",
    "If the image contains charts, tables, or diagrams, describe their content and purpose clearly.\n",
    "Your caption should be optimized for future retrieval when people ask questions about this content.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image in detail with UNDER 300 WORDS, focusing on its academic content:\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Extract the caption from the response\n",
    "        caption = response.choices[0].message.content\n",
    "        return caption\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return an error message if an exception occurs\n",
    "        return f\"Error generating caption: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c6f3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    Process all images and generate captions.\n",
    "\n",
    "    Args:\n",
    "        image_paths (List[Dict]): Paths to extracted images\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Image data with captions\n",
    "    \"\"\"\n",
    "    image_data = []  # Initialize an empty list to store image data with captions\n",
    "\n",
    "    print(f\"Generating captions for {len(image_paths)} images...\")  # Print the number of images to process\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}...\")  # Print the current image being processed\n",
    "        img_path = img_item[\"path\"]  # Get the image path\n",
    "        metadata = img_item[\"metadata\"]  # Get the image metadata\n",
    "\n",
    "        # Generate caption for the image\n",
    "        caption = generate_image_caption(img_path)\n",
    "\n",
    "        # Add the image data with caption to the list\n",
    "        image_data.append({\n",
    "            \"content\": caption,  # The generated caption\n",
    "            \"metadata\": metadata,  # The image metadata\n",
    "            \"image_path\": img_path  # The path to the image\n",
    "        })\n",
    "\n",
    "    return image_data  # Return the list of image data with captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4af2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation for multi-modal content.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, contents, and metadata\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "            content (str): The content (text or image caption)\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding vector, content, and metadata to their respective lists\n",
    "        self.vectors.append(embedding)\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "\n",
    "        Args:\n",
    "            items (List[Dict]): List of content items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        print(type(items), type(embeddings))\n",
    "        # Loop through items and embeddings and add each to the vector store\n",
    "        for item, embedding in zip(items, embeddings):\n",
    "            self.add_item(\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = cosine_similarity(query_embedding.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "            similarities.append((i, similarity))\n",
    "\n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
    "            })\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f504975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for multi-modal RAG.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MultiModalVectorStore, Dict]: Vector store and document info\n",
    "    \"\"\"\n",
    "    # Create a directory for extracted images\n",
    "    image_dir = \"extracted_images\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    # Extract text and images from the PDF\n",
    "    text_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n",
    "\n",
    "    # Chunk the extracted text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "\n",
    "    # Process the extracted images to generate captions\n",
    "    image_data = process_images(image_paths)\n",
    "\n",
    "    # Combine all content items (text chunks and image captions)\n",
    "    all_items = chunked_text + image_data\n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in all_items]\n",
    "    # Create embeddings for all content\n",
    "    print(\"Creating embeddings for all content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    # Build the vector store and add items with their embeddings\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(all_items, embeddings)\n",
    "\n",
    "    # Prepare document info with counts of text chunks and image captions\n",
    "    doc_info = {\n",
    "        \"text_count\": len(chunked_text),\n",
    "        \"image_count\": len(image_data),\n",
    "        \"total_items\": len(all_items),\n",
    "    }\n",
    "\n",
    "    # Print summary of added items\n",
    "    print(f\"Added {len(all_items)} items to vector store ({len(chunked_text)} text chunks, {len(image_data)} image captions)\")\n",
    "\n",
    "    # Return the vector store and document info\n",
    "    return vector_store, doc_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7581924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved results.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved content\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Format the context from the retrieved results\n",
    "    context = \"\"\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        # Determine the type of content (text or image caption)\n",
    "        content_type = \"Text\" if result[\"metadata\"].get(\"type\") == \"text\" else \"Image caption\"\n",
    "        # Get the page number from the metadata\n",
    "        page_num = result[\"metadata\"].get(\"page\", \"unknown\")\n",
    "\n",
    "        # Append the content type and page number to the context\n",
    "        context += f\"[{content_type} from page {page_num}]\\n\"\n",
    "        # Append the actual content to the context\n",
    "        context += result[\"content\"]\n",
    "        context += \"\\n\\n\"\n",
    "\n",
    "    # System message to guide the AI assistant\n",
    "    system_message = \"\"\"You are an AI assistant specializing in answering questions about documents\n",
    "    that contain both text and images. You have been given relevant text passages and image captions\n",
    "    from the document. Use this information to provide a comprehensive, accurate response to the query.\n",
    "    If information comes from an image or chart, mention this in your answer.\n",
    "    If the retrieved information doesn't fully answer the query, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    # User message containing the query and the formatted context\n",
    "    user_message = f\"\"\"Query: {query}\n",
    "\n",
    "    Retrieved content:\n",
    "    {context}\n",
    "\n",
    "    Please answer the query based on the retrieved content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Return the generated response\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9807c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Query the multi-modal RAG system.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (MultiModalVectorStore): Vector store with document content\n",
    "        k (int): Number of results to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dict: Query results and generated response\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query: {query} ===\\n\")\n",
    "\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = create_embeddings(query)[0]\n",
    "\n",
    "    # Retrieve relevant content from the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "\n",
    "    # Separate text and image results\n",
    "    text_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\n",
    "    image_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\n",
    "\n",
    "    print(f\"Retrieved {len(results)} relevant items ({len(text_results)} text, {len(image_results)} image captions)\")\n",
    "\n",
    "    # Generate a response using the retrieved content\n",
    "    response = generate_response(query, results)\n",
    "    print(response)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"response\": response,\n",
    "        \"text_results_count\": len(text_results),\n",
    "        \"image_results_count\": len(image_results)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec86e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Build a text-only vector store for comparison.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "\n",
    "    Returns:\n",
    "        MultiModalVectorStore: Text-only vector store\n",
    "    \"\"\"\n",
    "    # Extract text from PDF (reuse function but ignore images)\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, None)\n",
    "\n",
    "    # Chunk text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "\n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "\n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for text-only content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "\n",
    "    # Build vector store\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "\n",
    "    print(f\"Added {len(chunked_text)} text items to text-only vector store\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e6139fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, mm_response, text_response, reference=None):\n",
    "    \"\"\"\n",
    "    Compare multi-modal and text-only responses.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        mm_response (str): Multi-modal response\n",
    "        text_response (str): Text-only response\n",
    "        reference (str, optional): Reference answer\n",
    "\n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator comparing two RAG systems:\n",
    "    1. Multi-modal RAG: Retrieves from both text and image captions\n",
    "    2. Text-only RAG: Retrieves only from text\n",
    "\n",
    "    Evaluate which response better answers the query based on:\n",
    "    - Accuracy and correctness\n",
    "    - Completeness of information\n",
    "    - Relevance to the query\n",
    "    - Unique information from visual elements (for multi-modal)\"\"\"\n",
    "\n",
    "    # User prompt with query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Multi-modal RAG Response:\n",
    "    {mm_response}\n",
    "\n",
    "    Text-only RAG Response:\n",
    "    {text_response}\n",
    "    \"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "    Reference Answer:\n",
    "    {reference}\n",
    "    \"\"\"\n",
    "\n",
    "        user_prompt += \"\"\"\n",
    "    Compare these responses and explain which one better answers the query and why.\n",
    "    Note any specific information that came from images in the multi-modal response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate comparison using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f84b9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of multi-modal vs text-only RAG.\n",
    "\n",
    "    Args:\n",
    "        results (List[Dict]): Evaluation results for each query\n",
    "\n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Provide an overall analysis comparing\n",
    "    multi-modal RAG (text + images) versus text-only RAG based on multiple test queries.\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where multi-modal RAG outperforms text-only\n",
    "    2. Specific advantages of incorporating image information\n",
    "    3. Any disadvantages or limitations of the multi-modal approach\n",
    "    4. Overall recommendation on when to use each approach\"\"\"\n",
    "\n",
    "    # Create summary of evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Multi-modal retrieved {result['multimodal_results']['text_count']} text chunks and {result['multimodal_results']['image_count']} image captions\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt with evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of multi-modal vs text-only RAG across {len(results)} queries,\n",
    "    provide an overall analysis comparing these two approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of the relative strengths and weaknesses of multi-modal RAG\n",
    "    compared to text-only RAG, with specific attention to how image information contributed (or didn't contribute) to response quality.\"\"\"\n",
    "\n",
    "    # Generate overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce6a47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare multi-modal RAG with text-only RAG.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): Test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "\n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\\n\")\n",
    "\n",
    "    # Process document for multi-modal RAG\n",
    "    print(\"\\nProcessing document for multi-modal RAG...\")\n",
    "    mm_vector_store, mm_doc_info = process_document(pdf_path)\n",
    "\n",
    "    # Build text-only store\n",
    "    print(\"\\nProcessing document for text-only RAG...\")\n",
    "    text_vector_store = build_text_only_store(pdf_path)\n",
    "\n",
    "    # Run evaluation for each query\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}: {query} ===\")\n",
    "\n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "\n",
    "        # Run multi-modal RAG\n",
    "        print(\"\\nRunning multi-modal RAG...\")\n",
    "        mm_result = query_multimodal_rag(query, mm_vector_store)\n",
    "\n",
    "        # Run text-only RAG\n",
    "        print(\"\\nRunning text-only RAG...\")\n",
    "        text_result = query_multimodal_rag(query, text_vector_store)\n",
    "\n",
    "        # Compare responses\n",
    "        comparison = compare_responses(query, mm_result[\"response\"], text_result[\"response\"], reference)\n",
    "\n",
    "        # Add to results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"multimodal_response\": mm_result[\"response\"],\n",
    "            \"textonly_response\": text_result[\"response\"],\n",
    "            \"multimodal_results\": {\n",
    "                \"text_count\": mm_result[\"text_results_count\"],\n",
    "                \"image_count\": mm_result[\"image_results_count\"]\n",
    "            },\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison\n",
    "        })\n",
    "\n",
    "    # Generate overall analysis\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis,\n",
    "        \"multimodal_doc_info\": mm_doc_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e4ab1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\n",
      "\n",
      "\n",
      "Processing document for multi-modal RAG...\n",
      "Extracting content from data/attention_is_all_you_need.pdf...\n",
      "Extracted 15 text segments and 3 images\n",
      "Created 56 text chunks\n",
      "Generating captions for 3 images...\n",
      "Processing image 1/3...\n",
      "Processing image 2/3...\n",
      "Processing image 3/3...\n",
      "Creating embeddings for all content...\n",
      "<class 'list'> <class 'list'>\n",
      "Added 59 items to vector store (56 text chunks, 3 image captions)\n",
      "\n",
      "Processing document for text-only RAG...\n",
      "Extracting content from data/attention_is_all_you_need.pdf...\n",
      "Extracted 15 text segments and 3 images\n",
      "Created 56 text chunks\n",
      "Creating embeddings for text-only content...\n",
      "<class 'list'> <class 'list'>\n",
      "Added 56 text items to text-only vector store\n",
      "\n",
      "\n",
      "=== Evaluating Query 1: What is the architectural structure of a Transformer model, and how does it process input and output sequences? ===\n",
      "\n",
      "Running multi-modal RAG...\n",
      "\n",
      "=== Processing query: What is the architectural structure of a Transformer model, and how does it process input and output sequences? ===\n",
      "\n",
      "Retrieved 5 relevant items (3 text, 2 image captions)\n",
      "The Transformer model, as described in the retrieved content, follows an encoder-decoder architecture, relying entirely on self-attention mechanisms to process input and output sequences without using recurrent neural networks (RNNs) or convolutions (Text P2).\n",
      "\n",
      "**1. Architectural Structure:**\n",
      "\n",
      "The Transformer is composed of two main parts: an **Encoder** (left half) and a **Decoder** (right half), as shown in Figure 1 (Image Caption P3, Text P3). Both the encoder and decoder consist of a stack of N=6 identical layers (Text P3).\n",
      "\n",
      "*   **Encoder Structure:**\n",
      "    *   The encoder is a stack of six identical layers.\n",
      "    *   Each encoder layer has two sub-layers (Text P3, Image Caption P3):\n",
      "        1.  A **Multi-Head Self-Attention** mechanism: This allows the model to focus on different parts of the input sequence simultaneously (Image Caption P3). In self-attention, the queries, keys, and values all come from the output of the previous layer in the encoder, enabling each position to attend to all positions in the previous layer (Text P5).\n",
      "        2.  A simple, position-wise **Feed-Forward Network** (Text P3, Image Caption P3).\n",
      "    *   Around each of these two sub-layers, a residual connection is employed, followed by layer normalization. The output of each sub-layer is `LayerNorm(x + Sublayer(x))` (Text P3, Image Caption P3).\n",
      "    *   All sub-layers and embedding layers produce outputs of dimension `dmodel = 512` to facilitate these residual connections (Text P3).\n",
      "\n",
      "*   **Decoder Structure:**\n",
      "    *   The decoder is also a stack of six identical layers (Text P3, Image Caption P3).\n",
      "    *   Each decoder layer has three sub-layers (Image Caption P3):\n",
      "        1.  A **Masked Multi-Head Self-Attention** layer: This prevents attending to future positions during training, preserving the auto-regressive property (Image Caption P3, Text P5).\n",
      "        2.  A **Multi-Head Attention** layer (referred to as \"encoder-decoder attention\"): Here, the queries come from the previous decoder layer, while the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence (Image Caption P3, Text P5).\n",
      "        3.  A **Feed-Forward Network** (Image Caption P3).\n",
      "    *   Similar to the encoder, \"Add & Norm\" operations (residual connections followed by layer normalization) are applied after each sub-layer (Image Caption P3).\n",
      "    *   The decoder's output then passes through a **Linear** layer, which projects features to a vocabulary size, followed by a **Softmax** layer to convert scores into output probabilities for each potential token (Image Caption P3).\n",
      "\n",
      "*   **Multi-Head Attention Mechanism (Core Component):**\n",
      "    *   This mechanism takes Query (Q), Key (K), and Value (V) vectors as input (Image Caption P4).\n",
      "    *   These inputs are independently projected multiple times using parallel \"Linear\" layers for 'h' attention \"heads\" (Image Caption P4).\n",
      "    *   The outputs from these parallel linear transformations are fed into corresponding \"Scaled Dot-Product Attention\" modules, operating in parallel (Image Caption P4).\n",
      "    *   The outputs from all 'h' independent heads are then concatenated and passed through a final \"Linear\" layer to produce the desired output dimension (Image Caption P4). This allows the model to jointly attend to information from different representation subspaces at different positions (Image Caption P4).\n",
      "\n",
      "**2. Processing Input and Output Sequences:**\n",
      "\n",
      "*   **Input Processing (by the Encoder):**\n",
      "    *   The encoder processes the input sequence (x1, ..., xn) (Text P2).\n",
      "    *   \"Inputs\" are first converted into \"Input Embeddings\" (Image Caption P3).\n",
      "    *   To preserve the sequential order, \"Positional Encoding\" is added to these embeddings (Image Caption P3).\n",
      "    *   This combined representation then feeds into the stack of N=6 identical encoder layers (Image Caption P3).\n",
      "    *   Within each encoder layer, the Multi-Head Self-Attention mechanism allows each position in the encoder to attend to all positions in the previous layer of the encoder, computing representations (Text P5).\n",
      "    *   The encoder maps the input sequence to a sequence of continuous representations `z = (z1, ..., zn)` (Text P2).\n",
      "\n",
      "*   **Output Processing (by the Decoder):**\n",
      "    *   The decoder generates an output sequence (y1, ..., ym) one element at a time (Text P2).\n",
      "    *   \"Outputs (shifted right)\" are initially transformed into \"Output Embeddings,\" with \"Positional Encoding\" added (Image Caption P3).\n",
      "    *   These embeddings pass through the stack of N=6 identical decoder layers (Image Caption P3).\n",
      "    *   At each step, the model is auto-regressive, consuming the previously generated output (Text P2). The Masked Multi-Head Attention layer in the decoder ensures that attention is only applied to positions up to and including the current position, preventing information flow from future tokens (Image Caption P3, Text P5).\n",
      "    *   The encoder-decoder attention layer allows the decoder to attend over all positions in the input sequence (from the encoder's output), enabling it to focus on relevant information from the input when generating the output (Image Caption P3, Text P5).\n",
      "    *   Finally, the decoder's output is passed through a Linear layer and a Softmax layer to produce \"Output Probabilities\" for the next token in the sequence (Image Caption P3).\n",
      "\n",
      "Running text-only RAG...\n",
      "\n",
      "=== Processing query: What is the architectural structure of a Transformer model, and how does it process input and output sequences? ===\n",
      "\n",
      "Retrieved 5 relevant items (5 text, 0 image captions)\n",
      "The Transformer model features an encoder-decoder architecture, which is a common structure for neural sequence transduction models (as mentioned on page 3). It distinguishes itself by relying entirely on an attention mechanism to compute representations of its input and output, eschewing sequence-aligned recurrent neural networks (RNNs) or convolutions (page 2). This design allows for significantly more parallelization (page 2).\n",
      "\n",
      "**Architectural Structure:**\n",
      "*   **Overall Structure:** The Transformer consists of an encoder and a decoder, as depicted in Figure 1 (page 3).\n",
      "*   **Encoder:**\n",
      "    *   Composed of a stack of N=6 identical layers (page 3).\n",
      "    *   Each encoder layer has two sub-layers:\n",
      "        1.  A multi-head self-attention mechanism (page 3).\n",
      "        2.  A simple, position-wise fully connected feed-forward network (page 3).\n",
      "    *   Residual connections are employed around each sub-layer, followed by layer normalization. The output of each sub-layer is `LayerNorm(x + Sublayer(x))` (page 3).\n",
      "    *   All sub-layers and embedding layers produce outputs of dimension `dmodel = 512` to facilitate residual connections (page 3).\n",
      "*   **Decoder:**\n",
      "    *   Also composed of a stack of N=6 identical layers (page 3).\n",
      "    *   Similar to the encoder, it uses residual connections and layer normalization (implied by \"identical layers\" and the general architecture).\n",
      "\n",
      "**Processing Input and Output Sequences:**\n",
      "The Transformer processes sequences using multi-head attention in three distinct ways (page 5):\n",
      "\n",
      "1.  **Encoder Input Processing:**\n",
      "    *   The encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations (z1, ..., zn) (page 2).\n",
      "    *   Within the encoder, **self-attention layers** are used. In these layers, all keys, values, and queries come from the output of the previous layer in the encoder. This allows each position in the encoder to attend to all positions in the previous layer of the encoder (page 5).\n",
      "\n",
      "2.  **Decoder Output Processing:**\n",
      "    *   Given the encoder's output (z), the decoder generates an output sequence (y1, ..., ym) one element at a time (page 2).\n",
      "    *   The decoder is auto-regressive, meaning it consumes the previously generated symbols at each step (page 2).\n",
      "    *   The decoder utilizes two types of attention:\n",
      "        *   **Self-attention (in decoder):** Similar to the encoder, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. This mechanism is designed to prevent leftward information flow, ensuring that predictions for a given position only depend on known outputs (page 5).\n",
      "        *   **Encoder-Decoder Attention:** In these layers, the queries originate from the previous decoder layer, while the memory keys and values come from the output of the encoder. This crucial mechanism enables every position in the decoder to attend over all positions in the input sequence, mimicking typical encoder-decoder attention mechanisms in sequence-to-sequence models (page 5).\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "Based on the evaluation of Query 1, a clear pattern emerges regarding the strengths of multi-modal RAG, particularly when dealing with visually-oriented information. While this analysis is based on a single query, the results for \"architectural structure\" are highly indicative of the broader advantages of incorporating image information.\n",
      "\n",
      "Here's a comprehensive analysis:\n",
      "\n",
      "### Overall Analysis: Multi-modal RAG vs. Text-only RAG\n",
      "\n",
      "**Query 1: \"What is the architectural structure of a Transformer model, and how does it process input and output sequences?\"**\n",
      "\n",
      "For this query, the Multi-modal RAG response was deemed \"significantly better\" due to its ability to leverage both text chunks and image captions. This highlights the core advantage of multi-modal systems.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Types of Queries Where Multi-modal RAG Outperforms Text-only\n",
      "\n",
      "The performance on Query 1 strongly suggests that multi-modal RAG excels in scenarios where:\n",
      "\n",
      "*   **Complex Structures and Architectures are Involved:** Queries about the \"architectural structure\" of a Transformer model inherently benefit from visual representations (diagrams, flowcharts). Text alone, no matter how detailed, often struggles to convey spatial relationships and interconnections as effectively as a well-labeled diagram.\n",
      "*   **Processes and Workflows Need Explanation:** Understanding \"how it processes input and output sequences\" is often clarified by flow diagrams or step-by-step visual guides.\n",
      "*   **Visual Concepts are Central:** Any query where the answer is best understood through a visual aid (e.g., explaining a specific machine part, a biological process, a geographical feature, a historical event with visual context like a battle map or a famous painting).\n",
      "*   **Ambiguity in Text Needs Resolution:** Sometimes, textual descriptions can be ambiguous; an accompanying image can provide immediate clarity.\n",
      "\n",
      "### 2. Specific Advantages of Incorporating Image Information (as seen in Query 1)\n",
      "\n",
      "For Query 1, the inclusion of image captions provided several critical advantages:\n",
      "\n",
      "*   **Enhanced Clarity and Comprehension:** Diagrams of the Transformer architecture (encoder-decoder stack, attention mechanisms, feed-forward networks) are far more intuitive for understanding its structure than purely textual descriptions. Image captions likely described these visual elements, allowing the RAG system to synthesize a more coherent and understandable explanation.\n",
      "*   **Completeness and Detail:** Images can convey a wealth of information concisely. For instance, a single diagram can show the relative positions of different layers, the flow of data, and the various components (e.g., multi-head attention, add & norm) in a way that would require many paragraphs of text to describe. The image captions likely provided the textual hooks to retrieve these rich visual details.\n",
      "*   **Improved Accuracy and Specificity:** For architectural queries, the most accurate representation is often a diagram. By retrieving information linked to these diagrams (via captions), the multi-modal RAG can provide a more precise and less abstract answer.\n",
      "*   **Reduced Cognitive Load:** Visual information often reduces the cognitive effort required to understand complex topics, leading to a \"significantly better\" user experience.\n",
      "\n",
      "### 3. Disadvantages or Limitations of the Multi-modal Approach\n",
      "\n",
      "While powerful, multi-modal RAG is not without its drawbacks:\n",
      "\n",
      "*   **Increased Complexity and Cost:**\n",
      "    *   **Data Ingestion:** Processing and indexing images (e.g., generating embeddings, creating accurate captions) is more resource-intensive than just text.\n",
      "    *   **Retrieval Complexity:** Matching multi-modal queries to multi-modal documents requires more sophisticated indexing and retrieval mechanisms.\n",
      "    *   **Computational Overhead:** Running multi-modal models for embedding generation and retrieval adds to computational costs.\n",
      "*   **Reliance on Image Quality and Captioning:** The effectiveness of image information heavily depends on the quality of the images themselves and, crucially, the accuracy and descriptiveness of their associated captions or generated embeddings. Poor captions can lead to irrelevant retrievals or misinterpretations.\n",
      "*   **Irrelevance for Purely Textual Queries:** For queries that are inherently abstract, conceptual, or purely textual (e.g., \"What is the meaning of democracy?\", \"Summarize the plot of Hamlet\"), images may offer little to no benefit and could even introduce noise or irrelevant information.\n",
      "*   **Scalability Challenges:** Managing and updating a large multi-modal knowledge base is more complex than a text-only one.\n",
      "*   **Potential for Misinterpretation:** If an image is ambiguous or its caption is vague, it could lead to incorrect inferences by the RAG system.\n",
      "\n",
      "### 4. Overall Recommendation on When to Use Each Approach\n",
      "\n",
      "*   **Use Multi-modal RAG When:**\n",
      "    *   The domain is rich in visual information (e.g., science, engineering, medicine, art, history, product documentation).\n",
      "    *   Queries frequently involve explaining structures, processes, comparisons, or require visual context for complete understanding.\n",
      "    *   The goal is to provide the most comprehensive, clear, and intuitive answers, even if it means higher operational costs.\n",
      "    *   User experience and deep comprehension are paramount.\n",
      "\n",
      "*   **Use Text-only RAG When:**\n",
      "    *   The knowledge base is predominantly textual, and visual information is scarce or irrelevant to the core content.\n",
      "    *   Queries are primarily abstract, conceptual, or fact-based without a strong visual component.\n",
      "    *   Cost-efficiency, simplicity, and speed of deployment are higher priorities, and the added value of images is minimal for the typical query load.\n",
      "    *   The focus is on retrieving and synthesizing purely textual information.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Based on Query 1, multi-modal RAG demonstrates a significant advantage for queries that benefit from visual context, particularly those involving complex structures and processes. While it introduces increased complexity and cost, the ability to leverage image information leads to demonstrably superior answers for specific types of queries. Organizations should consider a multi-modal approach when their data and user queries frequently involve concepts that are best explained or understood visually, recognizing the trade-off between enhanced answer quality and operational overhead.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF document\n",
    "pdf_path = \"data/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Define test queries targeting both text and visual content\n",
    "test_queries = [\n",
    "    \"What is the architectural structure of a Transformer model, and how does it process input and output sequences?\",\n",
    "]\n",
    "\n",
    "# Optional reference answers for evaluation\n",
    "reference_answers = [\n",
    "    \"The Transformer model consists of an encoder-decoder architecture. The encoder processes the input sequence by applying multiple layers of multi-head attention, feed-forward networks, and normalization (Add & Norm). Each layer in the encoder captures contextual information from the entire input sequence. The decoder, on the other hand, processes the output sequence while attending to both the encoded input and its own previous outputs (using masked multi-head attention to ensure that future tokens are not seen during training). Both the encoder and decoder use positional encoding to incorporate the order of tokens in the sequence. Finally, the output probabilities are generated through a linear layer followed by a softmax function. This architecture enables efficient parallel processing and has been widely used in natural language processing tasks such as translation and text generation.\",\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_multimodal_vs_textonly(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
