{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDGDN--JrYbd"
      },
      "source": [
        "### Relevant Segment Extraction (RSE) for Enhanced RAG\n",
        "Relevant Segment Extraction (RSE) technique improves the context quality in our RAG system. Rather than simply retrieving a collection of isolated chunks, we identify and reconstruct continuous segments of text that provide better context to our language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRtdUPpcrni2"
      },
      "source": [
        "#### Key Concept\n",
        "Relevant chunks tend to be clustered together within documents. By identifying these clusters and preserving their continuity, we provide more coherent context for the LLM to work with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Work flow:\n",
        "1. Process data(text from pdf -> chunking -> create vector store)\n",
        "2. Calculate values for each chunks\n",
        "3. Implement **maximum sum subarray algorithm** to find best segments(contiguous chunks with highest score)\n",
        "4. Merge best contiguous chunks from index into text\n",
        "5. Generation\n",
        "6. Compare with Standard Retrieval\n",
        "7. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZanoWMEPdOhE",
        "outputId": "509803d9-16b7-422a-d59f-4b44a10595f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pymupdf\n",
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzLYWn_Ob7-G"
      },
      "source": [
        "#### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JW0MneUOdioz"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import tqdm\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXU7FsCacBWB"
      },
      "source": [
        "#### Define models\n",
        "\n",
        "Using **Llama3.2 3B** for generation and **bge-base-en-v1.5** for embedding\n",
        "\n",
        "Total VRAM needed is around 8GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759,
          "referenced_widgets": [
            "e242993f32294e71a2d8ecfdc59b4d8a",
            "a13fbc7347c14dc8a423a30e5ef47baa",
            "bb3b592dd0b844489012412649cc3b08",
            "031344a8aa3444c2b741e46fe27ed890",
            "604112d1455549ff94de98be956f0520",
            "ee4285c094c54b6e88a7a04c5232b7af",
            "3489eef65c764cd4a8ec4ad939a5b2dd",
            "4839854fc2fb43bbbdc79cf276181239",
            "60a0b66397824fcfbd740f7f03eb8168",
            "85d6b0835a1f4042be92aaf9032d061e",
            "98a49ef5c3bf418f888ad8dfa75c5a3c",
            "25228630dcad4430859092bde5332329",
            "5d07374086bd484c83ad51eba1cf76f6",
            "116a29d504904f60b518371894aca36e",
            "c476783244fe4136891b5818360ef80a",
            "a4c77472e07c4f26af8efdd790993400",
            "c382049226a44e3ea6f00707ba2d1da8",
            "8a7adeb83a3a40a69f6852c21f32b78e",
            "e181a134ca29444db2385356719653fa",
            "93d06518cb9a4189b79fa3a8f6d37a70",
            "3c65cd8e76d248e2b1b25c7a584a0b5b",
            "cb0d84fb9ef04d5a81aa50f776f3f788",
            "40dc0f8548cd49ebb50152f5d85b7de8",
            "5164f730767e4a028d498e778a8b9bee",
            "46e3fac1d9874d5eba3fd957f0364ed3",
            "76ef9cf4f80a4a0190edd0517abf9b42",
            "9dc0b687540249449e64a3263c75610a",
            "8a973396c67d4140bc50071d6dcd4315",
            "19f2b7272d314de184ff60c497291bc3",
            "a88815f1b46a410b8f484796e1e3595a",
            "9ae0ba5387e940c98c4239829f713ca1",
            "5bdbbdaa104f4e0bae358ad7335d6a7f",
            "4740aa10a76d4ab9bcb7ff279637c827",
            "8cc045df020643038179928f4bd81729",
            "ece9f4fb3df44f7c900d897be45f1192",
            "52c23baebdee46a2aa15485b75e9ed5f",
            "224edcdd1de24ec8b426709879fdc7fc",
            "47d5cb12d8f940f6be423755381e0601",
            "7e2a71bb28ea4c4698769fc86f2f906a",
            "d9a869c43a5a448bbca64315f63564a8",
            "3aaee5ad0ab043f0a0b13dff20245e92",
            "9567f5edf8bd4555a0496069675412ac",
            "0d9788d9c1c14ad49333a9a824ff4474",
            "bdad74f45f2d45bf8bf56c6a111c4dda",
            "bdfc9ac30fff442d9505fba9e871d962",
            "4490f9e9ca7742008c139df63b3f012d",
            "ffc5e65fb60442a595d19dddf174d077",
            "1bbb53f8ad6948d19789848b02c4714c",
            "890f76536a214ae8aa2159b589d4dcd1",
            "55818a831155440085d1dae67992a2c3",
            "95e6cc6114b541dc9cbc039d7b390609",
            "701f0fc6edc849cab514634877c3edad",
            "319b7312781a4cb8add64338f2fb7d0c",
            "f11cb81d21d34c72b11f1698a83bac91",
            "cf40ce2c01b54f0c89df190e019e8384",
            "92022834a8e246f880786b8eaf8ed37d",
            "dcfca9843d4048a4a77ff464ba2d03ab",
            "70d9a9508fa94c0395a32f6f9bafbb0b",
            "eaa41c4c7c22413ea9c766257429f2ba",
            "5cf5e0eb87a84ce3aadb252ab8970633",
            "11b35556f5554b6ea5bea54e77cc93c3",
            "21defd73c415461284a00447bf2f52e6",
            "e6150334fd1b4203b64e61c617740ee8",
            "5138bdd0b31b41d3b1207bb75fcd462c",
            "f7b3b55306f94dc887f74e8ff8fb07cf",
            "1ea44e6d3153489b96bca0b9d6efe2b7",
            "e71c931ca9fb48749b15fac4c99a0ad2",
            "b918164fc05141598128b4b82fea3652",
            "ece30b03e8784b838d019a0f9e1ab059",
            "12a2679332764e67a414b93b106902c7",
            "a381649cbf82433eb655940350657a38",
            "9696354c84b24fbd887e68e79248b096",
            "337ed4228a724e7ca8154cba7f060925",
            "a6561b123a184d4db14fad02a5bb2443",
            "11d8bdaaeb874b5e98f978707af5b049",
            "2ddebec5a2ec416fa89776871afb7fed",
            "328396679daf4347b33135f7ddb18a3c",
            "0a845bea016e44dd9b1b43e6c7f8be0b",
            "409a1b04427c4a6b8547b3bbedba1646",
            "56d6e6f2d46847bdaef40afcd6bd938d",
            "320b695094df48afa74326705eb88f2e",
            "5cc58f92de244f6d9506bf9f99eab258",
            "f9d2af1adbfe42f1a21c2253a43b2ab3",
            "9e1c162c4d4440758d01e373f212ad9d",
            "7b0b480387264f458803bee20a397223",
            "856fb89ebf834386b030e80d98336843",
            "ac59187b313a4d40bf35db6584cd7a49",
            "5fb9fbbe7e21480e906660b94da73001",
            "2f93ae89e4134f249f953f2e6ea2522e",
            "923b1c37236642eba65b4ed473519f82",
            "73e2e9dc9f814dc68802432ed9ebd86c",
            "245154d12f074ae898ad128ec9acb3d9",
            "0ef734ebdc1d495a9de6de3880ba9337",
            "2c28d220df8f42abba214b707ea3692c",
            "0d2ac5aed9dd4f5da6e2bbbddd990a4f",
            "248d53b9721f42b1ae8bf1e4d4ca7a23",
            "f64cd1d43df4454099c0da5d4dca75be",
            "97994226b96e4d52ad616e8ec2c1c1c9",
            "ff02f1cb69a1490c847b273360dff284",
            "4ff80e22ae0d4f188d2a651a224fb2fb",
            "d8889981ee204df3a77ca223b3e95eee",
            "f4992a5a1a85465fbd0dc1488e7eff3c",
            "536f7ca4cd21405b87cc52b17deb1535",
            "f5917aecae964a66acb9ecc21cc09202",
            "6187b7d78c8949518b44f577a4774c61",
            "1916d378a2f040039f423fc08e2095b6",
            "d10ce085ab9943c0abf3bd72aa871906",
            "cb55e2d309ea4c29906ebb6bfad8d687",
            "e05b038eda90498a9a6790198f5efa28",
            "2e8f06e278394945b5f02c3893a0a101",
            "2f57b818449b4f6ea9c30f5033e01024",
            "2ab9ccbfbce642b9a63ff67f5a14e74c",
            "a2a9d6f9104e429080c8e4c78195eac0",
            "c5e8c4965d9d4bfd84c7b4c72814f217",
            "cdb58b86c2c04b668d7c19514c451fea",
            "d9e555fe876b4c1ba8ade710044bee0b",
            "97bf3edebf324cc5bb52243f3efcf55a",
            "c194c289e8b646cb945a848536e2c962",
            "bc1969c7d93548139043bbf22d5d841f",
            "4bfbd878d8744b6bbc0ed86a0f24168b",
            "50f9d23751d94152878f46ab2cde2bf2",
            "d88cd520a6704e8e81266e2b38d8c208",
            "f7f9d351aa114cea860a9fd176987ada",
            "8a39b5eb747b40df9c10e94525a8cfe0",
            "80d7944610874e5f9c31f9d2dcc11b43",
            "0b01294f21ac4159936ac0c12db0d5ad",
            "cfe5685df17a4e7f944ba1752d5d45d5",
            "b9c9f99eeefa4b8eb0382e64edcc1097",
            "e85aea679cf44cb894ebd9a09e6ff161",
            "ece90e1770bb43a583a974f38a62d4a3",
            "f384968354744b64b13610e869a01ae1",
            "8c28a1fb9c9b4411a206c6bc629eef3b",
            "b1dbb36d675543149b2e770aa8a39774",
            "894c613ae252490185869751de89d199",
            "d0a79ce67605490ba9a7a5beab678b62",
            "2812f42212c44f899c71d12a704affc1",
            "ba862342ed14480c823d85b4636205ad",
            "1da56fb4c3e74c73a37f1009f6bf5886",
            "dc83aec4e3b94d9bbf8f9e93003b2eba",
            "7ad595fab79442aba479888cf0153b7c",
            "1b822be06c6042e1bee25f944191da77",
            "1f61be4ebcd64b4d80e04e80a4f1df3b",
            "f3507336561e4f2db999f534bd563e30",
            "b086a6b799ee4b7a8bc0e38ce763af12",
            "36b8aece7f2f47a998827b55cb119d26",
            "7ff2780620a847a295473c6a5fce3073",
            "36bd67e3229f4266b4f02e22395cfb19",
            "80009f8f43b44e6e85712270df62e326",
            "26768d6d00014bbeb1e41026c335f275",
            "e65f1d5d3c214225841d799591764c88",
            "9328e089543045d593e82edfc4825d3f",
            "32657484a54a4b6d8f58b53c07eb24da",
            "ef51ffd040f04484831c995b8c8de1b7",
            "bece4b9d30f344e29f75f5aba38e3040",
            "ad1e2d0d920a476b81b1acbd05e02357",
            "c94a2d6def5042eda2d0385790b1dc8f",
            "5ff23e4ca17a4f3ea18ae7ea31d39d9d",
            "30fa3d265ab54112b9481b16f91f2e58",
            "87b6bdccd7df4314b938354c5fc0de38",
            "f86748e191aa4497825da20ce9fb994b",
            "ec5a9be81ee74d0e8129c06320907459",
            "14ef25ddab304b54a81a3b3ba68f28d1",
            "7157df080eb241cfb8ae895a7cd90529",
            "516654c6a6f9411789b6368afdefc924",
            "e70e491e7ae64104b2d937596c43360e",
            "c5d07f7bc1264f6cb7ddaee8badfcd0c",
            "a6bc659c96af4c0dad513853b3044c4f",
            "2e1175f2141a4f21babaff5638ec45b2",
            "27b1698db8e14c8a9e4061fb1f4abe61",
            "4a857f71616248dbb06da4f0bcff6cf7",
            "f23ad1be31594b7492e440505d2fb408",
            "c796c956f6bb4e9b9cbd64994c4f5bd3",
            "accb5610627f47cda9028740972210be",
            "2558253f6d8948ce92b9e22fda84c3c5",
            "772f02909a3a4fa087bb838b8d3d4e3f",
            "7ee7d0fdb84641e78076a00612c33ee9",
            "720b93e1c14d488ebccf61181e3f3ff9",
            "47a2da1113234e109066e5e0cc052fc4",
            "4e06386aa4a140a7bc97d46d15da59b8",
            "3719728bdd1347068874afe7fe26f1d0",
            "26c7af67c3344e168354e1e487aae9f6",
            "e4f0b2c541754b74a709d34327c034b3",
            "7b359dc7f5774c069d37d15e7d13f59c",
            "79923409fe3b4953a8fa5f44e52a1394",
            "d3826a7c81ec47c78bf01c6165da5273",
            "a6ed366a2b01409094210b5c5de077f8",
            "f073ae0213264c83bcb6c3cfe28dbd95"
          ]
        },
        "id": "0dKLGPQReB1C",
        "outputId": "0bb4cfcb-3b0d-4656-f3a1-094a4ad7a59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading generation model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e242993f32294e71a2d8ecfdc59b4d8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25228630dcad4430859092bde5332329",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40dc0f8548cd49ebb50152f5d85b7de8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cc045df020643038179928f4bd81729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdfc9ac30fff442d9505fba9e871d962",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92022834a8e246f880786b8eaf8ed37d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e71c931ca9fb48749b15fac4c99a0ad2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a845bea016e44dd9b1b43e6c7f8be0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f93ae89e4134f249f953f2e6ea2522e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ff80e22ae0d4f188d2a651a224fb2fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f57b818449b4f6ea9c30f5033e01024",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d88cd520a6704e8e81266e2b38d8c208",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1dbb36d675543149b2e770aa8a39774",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b086a6b799ee4b7a8bc0e38ce763af12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad1e2d0d920a476b81b1acbd05e02357",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d07f7bc1264f6cb7ddaee8badfcd0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "720b93e1c14d488ebccf61181e3f3ff9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load generation model and tokenizer\n",
        "print(\"Loading generation model...\")\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load embedding model and tokenizer\n",
        "print(\"Loading embedding model...\")\n",
        "embed_model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Move embedding model to device\n",
        "embed_model = embed_model.to(device)\n",
        "\n",
        "print(\"All models loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QwDu4lPce2K"
      },
      "source": [
        "#### Extracting and chunking texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MP9VFvPZekgt"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = \"\"\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "  return text # str\n",
        "\n",
        "def chunk_text(text, chunk_size, overlap):\n",
        "  return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)] # list(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbpWL-LBcjyC"
      },
      "source": [
        "#### Define SimpleVectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "dOAxglY3fIf1"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A lightweight vector store implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self, dimension=768):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "\n",
        "        Args:\n",
        "            dimension (int): Dimension of embeddings\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.vectors = []\n",
        "        self.documents = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_documents(self, documents, vectors=None, metadata=None):\n",
        "        \"\"\"\n",
        "        Add documents to the vector store.\n",
        "\n",
        "        Args:\n",
        "            documents (List[str]): List of document chunks\n",
        "            vectors (List[List[float]], optional): List of embedding vectors\n",
        "            metadata (List[Dict], optional): List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        if vectors is None:\n",
        "            vectors = [None] * len(documents)\n",
        "\n",
        "        if metadata is None:\n",
        "            metadata = [{} for _ in range(len(documents))]\n",
        "\n",
        "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
        "            self.documents.append(doc)\n",
        "            self.vectors.append(vec)\n",
        "            self.metadata.append(meta)\n",
        "\n",
        "    def search(self, query_vector, top_k=5):\n",
        "        \"\"\"\n",
        "        Search for most similar documents.\n",
        "\n",
        "        Args:\n",
        "            query_vector (List[float]): Query embedding vector\n",
        "            top_k (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of results with documents, scores, and metadata\n",
        "        \"\"\"\n",
        "        if not self.vectors or not self.documents:\n",
        "            return []\n",
        "\n",
        "        # Convert query vector to numpy array\n",
        "        query_vector = query_vector.reshape(1, -1)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for i, vector in enumerate(self.vectors):\n",
        "            if vector is not None:\n",
        "                vector = vector.reshape(1, -1)\n",
        "                sim = cosine_similarity(query_vector, vector)[0][0]  # Extract scalar\n",
        "                similarities.append((i, sim))\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top-k results\n",
        "        results = []\n",
        "        for i, score in similarities[:top_k]:\n",
        "            results.append({\n",
        "                \"document\": self.documents[i],\n",
        "                \"score\": float(score),\n",
        "                \"metadata\": self.metadata[i]\n",
        "            })\n",
        "\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pNeBBtQcuLn"
      },
      "source": [
        "#### Define **create_embedding** function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b8Mr-YegmUA"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(text):\n",
        "    \"\"\"\n",
        "    Create embeddings for text using the loaded embedding model.\n",
        "\n",
        "    Args:\n",
        "        text: str or list of str - input text(s) to embed\n",
        "\n",
        "    Returns:\n",
        "        list[np.ndarray]: list of normalized embeddings, each of shape (dim,)\n",
        "    \"\"\"\n",
        "    # Handle single string input\n",
        "    is_single = isinstance(text, str)\n",
        "    if is_single: text = [text]\n",
        "\n",
        "    # Tokenize with error handling\n",
        "    try:\n",
        "        inputs = embed_tokenizer(\n",
        "            text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenization error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Generate embeddings with no gradient computation\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            output = embed_model(**inputs)\n",
        "            # Use CLS token embedding [CLS] at position 0\n",
        "            cls_emb = output.last_hidden_state[:, 0, :]\n",
        "            # L2 normalize for cosine similarity\n",
        "            emb_normalized = F.normalize(cls_emb, p=2, dim=1)\n",
        "\n",
        "        # Convert to list of np.ndarray, each of shape (dim,)\n",
        "        embeddings = [emb.cpu().numpy() for emb in emb_normalized]\n",
        "\n",
        "        # Return single embedding if input was single string\n",
        "        return embeddings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Embedding generation error: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wGzulpbc3q0"
      },
      "source": [
        "#### Define document processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C322nIvfihMM"
      },
      "outputs": [],
      "source": [
        "def process_document(pdf_path, chunk_size = 800):\n",
        "  print(\"Extracting text from PDF...\")\n",
        "  text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "  print(\"Chunking text...\")\n",
        "  text_chunks = chunk_text(text, chunk_size = chunk_size, overlap = 200)\n",
        "\n",
        "  print(\"Creating embeddings for chunks...\")\n",
        "  embeddings = create_embeddings(text_chunks)\n",
        "\n",
        "  vector_store = SimpleVectorStore()\n",
        "\n",
        "  metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(text_chunks))]\n",
        "\n",
        "  vector_store.add_documents(text_chunks, embeddings, metadata)\n",
        "\n",
        "  doc_info = {\n",
        "      \"chunks\": text_chunks,\n",
        "      \"source\": pdf_path\n",
        "  }\n",
        "\n",
        "  return text_chunks, vector_store, doc_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yl14VCT1RgX"
      },
      "source": [
        "#### **RSE Core Algorithm: Computing Chunk Values and Finding Best Segments**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDOWHF5ujgnc"
      },
      "outputs": [],
      "source": [
        "def calculate_chunk_values(query, text_chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
        "  \"\"\"\n",
        "  Calculate chunk values by combining relevance and position.\n",
        "\n",
        "  Args:\n",
        "    query (str): Query text\n",
        "    chunks (List[str]): List of document chunks\n",
        "    vector_store (SimpleVectorStore): Vector store containing the chunks\n",
        "    irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
        "\n",
        "  Returns:\n",
        "    List[float]: List of chunk values\n",
        "  \"\"\"\n",
        "  query_embedding = create_embeddings(query)[0] #nda(dim,)\n",
        "\n",
        "  num_chunks = len(text_chunks)\n",
        "\n",
        "  results = vector_store.search(query_embedding, top_k = num_chunks)\n",
        "\n",
        "  relevance_score = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
        "\n",
        "  chunk_values = []\n",
        "\n",
        "  for i in range(num_chunks):\n",
        "    score = relevance_score.get(i,0.0)\n",
        "    value = score - irrelevant_chunk_penalty\n",
        "    chunk_values.append(value)\n",
        "\n",
        "  return chunk_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ZspQehtCkk0z"
      },
      "outputs": [],
      "source": [
        "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
        "    \"\"\"\n",
        "    Find the best segments using a variant of the maximum sum subarray algorithm.\n",
        "\n",
        "    Args:\n",
        "        chunk_values (List[float]): Values for each chunk\n",
        "        max_segment_length (int): Maximum length of a single segment\n",
        "        total_max_length (int): Maximum total length across all segments\n",
        "        min_segment_value (float): Minimum value for a segment to be considered\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[int, int]]: List of (start, end) indices for best segments\n",
        "    \"\"\"\n",
        "    print(\"Finding optimal continuous text segments...\")\n",
        "\n",
        "    best_segments = []\n",
        "    segment_scores = []\n",
        "    total_included_chunks = 0\n",
        "\n",
        "    # Keep finding segments until we hit our limits\n",
        "    while total_included_chunks < total_max_length:\n",
        "        best_score = min_segment_value  # Minimum threshold for a segment\n",
        "        best_segment = None\n",
        "\n",
        "        # Try each possible starting position\n",
        "        for start in range(len(chunk_values)):\n",
        "            # Skip if this start position is already in a selected segment\n",
        "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
        "                continue\n",
        "\n",
        "            # Try each possible segment length\n",
        "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
        "                end = start + length\n",
        "\n",
        "                # Skip if end position is already in a selected segment\n",
        "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
        "                    continue\n",
        "\n",
        "                # Calculate segment value as sum of chunk values\n",
        "                segment_value = sum(chunk_values[start:end])\n",
        "\n",
        "                # Update best segment if this one is better\n",
        "                if segment_value > best_score:\n",
        "                    best_score = segment_value\n",
        "                    best_segment = (start, end)\n",
        "\n",
        "        # If we found a good segment, add it\n",
        "        if best_segment:\n",
        "            best_segments.append(best_segment)\n",
        "            segment_scores.append(best_score)\n",
        "            total_included_chunks += best_segment[1] - best_segment[0]\n",
        "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
        "        else:\n",
        "            # No more good segments to find\n",
        "            break\n",
        "\n",
        "    # Sort segments by their starting position for readability\n",
        "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
        "\n",
        "    return best_segments, segment_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reconstructing and Using Segments for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "YlJs1DpJ1NN4"
      },
      "outputs": [],
      "source": [
        "def reconstruct_segments(chunks, best_segments):\n",
        "    \"\"\"\n",
        "    Reconstruct text segments based on chunk indices.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of all document chunks\n",
        "        best_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of reconstructed text segments\n",
        "    \"\"\"\n",
        "    reconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\n",
        "\n",
        "    for start, end in best_segments:\n",
        "        # Join the chunks in this segment to form the complete segment text\n",
        "        segment_text = \" \".join(chunks[start:end])\n",
        "        # Append the segment text and its range to the reconstructed_segments list\n",
        "        reconstructed_segments.append({\n",
        "            \"text\": segment_text,\n",
        "            \"segment_range\": (start, end),\n",
        "        })\n",
        "\n",
        "    return reconstructed_segments  # Return the list of reconstructed text segments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "l6Fao8oikoIu"
      },
      "outputs": [],
      "source": [
        "def format_segments_for_context(segments):\n",
        "    \"\"\"\n",
        "    Format segments into a context string for the LLM.\n",
        "\n",
        "    Args:\n",
        "        segments (List[Dict]): List of segment dictionaries\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted context text\n",
        "    \"\"\"\n",
        "    context = []  # Initialize an empty list to store the formatted context\n",
        "\n",
        "    for i, segment in enumerate(segments):\n",
        "        # Create a header for each segment with its index and chunk range\n",
        "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
        "        context.append(segment_header)  # Add the segment header to the context list\n",
        "        context.append(segment['text'])  # Add the segment text to the context list\n",
        "        context.append(\"-\" * 80)  # Add a separator line for readability\n",
        "\n",
        "    # Join all elements in the context list with double newlines and return the result\n",
        "    return \"\\n\\n\".join(context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUpf1K6Lkt2w"
      },
      "outputs": [],
      "source": [
        "def gen(system_prompt, user_prompt, temperature=0):\n",
        "    text = gen_tokenizer.apply_chat_template(\n",
        "        conversation=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = gen_tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    if temperature > 0:\n",
        "        generated_ids = gen_model.generate(\n",
        "            **model_inputs,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            max_new_tokens=1024\n",
        "        )\n",
        "    else:\n",
        "        generated_ids = gen_model.generate(\n",
        "            **model_inputs,\n",
        "            do_sample=False,\n",
        "            temperature= None,\n",
        "            top_p=None,  # disable top_p and temperature to not receive warning\n",
        "            max_new_tokens=1024\n",
        "        )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = gen_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generating Responses with RSE Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UXUWvKdckszA"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context text from relevant segments\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    print(\"Generating response using relevant segments as context...\")\n",
        "\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "    The context consists of document segments that have been retrieved as relevant to the user's query.\n",
        "    Use the information from these segments to provide a comprehensive and accurate answer.\n",
        "    If the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n",
        "\n",
        "    # Create the user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a helpful answer based on the context provided.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate the response using the specified model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Completed pipeline for RSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "jSftugvsk5z-"
      },
      "outputs": [],
      "source": [
        "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline with Relevant Segment Extraction.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        query (str): User query\n",
        "        chunk_size (int): Size of chunks\n",
        "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
        "\n",
        "    Returns:\n",
        "        Dict: Result with query, segments, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
        "\n",
        "    # Calculate relevance scores and chunk values based on the query\n",
        "    print(\"\\nCalculating relevance scores and chunk values...\")\n",
        "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
        "\n",
        "    # Find the best segments of text based on chunk values\n",
        "    best_segments, scores = find_best_segments(\n",
        "        chunk_values,\n",
        "        max_segment_length=20,\n",
        "        total_max_length=30,\n",
        "        min_segment_value=0.2\n",
        "    )\n",
        "\n",
        "    # Reconstruct text segments from the best chunks\n",
        "    print(\"\\nReconstructing text segments from chunks...\")\n",
        "    segments = reconstruct_segments(chunks, best_segments)\n",
        "\n",
        "    # Format the segments into a context string for the language model\n",
        "    context = format_segments_for_context(segments)\n",
        "\n",
        "    # Generate a response from the language model using the context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Compile the result into a dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"segments\": segments,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== FINAL RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparing with Standard Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbKwLVyok9Lm"
      },
      "outputs": [],
      "source": [
        "def standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n",
        "    \"\"\"\n",
        "    Standard RAG with top-k retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        query (str): User query\n",
        "        k (int): Number of chunks to retrieve\n",
        "        chunk_size (int): Size of chunks\n",
        "\n",
        "    Returns:\n",
        "        Dict: Result with query, chunks, and response\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Process the document to extract text, chunk it, and create embeddings\n",
        "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
        "\n",
        "    # Create an embedding for the query\n",
        "    print(\"Creating query embedding and retrieving chunks...\")\n",
        "    query_embedding = create_embeddings([query])[0]\n",
        "\n",
        "    # Retrieve the top-k most relevant chunks based on the query embedding\n",
        "    results = vector_store.search(query_embedding, top_k=k)\n",
        "    retrieved_chunks = [result[\"document\"] for result in results]\n",
        "\n",
        "    # Format the retrieved chunks into a context string\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"CHUNK {i+1}:\\n{chunk}\"\n",
        "        for i, chunk in enumerate(retrieved_chunks)\n",
        "    ])\n",
        "\n",
        "    # Generate a response from the language model using the context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Compile the result into a dictionary\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"chunks\": retrieved_chunks,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== FINAL RESPONSE ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "AVgfw2U4lBBq"
      },
      "outputs": [],
      "source": [
        "def evaluate_methods(pdf_path, query, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Compare RSE with standard top-k retrieval.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the document\n",
        "        query (str): User query\n",
        "        reference_answer (str, optional): Reference answer for evaluation\n",
        "    \"\"\"\n",
        "    print(\"\\n========= EVALUATION =========\\n\")\n",
        "\n",
        "    # Run the RAG with Relevant Segment Extraction (RSE) method\n",
        "    rse_result = rag_with_rse(pdf_path, query)\n",
        "\n",
        "    # Run the standard top-k retrieval method\n",
        "    standard_result = standard_top_k_retrieval(pdf_path, query)\n",
        "\n",
        "    # If a reference answer is provided, evaluate the responses\n",
        "    if reference_answer:\n",
        "        print(\"\\n=== COMPARING RESULTS ===\")\n",
        "\n",
        "        # Create an evaluation prompt to compare the responses against the reference answer\n",
        "        evaluation_prompt = f\"\"\"\n",
        "            Query: {query}\n",
        "\n",
        "            Reference Answer:\n",
        "            {reference_answer}\n",
        "\n",
        "            Response from Standard Retrieval:\n",
        "            {standard_result[\"response\"]}\n",
        "\n",
        "            Response from Relevant Segment Extraction:\n",
        "            {rse_result[\"response\"]}\n",
        "\n",
        "            Compare these two responses against the reference answer. Which one is:\n",
        "            1. More accurate and comprehensive\n",
        "            2. Better at addressing the user's query\n",
        "            3. Less likely to include irrelevant information\n",
        "\n",
        "            Explain your reasoning for each point.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Evaluating responses against reference answer...\")\n",
        "        system_prompt = \"You are an objective evaluator of RAG system responses.\"\n",
        "        # Generate the evaluation using the specified model\n",
        "        evaluation = gen(system_prompt, evaluation_prompt)\n",
        "\n",
        "        # Print the evaluation results\n",
        "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "        print(evaluation)\n",
        "\n",
        "    # Return the results of both methods\n",
        "    return {\n",
        "        \"rse_result\": rse_result,\n",
        "        \"standard_result\": standard_result\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX69jgmalN1i",
        "outputId": "12acd364-e278-49e4-f443-f4f9c7297691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========= EVALUATION =========\n",
            "\n",
            "\n",
            "=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\n",
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Creating embeddings for chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calculating relevance scores and chunk values...\n",
            "Finding optimal continuous text segments...\n",
            "Found segment (32, 52) with score 9.0159\n",
            "Found segment (0, 20) with score 8.6664\n",
            "\n",
            "Reconstructing text segments from chunks...\n",
            "Generating response using relevant segments as context...\n",
            "\n",
            "=== FINAL RESPONSE ===\n",
            "Based on the provided context, Explainable AI (XAI) refers to techniques and methods developed to make AI systems more transparent and understandable. The goal of XAI is to provide insights into how AI models make decisions, enhancing trust and accountability in AI systems.\n",
            "\n",
            "Explainable AI is considered important because it addresses the challenges of \"black box\" AI systems, which are often difficult to understand and interpret. By providing explanations for AI decisions, XAI helps users assess the reliability and fairness of AI systems, which is crucial for building trust in AI.\n",
            "\n",
            "XAI is essential for several reasons:\n",
            "\n",
            "1. **Transparency**: Explainable AI provides insights into how AI models make decisions, allowing users to understand the reasoning behind the output.\n",
            "2. **Accountability**: By providing explanations, XAI enables users to hold AI systems accountable for their actions and decisions.\n",
            "3. **Trust**: Explainable AI helps build trust in AI systems by demonstrating that they are fair, transparent, and reliable.\n",
            "4. **Fairness**: XAI can help identify and mitigate biases in AI systems, ensuring that they do not perpetuate existing social inequalities.\n",
            "\n",
            "Overall, Explainable AI is a critical aspect of responsible AI development and deployment, as it enables users to make informed decisions about AI systems and promotes the development of trustworthy and fair AI.\n",
            "\n",
            "=== STARTING STANDARD TOP-K RETRIEVAL ===\n",
            "Query: What is 'Explainable AI' and why is it considered important?\n",
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Creating embeddings for chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating query embedding and retrieving chunks...\n",
            "Generating response using relevant segments as context...\n",
            "\n",
            "=== FINAL RESPONSE ===\n",
            "Based on the provided context, Explainable AI (XAI) refers to techniques that aim to make AI systems more transparent and understandable, enabling users to assess their fairness and accuracy. The primary goal of XAI is to provide insights into how AI models make decisions, enhancing trust and accountability in AI systems.\n",
            "\n",
            "Explainable AI is considered important for several reasons:\n",
            "\n",
            "1. **Building trust**: By providing transparency and explainability, XAI helps users understand how AI models arrive at their decisions, which is essential for building trust in AI systems.\n",
            "2. **Addressing fairness and accuracy**: XAI techniques can help identify biases and errors in AI systems, ensuring that they are fair and accurate.\n",
            "3. **Accountability**: Explainable AI enables accountability by providing a clear understanding of how AI decisions are made, which is crucial for addressing potential harms and ensuring ethical behavior.\n",
            "4. **Improved decision-making**: By providing insights into AI decision-making processes, XAI can facilitate more informed decision-making and better decision support.\n",
            "\n",
            "Overall, Explainable AI is a critical aspect of building trust, ensuring fairness and accuracy, and promoting accountability in AI systems, ultimately leading to a more responsible and beneficial use of AI.\n",
            "\n",
            "=== COMPARING RESULTS ===\n",
            "Evaluating responses against reference answer...\n",
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "After comparing the two responses against the reference answer, I would conclude:\n",
            "\n",
            "1. **More accurate and comprehensive: Response from Standard Retrieval**\n",
            "The response from Standard Retrieval is more accurate and comprehensive. It provides a clear and concise definition of Explainable AI (XAI) and its importance. The response covers the primary goals of XAI, its benefits, and the reasons why it is considered important. The response also provides specific points that are relevant to the user's query, such as building trust, addressing fairness and accuracy, and promoting accountability.\n",
            "\n",
            "The response from Relevant Segment Extraction is more fragmented and lacks a clear structure. It jumps between different points and does not provide a cohesive explanation of Explainable AI. While it covers some of the same points as the Standard Retrieval response, it does not provide as much detail or clarity.\n",
            "\n",
            "2. **Better at addressing the user's query: Response from Standard Retrieval**\n",
            "The response from Standard Retrieval is better at addressing the user's query. It provides a clear and concise definition of Explainable AI and its importance, which directly answers the user's question. The response also covers the primary goals and benefits of XAI, making it more relevant to the user's query.\n",
            "\n",
            "The response from Relevant Segment Extraction is more focused on the general concept of Explainable AI, but it does not provide as much detail or clarity as the Standard Retrieval response. It also jumps between different points, making it less easy to follow.\n",
            "\n",
            "3. **Less likely to include irrelevant information: Response from Relevant Segment Extraction**\n",
            "The response from Relevant Segment Extraction is less likely to include irrelevant information. While it provides some relevant points, such as transparency, accountability, trust, and fairness, it also includes some extraneous information, such as the phrase \"black box\" AI systems, which is not directly related to the user's query.\n",
            "\n",
            "The response from Standard Retrieval does not include any irrelevant information and stays focused on providing a clear and concise explanation of Explainable AI and its importance.\n"
          ]
        }
      ],
      "source": [
        " # Load the validation data from a JSON file\n",
        "with open('data/val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[0]['question']\n",
        "\n",
        "# Extract the reference answer from the validation data\n",
        "reference_answer = data[0]['ideal_answer']\n",
        "\n",
        "# pdf_path\n",
        "pdf_path = \"data/AI_Information.pdf\"\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluate_methods(pdf_path, query, reference_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "K-ULTlVWa5Fz"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
