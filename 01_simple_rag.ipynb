{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4fcf685",
   "metadata": {},
   "source": [
    "In a simple RAG setup, we follow these steps:\n",
    "1. **Data Ingestion**: Load and preprocess text data\n",
    "2. **Chunking**: Break data into smaller chunks to improve retrieval performance\n",
    "3. **Embedding**: Convert the text chunks into numerical representations using an emdedding model\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on user query\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieval text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f8784",
   "metadata": {},
   "source": [
    "### 1. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92793e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeff84f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"conf.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70686f0d",
   "metadata": {},
   "source": [
    "### 2. Ingest data(PDF for this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c485ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in mypdf:\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    return all_text.strip()  # Return the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f0393",
   "metadata": {},
   "source": [
    "### 3. Chunk the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71906188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks # list[text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ea5d9",
   "metadata": {},
   "source": [
    "#### Set up OpenAI Client\n",
    "Access [this](https://aistudio.google.com/apikey) to get your free API key from Google Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b66984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dbbdb",
   "metadata": {},
   "source": [
    "now load pdf, extract text and split it into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d19b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 42\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e7abb",
   "metadata": {},
   "source": [
    "### 4. Emdedding\n",
    "Convert text into numerical vectors, which allow for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e2a52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"gemini-embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    is_string = isinstance(text, str)\n",
    "    input_text = [text] if is_string else text\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # return a list of nda(dim,)\n",
    "    return np.array(response.data[0].embedding) if is_string else [np.array(item.embedding) for item in response.data]\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25d9ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.0049712 ,  0.00908941,  0.02203033, ...,  0.00198734,\n",
       "        -0.00881044, -0.01199376], shape=(3072,)),\n",
       " array([-0.00183863, -0.01369122,  0.02448579, ...,  0.00070203,\n",
       "        -0.00132361, -0.00723084], shape=(3072,)),\n",
       " array([-0.01276625, -0.0080712 , -0.01626536, ..., -0.0035511 ,\n",
       "         0.01299587,  0.00095135], shape=(3072,)),\n",
       " array([-0.01317397, -0.00019218, -0.00754517, ..., -0.00856328,\n",
       "        -0.00673063, -0.01202932], shape=(3072,)),\n",
       " array([ 0.01870531,  0.01208002,  0.00602027, ..., -0.00929642,\n",
       "         0.0001431 , -0.00295126], shape=(3072,))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "629771b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between '0' and '1': 0.8222\n",
      "Similarity between '0' and '2': 0.7588\n",
      "Similarity between '0' and '3': 0.6605\n",
      "Similarity between '0' and '4': 0.6777\n",
      "Similarity between '1' and '2': 0.8194\n",
      "Similarity between '1' and '3': 0.6857\n",
      "Similarity between '1' and '4': 0.7175\n",
      "Similarity between '2' and '3': 0.7799\n",
      "Similarity between '2' and '4': 0.7008\n",
      "Similarity between '3' and '4': 0.7937\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = cosine_similarity(response)\n",
    "\n",
    "for i, text1 in enumerate(text_chunks):\n",
    "    for j in range(i + 1, 5):\n",
    "        text2 = text_chunks[j]\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"Similarity between '{i}' and '{j}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab540f83",
   "metadata": {},
   "source": [
    "implement semantic search to find contextual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a4f5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query) # nda(dim,)\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(query_embedding.reshape(1, -1), chunk_embedding.reshape(1, -1))\n",
    "\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a6644",
   "metadata": {},
   "source": [
    "running a query on extracted chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cb926b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "lainability \n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability. \n",
      " \n",
      "  \n",
      "Privacy and Security \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
      "Protecting sensitive information and ensuring responsible data handling are essential. \n",
      "Job Displacement \n",
      "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
      "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
      "of AI-driven automation is a key challenge. \n",
      "Autonomy and Control \n",
      "As AI systems become more autonomous, questions arise about control, accountability, and the \n",
      "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
      "AI development and deployment is crucial. \n",
      "Weaponization of AI \n",
      "Th\n",
      "=====================================\n",
      "Context 2:\n",
      "lity are key to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "Ensuring that AI systems are robust and reliable is essential for building trust. This includes \n",
      "testing and validating AI models, monitoring their performance, and addressing potential \n",
      "vulnerabilities. \n",
      "User Control and Agency \n",
      "Empowering users with control over AI systems and providing them with agency in their \n",
      "interactions with AI enhances trust. This includes allowing users to customize AI settings, \n",
      "understand how their data is used, and opt out of AI-driven features. \n",
      "Ethical Design and Development \n",
      "Incorporating ethical considerations into the design and development of AI systems is crucial for \n",
      "building trust. This includes conducting ethical impact assessments, engaging stakeholders, and \n",
      "adhering to ethical guidelines and standards. \n",
      "Public Engagement and Education\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Perform semantic search to find the top 2 most relevant text chunks for the query\n",
    "top_chunks = semantic_search(query, text_chunks, response, k=2)\n",
    "\n",
    "# Print the query\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Print the top 2 most relevant text chunks\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed0883",
   "metadata": {},
   "source": [
    "### 5. Generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bddab884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gemini-2.5-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f89c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response_text = ai_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f755b390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainable AI refers to making AI systems understandable and providing insights into their decision-making processes. It is considered important because enhancing transparency and explainability is crucial for building trust and accountability, and it helps users assess the reliability and fairness of AI systems.\n"
     ]
    }
   ],
   "source": [
    "print(ai_response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd60a8",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cf5b4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6b72d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00706063,  0.01645437,  0.00187743, ..., -0.0280184 ,\n",
       "        0.02375095, -0.00603804], shape=(3072,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_gen = np.array(create_embeddings(ai_response.choices[0].message.content))\n",
    "ai_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63f01b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00634933,  0.01132035,  0.00591608, ..., -0.02739728,\n",
       "        0.02021963, -0.00203079], shape=(3072,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_re = np.array(create_embeddings(data[0]['ideal_answer']))\n",
    "ideal_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a1d8ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9472308]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(ai_gen.reshape(1, -1), ideal_re.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7f20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
