{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Feedback Loop in RAG\n",
        "In this notebook, I implement a RAG system with a feedback loop mechanism that continuously improves over time. By collecting and incorporating user feedback, our system learns to provide more relevant and higher-quality responses with each interaction.\n",
        "\n",
        "Traditional RAG systems are static - they retrieve information based solely on embedding similarity. With a feedback loop, we create a dynamic system that:\n",
        "- Remembers what worked (and what didn't)\n",
        "- Adjusts document relevance scores over time\n",
        "- Incorporates successful Q&A pairs into its knowledge base\n",
        "- Gets smarter with each user interaction"
      ],
      "metadata": {
        "id": "4NcGssnfywqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymupdf\n",
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S85Ocrly_En",
        "outputId": "80bba6ee-38cd-454e-bbbb-c96e5297220e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KJzs3n9oyqtz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define models"
      ],
      "metadata": {
        "id": "vJ2fkLnCzitY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    torch_dtype = \"auto\",\n",
        "    device_map = \"auto\"\n",
        ")\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
        "\n",
        "embed_model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\").to(device)\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688,
          "referenced_widgets": [
            "0078c331f2d14f4f80fbcc3472222f70",
            "d841add063514b8f96e0fb0431b8e4b7",
            "e280b2f4a0054fbf9f2f6921b0468631",
            "1a67f96e3cdc4ad5a957717feca06c33",
            "d99070a15494487fa862ad9d5d7b860d",
            "48bfca1f1a694095b06c7a46de3e7eec",
            "075a7bfedac141b6a710ddcce24f1853",
            "ab0a3b9d2c4b4a13b074b11545c9a2a2",
            "ab97108d37c74e78ada6688ef848aeaa",
            "0a3ee95b17834c22b003c874ad3151be",
            "b4677cd8380442548db6299ab406d928",
            "37c69fc3e467426abadacab647ec35d6",
            "90675ed8dfc040e9898cee4c79a26a5e",
            "e51b5a5ae7354c03b6f86cbbe38ed4e4",
            "74804a237a4044179768ec2aaa0f745b",
            "ce1b43a3640049f6a8e227573a224044",
            "b68bb232a08849948384fb2838ffce11",
            "f90843d8d3714c5e8cb0f73caabea399",
            "5e502ecab2dd424f9784d4613555e83c",
            "0c50a6119cb84daf84f1a34b91517b65",
            "c17ab57f1e5b4dcd9b878d330088bcf4",
            "2f50d79e11f14990aa6c1d034dce17bf",
            "f67efce34b764a2ab11b0b147ef78ef8",
            "8c872f8571e84039a650a8c29e5691e2",
            "a23bf71f2c274a89975d4b02052ec060",
            "5e1c6ff408804779901a3b0f5bf6b9fd",
            "cd10d350df4046b7963492dab0a231cc",
            "6342655729ef48af9e99f6d132e9669a",
            "bb6278adc9674a23bb14d74f35c3005b",
            "8f62a017001242088e8a0888b53f6d2d",
            "1505e5bd0d934ebf8d1863b97d9dd60a",
            "19231ef10e7a40d09d221027b7c28a08",
            "d29ceb3164b840d698b961b6c266c497",
            "be87fbb4d5f543e8a73cd2be810b0e21",
            "407f9f9a5e9e41919e8ae695a65f757b",
            "3e111385a9204a7099ee715ee9dd9308",
            "038861ec1c894a9f899a6fc769ba8823",
            "0031e37a1b7448b68a3b5935d232b9cb",
            "4d462a3cf7f54102b8480bdbff155db5",
            "8c0752c9692044b2acd2d0294100a9a1",
            "61d156d3dc2c477d824c2ee7a95103f4",
            "bbdb3effa40c492590f63730a5326625",
            "1a768f496dfa443fa981eaa5fd87ac21",
            "792fccb206034b0da327e12807af5fea",
            "66716c1c04c44e3a97414c41e47de82f",
            "5078bd6d73714fa99c9b7a0d3ec1af37",
            "42c6758bd9d34149975df322fc9e6344",
            "bda1584c8e6c4286abb1b598280e10c6",
            "aa5b5788b61c425689f596952f68fbba",
            "17db6345b78f438994c170ea3ff20fd6",
            "36471fe8eacf4c508918f0e52529ea03",
            "382a471c5b63473499cdc32c7f9b61ee",
            "892864b2b55148ad83f5c75011aba4d3",
            "d7b8ebeb6ea942a18f67359726c18923",
            "1e6feda50e284f16a90863cb334e454e",
            "5e49c35bf9e543a59341fc0c1ea82d13",
            "d032cd9eacd74024b036be94fd54a700",
            "df924a57672d4743a25824aeef5e1214",
            "dcb7e976c36b47088d905faf6754f2d5",
            "e451e3bf010b4c20a580042d086afb12",
            "f8eb8b5b84c042fab64fec1ec13591db",
            "fcf8d853a11249b1b78c05369900cbb3",
            "eaeb62f3662f4fce8c98f5e28008be71",
            "10df869499de440ca2ae126e02b9331b",
            "d4bcaee504cf4316a65dedfd1905f9de",
            "8282455a72b9400383d83c02a66f6e20",
            "fff891a5ca3548de87419e13ab6ca466",
            "a48fe28e1d8e4b10a2e76091c7a5c6a3",
            "c7d5b79b6715400f8f55210cc43e72b1",
            "22d5eea961e54b93a06408f2e39d1e75",
            "21e21b66668f4008b3eb15a02d22d3a7",
            "ef17ebe1fd8348ed874c1ee73749b8e7",
            "e953056cb064464fb317b239ca9081ed",
            "cc987db090fd43ff9c35dd9916345ecc",
            "83ecafafebda46eb9c4c196c31259839",
            "cc859a8004424bf9a74457d65f797736",
            "c00e924b29264145a67c5016ae320fcf",
            "e1801a3e8ba04976b54354e54be2d0ff",
            "a77a46aaa29141428b317f074e5738b0",
            "fcaeb575e3eb4f6bb5299f380600a22c",
            "f5d98497375d4d4cbb03c8d314c4b7c9",
            "ad93d13cf3b8460e8c852183fa26ff7b",
            "5e50b4df8e7047f4b7f8b7133ebe2fc8",
            "84f4586ff4694c0084c86f262fa7c95a",
            "3cf7370e715e44a994160902670b664e",
            "2068e5d39adf4cca86d08a9996788527",
            "cc19169c848d413480196cbdc3a53d52",
            "99f6a5c2eb6143ae8d86f0cc375feed7",
            "a98b104063de41eca265c8e7ca4a319e",
            "6d64c54f540f463cb9bd7665b1b2dc50",
            "9acda79ea1134cf1b9d0b967a6388792",
            "d3376e6be4374fc4bd6c7c96af67b9c3",
            "6dc16019a0ed417c8e6d02be34d25041",
            "06cb9e8acfa14c2a988a74bcb1867e7d",
            "6767e847799049a39316c19d19231781",
            "0da204b875f944a59922aea4b6ed5e57",
            "9a633e7519e14da4b569ae7fd2595a06",
            "75461d60a6754e88a4423653961843dd",
            "de19340ad479413fb993fe7a0fb93620",
            "d441be08a19a422fb10165ac11e6e232",
            "e1254ce11474428a8d96fd3340163838",
            "8d848f6ef12d489b8e5326e6031d2fc4",
            "36bdedfe3f1645a48da3dc139aec7b5c",
            "d02ff72dfb0b4b9e860427f9d9e4d7a3",
            "4772798ac5e34eb2998f7dcabb12dceb",
            "84b0144a4c1245249c4361cccae8ebc1",
            "8842f61bddce439fb19b800d4dad91bd",
            "1e05e7d61b47461dbe4dda0915942add",
            "afe0bed916384d748e5f7e079e14c6bb",
            "4bee99aa604a41fb985f8ca469ca06ee",
            "91c8e532d9f84a3585089d6fa13e48b0",
            "afbdb4bffdfd4a759d3d014e9af9caa6",
            "b64e4e9388b04bf781153f459253def3",
            "0f6fc24169a04bae99c6dc44046f9174",
            "12ad8cb515ab4946957d5c8d3e2f86d0",
            "3449fdf1d28d44fdb9bbc8fcaf607210",
            "59ef2196fb8243a7aee59e5fd18e93be",
            "180d0bc4512d43bd9dc6f3c2af8f4859",
            "eb44807652724bb0aa4a64075f1c738b",
            "d0c69fb7200f405296c3c94ff07528de",
            "6c4bdb3f85a94eb1a053c22d1bc59bc8",
            "2115e83032294af695b9a43ad28b5837",
            "b74d5fc1365043d894e9c9bc995a28dd",
            "12938cd0b8494887b4c55ff45a80cbd3",
            "7a8ab6605c704a809a57ab7191040d79",
            "8e890d9467274c2baed37bc9fea07138",
            "2d6a3bfb817740d4a098b0d6814f2cef",
            "38637bc9a7a04076bab53ccfc0019f36",
            "5a4b083313b34d75b4282dc304bf60a7",
            "3401fd38f55d44c9aed8a0f5e6dfa11d",
            "96a9088678924a748ac090684b52518a",
            "b2cf2c44f80d43618a9f1a3497e4a006",
            "1f635012e8b54367be91f8e97989f6c1",
            "72626c1d62a64923a53e4e83dbfc30ed",
            "70b42b63c8304b9588dc8c3a2d0b41bb",
            "2159568212a24f5ca8fb4f4998e51a52",
            "465f0ce1d9a74ca7aa89fe93b3d754d2",
            "059a7be136264088939339f08213976f",
            "ed596c8db25849b38ca10371e67ebd0a",
            "e55cb60522484a9491f41dc1cb323f00",
            "dbebbdf5928d455fb81cce428a752054",
            "d7942428406f41409162568535d7a803",
            "860d0d6a9aba47a8a173db9b56e6a3b6",
            "afd856dc51b34eea931508bc17e0f5e7",
            "63997ed3c36144e795d06393deda8a56",
            "0bd9684ce1274a3c8e12656c8d2c432f",
            "e80facbb6ee34a12a1156594b5191b05",
            "5525ef2a5ef449788fd1f7b6ed96537d",
            "5b36f5ff581344e7840edabed15f3aaa",
            "65543fc169304505a896cf5cca18f350",
            "d8d3ea9698b24b7693bac60008be1b2f",
            "e05a851d747f430cb0a9900a48e2d24e",
            "91b8e965ed9a408c9781fe600f9ea423",
            "95464f6a047a4bd4974d5f277df1b020",
            "eea1a722032942949f4c4ec8f283256a",
            "0fceba18e7bb41acbf271b529498c271",
            "5e6c336f52ca408a87563f8c3e59705f",
            "a94c103463464f65bb06aaa579946bcd",
            "7c943a823c834aef92adf187fb36eb9c",
            "b0a16fe637404a85b15cce59a17c5441",
            "3fd7a3f176f04da297398610716022ef",
            "7e6f4afa26cd4fa9beea9d2c11b2def8",
            "62370586a1af415bbf4a5b00ed170424",
            "547661a5200948f9b51d67772bef4bb8",
            "a9c3318cf65948a39b5dc21c46473eaf",
            "22846e4eaeee45629db8bf69ea7a01d8",
            "bf63397fa69446119ec21c2ce2725c83",
            "80c11e4bfbed4f6c91e379b234de366b",
            "34441d46140a44109402459d6f01fc09",
            "7ec1ee4092044523b1e82be28362084a",
            "a8b43e218b9d49cbb28ecfc39e243660",
            "fde30c2d077b482299c269ca23fa23af",
            "308c0c9e3c5f47cbaaf21b5b61b0bfcd",
            "982b8554f4bb481fbc4e8710fafc14ae",
            "01016f20a8ea4aeb95bf50f10c3fb75a",
            "f6eafeac956447fca496a68571683be9",
            "9e7fad119b1d41eba5a65205a6e67b39",
            "da3e4ab346954563ab9552305ea68eaa",
            "e3fcb290736347a4adccf63f40e2417b",
            "1fe91761596044528055317b614c8d18",
            "a1ac2e87410542b3a53cb4e40f8072cb",
            "8f5503ede37f47ccb3906bb823299972",
            "b5267dc6ec2a4dab89bc4de0eae26abf",
            "c09631d13732476483aeeef4f75f1008",
            "903f56a38e0a44138ec3599b29bc8b3b",
            "5ce5ca4bcfd6469bb12b73783492e0e5",
            "ffb9fec174a4430eb908ff44d643120f"
          ]
        },
        "id": "aegV_K0hzlGu",
        "outputId": "31988023-a66b-46ea-949f-8735f4d24250"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0078c331f2d14f4f80fbcc3472222f70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37c69fc3e467426abadacab647ec35d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f67efce34b764a2ab11b0b147ef78ef8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be87fbb4d5f543e8a73cd2be810b0e21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66716c1c04c44e3a97414c41e47de82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e49c35bf9e543a59341fc0c1ea82d13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fff891a5ca3548de87419e13ab6ca466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1801a3e8ba04976b54354e54be2d0ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a98b104063de41eca265c8e7ca4a319e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d441be08a19a422fb10165ac11e6e232"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91c8e532d9f84a3585089d6fa13e48b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2115e83032294af695b9a43ad28b5837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f635012e8b54367be91f8e97989f6c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afd856dc51b34eea931508bc17e0f5e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eea1a722032942949f4c4ec8f283256a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22846e4eaeee45629db8bf69ea7a01d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e7fad119b1d41eba5a65205a6e67b39"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define some necessary functions(create embeddings, generation)"
      ],
      "metadata": {
        "id": "M1dJoL6Ece0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(text):\n",
        "  is_single = isinstance(text, str)\n",
        "  if is_single: text = [text]\n",
        "\n",
        "  try:\n",
        "    inputs = embed_tokenizer(\n",
        "        text,\n",
        "        padding = True,\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(device)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None\n",
        "\n",
        "  try:\n",
        "    with torch.no_grad():\n",
        "      outputs = embed_model(**inputs)\n",
        "      cls = outputs.last_hidden_state[:, 0, :]\n",
        "      embed_normalized = F.normalize(cls, p = 2, dim = 1)\n",
        "    embeddings = [embed.cpu().numpy() for embed in embed_normalized]\n",
        "\n",
        "    return embeddings\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None"
      ],
      "metadata": {
        "id": "0OzTZM4Y0SBk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(system_prompt, user_prompt): # work with unsloth/Llama-3.2-3B-Instruct\n",
        "    text = gen_tokenizer.apply_chat_template(\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            }\n",
        "        ],\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = False\n",
        "    )\n",
        "\n",
        "    model_inputs = gen_tokenizer([text], return_tensors = \"pt\").to(device)\n",
        "\n",
        "    generated_ids = gen_model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample = True\n",
        "    ).to(device)\n",
        "\n",
        "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "\n",
        "    response =  gen_tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0].strip(\"assistant\\n\\n\")\n",
        "\n",
        "    # print(\"===========================================\")\n",
        "    # print(f\"resposne: \\n{response}\")\n",
        "    # print(\"===========================================\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "4QadOYBo1o_6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen(\"you are a dentist\", \"why my teeth hurts?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "Rymjma6mW8zT",
        "outputId": "69b54502-f58e-4557-dfc6-4bfcc834ec1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm so sorry to hear that your teeth are hurting. There are many possible reasons why your teeth might be aching, and I'll do my best to help you identify some potential causes.\\n\\nHere are some common reasons why teeth might hurt:\\n\\n1. **Tooth Decay or Cavities**: Bacteria in your mouth can cause cavities, which can lead to toothaches.\\n2. **Gum Disease**: Gingivitis or periodontitis can cause inflammation and pain in the gums, which can radiate to the teeth.\\n3. **Cracked or Chipped Tooth**: A cracked or chipped tooth can expose the pulp, leading to sensitivity and pain.\\n4. **Tooth Grinding or Clenching**: Grinding or clenching your teeth can cause wear and tear on the enamel, leading to sensitivity and pain.\\n5. **Dental Work**: If you've recently had dental work such as a filling, crown, or root canal, it's normal to experience some sensitivity or discomfort.\\n6. **Food Traps**: Stuck food particles can cause irritation and inflammation, leading to pain.\\n7. **Misaligned Teeth**: Teeth that are not properly aligned can put pressure on the surrounding teeth and gums, leading to pain.\\n8. **Hormonal Changes**: Hormonal fluctuations during pregnancy, menstruation, or menopause can cause tooth sensitivity.\\n9. **Dry Mouth**: A lack of saliva can lead to dry mouth, which can cause tooth sensitivity.\\n10. **Other Medical Conditions**: Certain medical conditions, such as arthritis, temporomandibular joint (TMJ) disorder, or sinus infections, can cause tooth pain.\\n\\nTo determine the cause of your tooth pain, I would need to examine your teeth and gums, take a medical history, and possibly take some X-rays.\\n\\nIn the meantime, here are some things you can try to alleviate the pain:\\n\\n* Apply a cold compress to the affected area to reduce swelling and ease pain.\\n* Take over-the-counter pain relievers such as ibuprofen or acetaminophen.\\n* Use a desensitizing toothpaste or mouthwash to help reduce sensitivity.\\n* Avoid hot or cold foods and drinks that can exacerbate the pain.\\n* Practice good oral hygiene by brushing and flossing regularly.\\n\\nPlease schedule an appointment with me as soon as possible so we can further investigate the cause of your tooth pain and develop a treatment plan to get you feeling comfortable and pain-free again.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector store"
      ],
      "metadata": {
        "id": "23Zei5CDcno7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVectorStore:\n",
        "  def __init__(self):\n",
        "    self.vectors = []\n",
        "    self.texts = []\n",
        "    self.metadata = []\n",
        "\n",
        "  def add_item(self, vector, text, metadata = None):\n",
        "    self.vectors.append(vector)\n",
        "    self.texts.append(text)\n",
        "    self.metadata.append(metadata or {})\n",
        "\n",
        "  def similarity_search(self, query_embedding, k = 5, filter_func = None):\n",
        "    \"\"\"\n",
        "    Find the most similar items to a query embedding using cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query_embedding nda(dim,): Query embedding vector to compare against stored vectors.\n",
        "        k (int): Number of most similar results to return.\n",
        "        filter_func (callable, optional): Function to filter results based on metadata.\n",
        "                                          Takes metadata dict as input and returns boolean.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Top k most similar items, each containing:\n",
        "            - text: The original text\n",
        "            - metadata: Associated metadata\n",
        "            - similarity: Raw cosine similarity score\n",
        "            - relevance_score: Either metadata-based relevance or calculated similarity\n",
        "\n",
        "    Note: Returns empty list if no vectors are stored or none pass the filter.\n",
        "    \"\"\"\n",
        "    if not self.vectors: return []\n",
        "\n",
        "    similarities = [\n",
        "        (i, cosine_similarity(query_embedding.reshape(1, -1), vector.reshape(1, -1))[0][0])\n",
        "        for i, vector in enumerate(self.vectors)\n",
        "    ]\n",
        "\n",
        "    similarities.sort(key = lambda x:x[1], reverse = True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(min(k, len(similarities))):\n",
        "      idx, score = similarities[i]\n",
        "      results.append({\n",
        "          \"text\": self.texts[idx],\n",
        "          \"metadata\": self.metadata[idx],\n",
        "          \"similarity\": score,\n",
        "          \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n",
        "      })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "o1cFFOZk3PdH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Process data"
      ],
      "metadata": {
        "id": "JOIpNW_Acsx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(pdf_path, chunk_size = 1000, overlap = 200):\n",
        "  print(\"Extracting text...\")\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = \"\"\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "  print(f\"Texts length: {len(text)}\")\n",
        "\n",
        "  print(\"Chunking text...\")\n",
        "  chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]\n",
        "  print(f\"Chunks length: {len(chunks)}\")\n",
        "  print(\"Creating embeddings\")\n",
        "  embeddings = create_embeddings(chunks)\n",
        "  print(f\"Embeddings length: {len(embeddings)}\")\n",
        "  print(\"Create vector store:\")\n",
        "  store = SimpleVectorStore()\n",
        "\n",
        "  for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "    store.add_item(\n",
        "        text = chunk,\n",
        "        vector = embedding,\n",
        "        metadata = {\n",
        "            \"index\": i,\n",
        "            \"source\": pdf_path,\n",
        "            \"relevance_score\": 1.0, #initial relevance score that will be updated with feedback\n",
        "            \"feedback_count\": 0 # counter for chunk's received feedback\n",
        "        }\n",
        "    )\n",
        "  print(f\"Added {len(chunks)}\")\n",
        "  return chunks, store"
      ],
      "metadata": {
        "id": "BVNqAk4i5KWB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feedback processing\n"
      ],
      "metadata": {
        "id": "e1Hi_oAmdqJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
        "    \"\"\"\n",
        "    Format user feedback in a dictionary.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query\n",
        "        response (str): System's response\n",
        "        relevance (int): Relevance score (1-5)\n",
        "        quality (int): Quality score (1-5)\n",
        "        comments (str): Optional feedback comments\n",
        "\n",
        "    Returns:\n",
        "        Dict: Formatted feedback\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": response,\n",
        "        \"relevance\": int(relevance),\n",
        "        \"quality\": int(quality),\n",
        "        \"comments\": comments,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "9y5Z9AqV683D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n",
        "    \"\"\"\n",
        "    Store feedback in a JSON file.\n",
        "\n",
        "    Args:\n",
        "        feedback (Dict): Feedback data\n",
        "        feedback_file (str): Path to feedback file\n",
        "    \"\"\"\n",
        "    with open(feedback_file, \"a\") as f:\n",
        "        json.dump(feedback, f)\n",
        "        f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "I9CqY_pmPfbW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_feedback_data(feedback_file=\"feedback_data.json\"):\n",
        "    \"\"\"\n",
        "    Load feedback data from file.\n",
        "\n",
        "    Args:\n",
        "        feedback_file (str): Path to feedback file\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of feedback entries\n",
        "    \"\"\"\n",
        "    feedback_data = []\n",
        "    try:\n",
        "        with open(feedback_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    feedback_data.append(json.loads(line.strip()))\n",
        "    except FileNotFoundError:\n",
        "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
        "\n",
        "    return feedback_data\n"
      ],
      "metadata": {
        "id": "iXPIKnZ4PjHK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_feedback_relevance(query, doc_text, feedback):\n",
        "    \"\"\"\n",
        "    Use LLM to assess if a past feedback entry is relevant to the current query and document.\n",
        "\n",
        "    This function helps determine which past feedback should influence the current retrieval\n",
        "    by sending the current query, past query+feedback, and document content to an LLM\n",
        "    for relevance assessment.\n",
        "\n",
        "    Args:\n",
        "        query (str): Current user query that needs information retrieval\n",
        "        doc_text (str): Text content of the document being evaluated\n",
        "        feedback (Dict): Previous feedback data containing 'query' and 'response' keys\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the feedback is seemed relevant to current query/document, False otherwise\n",
        "    \"\"\"\n",
        "    # Define system prompt instructing the LLM to make binary relevance judgments only\n",
        "    system_prompt = \"\"\"You are an AI system that determines if a past feedback is relevant to a current query and document.\n",
        "    Answer with ONLY 'yes' or 'no'. Your job is strictly to determine relevance, not to provide explanations.\"\"\"\n",
        "\n",
        "    # Construct user prompt with current query, past feedback data, and truncated document content\n",
        "    user_prompt = f\"\"\"\n",
        "    Current query: {query}\n",
        "    Past query that received feedback: {feedback['query']}\n",
        "    Document content: {doc_text[:500]}... [truncated]\n",
        "    Past response that received feedback: {feedback['response'][:500]}... [truncated]\n",
        "\n",
        "    Is this past feedback relevant to the current query and document? (yes/no)\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the LLM API with zero temperature for deterministic output\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Extract and normalize the response to determine relevance\n",
        "    answer = response.lower()\n",
        "    return 'yes' in answer  # Return True if the answer contains 'yes'"
      ],
      "metadata": {
        "id": "Qw-i7vauPntX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_relevance_scores(query, results, feedback_data):\n",
        "    \"\"\"\n",
        "    Adjust document relevance scores based on historical feedback to improve retrieval quality.\n",
        "\n",
        "    This function analyzes past user feedback to dynamically adjust the relevance scores of\n",
        "    retrieved documents. It identifies feedback that is relevant to the current query context,\n",
        "    calculates score modifiers based on relevance ratings, and re-ranks the results accordingly.\n",
        "\n",
        "    Args:\n",
        "        query (str): Current user query\n",
        "        results (List[Dict]): Retrieved documents with their original similarity scores\n",
        "        feedback_data (List[Dict]): Historical feedback containing user ratings\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Results with adjusted relevance scores, sorted by the new scores\n",
        "    \"\"\"\n",
        "    # If no feedback data available, return original results unchanged\n",
        "    if not feedback_data:\n",
        "        return results\n",
        "\n",
        "    print(\"Adjusting relevance scores based on feedback history...\")\n",
        "\n",
        "    # Process each retrieved document\n",
        "    for i, result in enumerate(results):\n",
        "        document_text = result[\"text\"]\n",
        "        relevant_feedback = []\n",
        "\n",
        "        # Find relevant feedback for this specific document and query combination\n",
        "        # by querying the LLM to assess relevance of each historical feedback item\n",
        "        for feedback in feedback_data:\n",
        "            is_relevant = assess_feedback_relevance(query, document_text, feedback)\n",
        "            if is_relevant:\n",
        "                relevant_feedback.append(feedback)\n",
        "\n",
        "        # Apply score adjustments if relevant feedback exists\n",
        "        if relevant_feedback:\n",
        "            # Calculate average relevance rating from all applicable feedback entries\n",
        "            # Feedback relevance is on a 1-5 scale (1=not relevant, 5=highly relevant)\n",
        "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
        "\n",
        "            # Convert the average relevance to a score modifier in range 0.5-1.5\n",
        "            # - Scores below 3/5 will reduce the original similarity (modifier < 1.0)\n",
        "            # - Scores above 3/5 will increase the original similarity (modifier > 1.0)\n",
        "            modifier = 0.5 + (avg_relevance / 5.0)\n",
        "\n",
        "            # Apply the modifier to the original similarity score\n",
        "            original_score = result[\"similarity\"]\n",
        "            adjusted_score = original_score * modifier\n",
        "\n",
        "            # Update the result dictionary with new scores and feedback metadata\n",
        "            result[\"original_similarity\"] = original_score  # Preserve the original score\n",
        "            result[\"similarity\"] = adjusted_score           # Update the primary score\n",
        "            result[\"relevance_score\"] = adjusted_score      # Update the relevance score\n",
        "            result[\"feedback_applied\"] = True               # Flag that feedback was applied\n",
        "            result[\"feedback_count\"] = len(relevant_feedback)  # Number of feedback entries used\n",
        "\n",
        "            # Log the adjustment details\n",
        "            print(f\"  Document {i+1}: Adjusted score from {original_score:.4f} to {adjusted_score:.4f} based on {len(relevant_feedback)} feedback(s)\")\n",
        "\n",
        "    # Re-sort results by adjusted scores to ensure higher quality matches appear first\n",
        "    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "xiNxV3ioP77t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_index(current_store, chunks, feedback_data):\n",
        "    \"\"\"\n",
        "    Enhance vector store with high-quality feedback to improve retrieval quality over time.\n",
        "\n",
        "    This function implements a continuous learning process by:\n",
        "    1. Identifying high-quality feedback (highly rated Q&A pairs)\n",
        "    2. Creating new retrieval items from successful interactions\n",
        "    3. Adding these to the vector store with boosted relevance weights\n",
        "\n",
        "    Args:\n",
        "        current_store (SimpleVectorStore): Current vector store containing original document chunks\n",
        "        chunks (List[str]): Original document text chunks\n",
        "        feedback_data (List[Dict]): Historical user feedback with relevance and quality ratings\n",
        "\n",
        "    Returns:\n",
        "        SimpleVectorStore: Enhanced vector store containing both original chunks and feedback-derived content\n",
        "    \"\"\"\n",
        "    print(\"Fine-tuning index with high-quality feedback...\")\n",
        "\n",
        "    # Filter for only high-quality responses (both relevance and quality rated 4 or 5)\n",
        "    # This ensures we only learn from the most successful interactions\n",
        "    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
        "\n",
        "    if not good_feedback:\n",
        "        print(\"No high-quality feedback found for fine-tuning.\")\n",
        "        return current_store  # Return original store unchanged if no good feedback exists\n",
        "\n",
        "    # Initialize new store that will contain both original and enhanced content\n",
        "    new_store = SimpleVectorStore()\n",
        "\n",
        "    # First transfer all original document chunks with their existing metadata\n",
        "    for i in range(len(current_store.texts)):\n",
        "        new_store.add_item(\n",
        "            text=current_store.texts[i],\n",
        "            vector=current_store.vectors[i],\n",
        "            metadata=current_store.metadata[i].copy()  # Use copy to prevent reference issues\n",
        "        )\n",
        "\n",
        "    # Create and add enhanced content from good feedback\n",
        "    for feedback in good_feedback:\n",
        "        # Format a new document that combines the question and its high-quality answer\n",
        "        # This creates retrievable content that directly addresses user queries\n",
        "        enhanced_text = f\"Question: {feedback['query']}\\nAnswer: {feedback['response']}\"\n",
        "\n",
        "        # Generate embedding vector for this new synthetic document\n",
        "        embedding = create_embeddings(enhanced_text)[0]\n",
        "\n",
        "        # Add to vector store with special metadata that identifies its origin and importance\n",
        "        new_store.add_item(\n",
        "            text=enhanced_text,\n",
        "            vector=embedding,\n",
        "            metadata={\n",
        "                \"type\": \"feedback_enhanced\",  # Mark as derived from feedback\n",
        "                \"query\": feedback[\"query\"],   # Store original query for reference\n",
        "                \"relevance_score\": 1.2,       # Boost initial relevance to prioritize these items\n",
        "                \"feedback_count\": 1,          # Track feedback incorporation\n",
        "                \"original_feedback\": feedback # Preserve complete feedback record\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Added enhanced content from feedback: {feedback['query'][:50]}...\")\n",
        "\n",
        "    # Log summary statistics about the enhancement\n",
        "    print(f\"Fine-tuned index now has {len(new_store.texts)} items (original: {len(chunks)})\")\n",
        "    return new_store\n"
      ],
      "metadata": {
        "id": "dJO2TnMOQN4_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query, context):\n",
        "    \"\"\"\n",
        "    Generate a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Context text from retrieved documents\n",
        "        model (str): LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
        "\n",
        "    # Create the user prompt by combining the context and the query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the OpenAI API to generate a response based on the system and user prompts\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response"
      ],
      "metadata": {
        "id": "J62ePx4uRiv2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_with_feedback_loop(query, vector_store, feedback_data, k=5):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline incorporating feedback loop.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store with document chunks\n",
        "        feedback_data (List[Dict]): History of feedback\n",
        "        k (int): Number of documents to retrieve\n",
        "        model (str): LLM model for response generation\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, retrieved documents, and response\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Processing query with feedback-enhanced RAG ===\")\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    # Step 1: Create query embedding\n",
        "    query_embedding = create_embeddings(query)\n",
        "\n",
        "    # Step 2: Perform initial retrieval based on query embedding\n",
        "    results = vector_store.similarity_search(query_embedding[0], k=k)\n",
        "\n",
        "    # Step 3: Adjust relevance scores of retrieved documents based on feedback\n",
        "    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n",
        "\n",
        "    # Step 4: Extract texts from adjusted results for context building\n",
        "    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n",
        "\n",
        "    # Step 5: Build context for response generation by concatenating retrieved texts\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "    # Step 6: Generate response using the context and query\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Step 7: Compile the final result\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"retrieved_documents\": adjusted_results,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Response ===\")\n",
        "    print(response)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "f1dDugaXRodZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_rag_workflow(pdf_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n",
        "    \"\"\"\n",
        "    Execute a complete RAG workflow with feedback integration for continuous improvement.\n",
        "\n",
        "    This function orchestrates the entire Retrieval-Augmented Generation process:\n",
        "    1. Load historical feedback data\n",
        "    2. Process and chunk the document\n",
        "    3. Optionally fine-tune the vector index with prior feedback\n",
        "    4. Perform retrieval and generation with feedback-adjusted relevance scores\n",
        "    5. Collect new user feedback for future improvement\n",
        "    6. Store feedback to enable system learning over time\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF document to be processed\n",
        "        query (str): User's natural language query\n",
        "        feedback_data (List[Dict], optional): Pre-loaded feedback data, loads from file if None\n",
        "        feedback_file (str): Path to the JSON file storing feedback history\n",
        "        fine_tune (bool): Whether to enhance the index with successful past Q&A pairs\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results containing the response and retrieval metadata\n",
        "    \"\"\"\n",
        "    # Step 1: Load historical feedback for relevance adjustment if not explicitly provided\n",
        "    if feedback_data is None:\n",
        "        feedback_data = load_feedback_data(feedback_file)\n",
        "        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n",
        "\n",
        "    # Step 2: Process document through extraction, chunking and embedding pipeline\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "    # Step 3: Fine-tune the vector index by incorporating high-quality past interactions\n",
        "    # This creates enhanced retrievable content from successful Q&A pairs\n",
        "    if fine_tune and feedback_data:\n",
        "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
        "\n",
        "    # Step 4: Execute core RAG with feedback-aware retrieval\n",
        "    # Note: This depends on the rag_with_feedback_loop function which should be defined elsewhere\n",
        "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
        "\n",
        "    # Step 5: Collect user feedback to improve future performance\n",
        "    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n",
        "    print(\"Rate relevance (1-5, with 5 being most relevant):\")\n",
        "    relevance = input()\n",
        "\n",
        "    print(\"Rate quality (1-5, with 5 being highest quality):\")\n",
        "    quality = input()\n",
        "\n",
        "    print(\"Any comments? (optional, press Enter to skip)\")\n",
        "    comments = input()\n",
        "\n",
        "    # Step 6: Format feedback into structured data\n",
        "    feedback = get_user_feedback(\n",
        "        query=query,\n",
        "        response=result[\"response\"],\n",
        "        relevance=int(relevance),\n",
        "        quality=int(quality),\n",
        "        comments=comments\n",
        "    )\n",
        "\n",
        "    # Step 7: Persist feedback to enable continuous system learning\n",
        "    store_feedback(feedback, feedback_file)\n",
        "    print(\"Feedback recorded. Thank you!\")\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "curiy8MvR79b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_feedback_loop(pdf_path, test_queries, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Evaluate the impact of feedback loop on RAG quality by comparing performance before and after feedback integration.\n",
        "\n",
        "    This function runs a controlled experiment to measure how incorporating feedback affects retrieval and generation:\n",
        "    1. First round: Run all test queries with no feedback\n",
        "    2. Generate synthetic feedback based on reference answers (if provided)\n",
        "    3. Second round: Run the same queries with feedback-enhanced retrieval\n",
        "    4. Compare results between rounds to quantify feedback impact\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF document used as the knowledge base\n",
        "        test_queries (List[str]): List of test queries to evaluate system performance\n",
        "        reference_answers (List[str], optional): Reference/gold standard answers for evaluation\n",
        "                                                and synthetic feedback generation\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results containing:\n",
        "            - round1_results: Results without feedback\n",
        "            - round2_results: Results with feedback\n",
        "            - comparison: Quantitative comparison metrics between rounds\n",
        "    \"\"\"\n",
        "    print(\"=== Evaluating Feedback Loop Impact ===\")\n",
        "\n",
        "    # Create a temporary feedback file for this evaluation session only\n",
        "    temp_feedback_file = \"temp_evaluation_feedback.json\"\n",
        "\n",
        "    # Initialize feedback collection (empty at the start)\n",
        "    feedback_data = []\n",
        "\n",
        "    # ----------------------- FIRST EVALUATION ROUND -----------------------\n",
        "    # Run all queries without any feedback influence to establish baseline performance\n",
        "    print(\"\\n=== ROUND 1: NO FEEDBACK ===\")\n",
        "    round1_results = []\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "\n",
        "        # Process document to create initial vector store\n",
        "        chunks, vector_store = process_document(pdf_path)\n",
        "\n",
        "        # Execute RAG without feedback influence (empty feedback list)\n",
        "        result = rag_with_feedback_loop(query, vector_store, [])\n",
        "        round1_results.append(result)\n",
        "\n",
        "        # Generate synthetic feedback if reference answers are available\n",
        "        # This simulates user feedback for training the system\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            # Calculate synthetic feedback scores based on similarity to reference answer\n",
        "            similarity_to_ref = cosine_similarity(\n",
        "                create_embeddings(result[\"response\"])[0].reshape(1, -1),\n",
        "                create_embeddings(reference_answers[i])[0].reshape(1, -1)\n",
        "                )[0][0]\n",
        "            # Convert similarity (0-1) to rating scale (1-5)\n",
        "            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n",
        "            quality = max(1, min(5, int(similarity_to_ref * 5)))\n",
        "\n",
        "            # Create structured feedback entry\n",
        "            feedback = get_user_feedback(\n",
        "                query=query,\n",
        "                response=result[\"response\"],\n",
        "                relevance=relevance,\n",
        "                quality=quality,\n",
        "                comments=f\"Synthetic feedback based on reference similarity: {similarity_to_ref:.2f}\"\n",
        "            )\n",
        "\n",
        "            # Add to in-memory collection and persist to temporary file\n",
        "            feedback_data.append(feedback)\n",
        "            store_feedback(feedback, temp_feedback_file)\n",
        "\n",
        "    # ----------------------- SECOND EVALUATION ROUND -----------------------\n",
        "    # Run the same queries with feedback incorporation to measure improvement\n",
        "    print(\"\\n=== ROUND 2: WITH FEEDBACK ===\")\n",
        "    round2_results = []\n",
        "\n",
        "    # Process document and enhance with feedback-derived content\n",
        "    chunks, vector_store = process_document(pdf_path)\n",
        "    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "\n",
        "        # Execute RAG with feedback influence\n",
        "        result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
        "        round2_results.append(result)\n",
        "\n",
        "    # ----------------------- RESULTS ANALYSIS -----------------------\n",
        "    # Compare performance metrics between the two rounds\n",
        "    comparison = compare_results(test_queries, round1_results, round2_results, reference_answers)\n",
        "\n",
        "    # Clean up temporary evaluation artifacts\n",
        "    if os.path.exists(temp_feedback_file):\n",
        "        os.remove(temp_feedback_file)\n",
        "\n",
        "    return {\n",
        "        \"round1_results\": round1_results,\n",
        "        \"round2_results\": round2_results,\n",
        "        \"comparison\": comparison\n",
        "    }\n"
      ],
      "metadata": {
        "id": "z4Jsqx6MSBHM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_results(queries, round1_results, round2_results, reference_answers=None):\n",
        "    \"\"\"\n",
        "    Compare results from two rounds of RAG.\n",
        "\n",
        "    Args:\n",
        "        queries (List[str]): Test queries\n",
        "        round1_results (List[Dict]): Results from round 1\n",
        "        round2_results (List[Dict]): Results from round 2\n",
        "        reference_answers (List[str], optional): Reference answers\n",
        "\n",
        "    Returns:\n",
        "        str: Comparison analysis\n",
        "    \"\"\"\n",
        "    print(\"\\n=== COMPARING RESULTS ===\")\n",
        "\n",
        "    # System prompt to guide the AI's evaluation behavior\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from two versions:\n",
        "        1. Standard RAG: No feedback used\n",
        "        2. Feedback-enhanced RAG: Uses a feedback loop to improve retrieval\n",
        "\n",
        "        Analyze which version provides better responses in terms of:\n",
        "        - Relevance to the query\n",
        "        - Accuracy of information\n",
        "        - Completeness\n",
        "        - Clarity and conciseness\n",
        "    \"\"\"\n",
        "\n",
        "    comparisons = []\n",
        "\n",
        "    # Iterate over each query and its corresponding results from both rounds\n",
        "    for i, (query, r1, r2) in enumerate(zip(queries, round1_results, round2_results)):\n",
        "        # Create a prompt for comparing the responses\n",
        "        comparison_prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "\n",
        "        Standard RAG Response:\n",
        "        {r1[\"response\"]}\n",
        "\n",
        "        Feedback-enhanced RAG Response:\n",
        "        {r2[\"response\"]}\n",
        "        \"\"\"\n",
        "\n",
        "        # Include reference answer if available\n",
        "        if reference_answers and i < len(reference_answers):\n",
        "            comparison_prompt += f\"\"\"\n",
        "            Reference Answer:\n",
        "            {reference_answers[i]}\n",
        "            \"\"\"\n",
        "\n",
        "        comparison_prompt += \"\"\"\n",
        "        Compare these responses and explain which one is better and why.\n",
        "        Focus specifically on how the feedback loop has (or hasn't) improved the response quality.\n",
        "        \"\"\"\n",
        "\n",
        "        # Call the OpenAI API to generate a comparison analysis\n",
        "        response = gen(system_prompt, comparison_prompt)\n",
        "\n",
        "        # Append the comparison analysis to the results\n",
        "        comparisons.append({\n",
        "            \"query\": query,\n",
        "            \"analysis\": response\n",
        "        })\n",
        "\n",
        "        # Print a snippet of the analysis for each query\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "        print(f\"Analysis: {response}...\")\n",
        "\n",
        "    return comparisons\n"
      ],
      "metadata": {
        "id": "5-LmSbH0SOEM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI Document Path\n",
        "pdf_path = \"AI_Information.pdf\"\n",
        "\n",
        "# Define test queries\n",
        "test_queries = [\n",
        "    \"What is a neural network and how does it function?\",\n",
        "\n",
        "    #################################################################################\n",
        "    ### Commented out queries to reduce the number of queries for testing purposes ###\n",
        "\n",
        "    # \"Describe the process and applications of reinforcement learning.\",\n",
        "    # \"What are the main applications of natural language processing in today's technology?\",\n",
        "    # \"Explain the impact of overfitting in machine learning models and how it can be mitigated.\"\n",
        "]\n",
        "\n",
        "# Define reference answers for evaluation\n",
        "reference_answers = [\n",
        "    \"A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. It consists of layers of nodes, with each node representing a neuron. Neural networks function by adjusting the weights of connections between nodes based on the error of the output compared to the expected result.\",\n",
        "\n",
        "    ############################################################################################\n",
        "    #### Commented out reference answers to reduce the number of queries for testing purposes ###\n",
        "\n",
        "#     \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. It involves exploration, exploitation, and learning from the consequences of actions. Applications include robotics, game playing, and autonomous vehicles.\",\n",
        "#     \"The main applications of natural language processing in today's technology include machine translation, sentiment analysis, chatbots, information retrieval, text summarization, and speech recognition. NLP enables machines to understand and generate human language, facilitating human-computer interaction.\",\n",
        "#     \"Overfitting in machine learning models occurs when a model learns the training data too well, capturing noise and outliers. This results in poor generalization to new data, as the model performs well on training data but poorly on unseen data. Mitigation techniques include cross-validation, regularization, pruning, and using more training data.\"\n",
        "]\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = evaluate_feedback_loop(\n",
        "    pdf_path=pdf_path,\n",
        "    test_queries=test_queries,\n",
        "    reference_answers=reference_answers\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-TwiehASW-D",
        "outputId": "1a56c125-0c47-4c53-edfc-23b051354eea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluating Feedback Loop Impact ===\n",
            "\n",
            "=== ROUND 1: NO FEEDBACK ===\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "Extracting text...\n",
            "Texts length: 33499\n",
            "Chunking text...\n",
            "Chunks length: 42\n",
            "Creating embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings length: 42\n",
            "Create vector store:\n",
            "Added 42\n",
            "\n",
            "=== Processing query with feedback-enhanced RAG ===\n",
            "Query: What is a neural network and how does it function?\n",
            "Generating response...\n",
            "\n",
            "=== Response ===\n",
            "Based on the context provided, a neural network is a type of deep neural network, particularly effective for processing images and videos. It uses convolutional layers to automatically learn features from the input data. Neural networks are inspired by the structure and function of the human brain. They are widely used in various applications such as object detection, facial recognition, and medical image analysis.\n",
            "\n",
            "The context does not provide a comprehensive definition of a neural network, but it describes its function, which is to learn features from input data using convolutional layers. The explanation of convolutional neural networks (CNNs) in the context implies that a neural network is a type of CNN.\n",
            "\n",
            "=== ROUND 2: WITH FEEDBACK ===\n",
            "Extracting text...\n",
            "Texts length: 33499\n",
            "Chunking text...\n",
            "Chunks length: 42\n",
            "Creating embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings length: 42\n",
            "Create vector store:\n",
            "Added 42\n",
            "Fine-tuning index with high-quality feedback...\n",
            "Added enhanced content from feedback: What is a neural network and how does it function?...\n",
            "Fine-tuned index now has 43 items (original: 42)\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "\n",
            "=== Processing query with feedback-enhanced RAG ===\n",
            "Query: What is a neural network and how does it function?\n",
            "Adjusting relevance scores based on feedback history...\n",
            "Generating response...\n",
            "\n",
            "=== Response ===\n",
            "Based on the provided context, a neural network is a type of deep neural network, particularly effective for processing images and videos. It uses convolutional layers to automatically learn features from the input data. Neural networks are inspired by the structure and function of the human brain. They are widely used in various applications such as object detection, facial recognition, and medical image analysis.\n",
            "\n",
            "In more detail, a neural network functions by:\n",
            "\n",
            "1. Receiving input data, which can be images, videos, or sequential data.\n",
            "2. Applying convolutional layers to automatically learn features from the input data.\n",
            "3. These features are then used to make predictions or classify the input data.\n",
            "\n",
            "Neural networks are a type of deep learning model, which is a subfield of machine learning that uses artificial neural networks with multiple layers to analyze data. They are designed to mimic the structure and function of the human brain, and have achieved significant breakthroughs in areas such as image recognition, natural language processing, and speech recognition.\n",
            "\n",
            "=== COMPARING RESULTS ===\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "Analysis: After analyzing the responses, I can conclude that the feedback-enhanced RAG response is better than the standard RAG response in terms of relevance, accuracy, completeness, and clarity.\n",
            "\n",
            "**Relevance to the query:**\n",
            "The standard RAG response is relevant to the query, but it lacks a comprehensive definition of a neural network. The feedback-enhanced RAG response provides a more detailed explanation of what a neural network is, including its definition, function, and architecture.\n",
            "\n",
            "**Accuracy of information:**\n",
            "The standard RAG response contains some accurate information, but it is incomplete and lacks specificity. The feedback-enhanced RAG response provides a more accurate and detailed explanation of how neural networks function, including the process of adjusting weights between nodes.\n",
            "\n",
            "**Completeness:**\n",
            "The standard RAG response is incomplete, as it only provides a brief overview of neural networks without delving into the details of their function and architecture. The feedback-enhanced RAG response provides a more comprehensive explanation, covering the input process, convolutional layers, and the overall structure of neural networks.\n",
            "\n",
            "**Clarity and conciseness:**\n",
            "The standard RAG response is somewhat concise, but it is not entirely clear and concise. The feedback-enhanced RAG response is more concise and easier to understand, as it breaks down the explanation into clear steps and provides a more detailed description of the neural network's function.\n",
            "\n",
            "The feedback loop in the feedback-enhanced RAG response has improved the response quality in several ways:\n",
            "\n",
            "1. **Added specificity**: The feedback loop has provided more specific and detailed information about neural networks, making the response more accurate and informative.\n",
            "2. **Improved structure**: The feedback loop has helped to organize the response into clear steps, making it easier to understand the neural network's function and architecture.\n",
            "3. **Increased clarity**: The feedback loop has clarified the response, eliminating ambiguity and ensuring that the reader understands the concept of neural networks.\n",
            "4. **Enhanced relevance**: The feedback loop has ensured that the response is more relevant to the query, providing a comprehensive definition and explanation of neural networks.\n",
            "\n",
            "Overall, the feedback-enhanced RAG response is more informative, accurate, and clear than the standard RAG response, demonstrating the benefits of using a feedback loop to improve the quality of RAG responses....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# Run a full RAG workflow\n",
        "#######################################\n",
        "\n",
        "# Run an interactive example\n",
        "print(\"\\n\\n=== INTERACTIVE EXAMPLE ===\")\n",
        "print(\"Enter your query about AI:\")\n",
        "user_query = input()\n",
        "\n",
        "# Load accumulated feedback\n",
        "all_feedback = load_feedback_data()\n",
        "\n",
        "# Run full workflow\n",
        "result = full_rag_workflow(\n",
        "    pdf_path=pdf_path,\n",
        "    query=user_query,\n",
        "    feedback_data=all_feedback,\n",
        "    fine_tune=True\n",
        ")\n",
        "\n",
        "#######################################\n",
        "# Run a full RAG workflow\n",
        "#######################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmnrnrD8S9VB",
        "outputId": "3e212b50-3572-4213-cf82-814987882b37"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== INTERACTIVE EXAMPLE ===\n",
            "Enter your query about AI:\n",
            "How does AI work?\n",
            "No feedback data file found. Starting with empty feedback.\n",
            "Extracting text...\n",
            "Texts length: 33499\n",
            "Chunking text...\n",
            "Chunks length: 42\n",
            "Creating embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings length: 42\n",
            "Create vector store:\n",
            "Added 42\n",
            "\n",
            "=== Processing query with feedback-enhanced RAG ===\n",
            "Query: How does AI work?\n",
            "Generating response...\n",
            "\n",
            "=== Response ===\n",
            "Unfortunately, the context provided does not give a clear, comprehensive answer to the question of how AI works. However, based on the information given, it can be inferred that AI is a complex system that involves various intellectual processes characteristic of humans, such as reasoning, discovery, generalization, and learning from past experience.\n",
            "\n",
            "AI algorithms are able to perform tasks commonly associated with intelligent beings, such as creating original works of art, composing music, writing articles, and assisting in various industries. These systems learn from existing data and generate new pieces that exhibit unique styles and patterns.\n",
            "\n",
            "While the context does not provide a detailed explanation of the underlying mechanisms or processes that enable AI to function, it does mention that AI algorithms can analyze large datasets, identify patterns, and generate new ideas, which suggests that AI may involve some form of data processing and pattern recognition.\n",
            "\n",
            "However, without further information, it is difficult to provide a comprehensive and detailed explanation of how AI works. The context primarily focuses on the applications and potential of AI, rather than its underlying mechanisms or principles.\n",
            "\n",
            "=== Would you like to provide feedback on this response? ===\n",
            "Rate relevance (1-5, with 5 being most relevant):\n",
            "4\n",
            "Rate quality (1-5, with 5 being highest quality):\n",
            "5\n",
            "Any comments? (optional, press Enter to skip)\n",
            "\n",
            "Feedback recorded. Thank you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the comparison data which contains the analysis of feedback impact\n",
        "comparisons = evaluation_results['comparison']\n",
        "\n",
        "# Print out the analysis results to visualize feedback impact\n",
        "print(\"\\n=== FEEDBACK IMPACT ANALYSIS ===\\n\")\n",
        "for i, comparison in enumerate(comparisons):\n",
        "    print(f\"Query {i+1}: {comparison['query']}\")\n",
        "    print(f\"\\nAnalysis of feedback impact:\")\n",
        "    print(comparison['analysis'])\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Additionally, we can compare some metrics between rounds\n",
        "round_responses = [evaluation_results[f'round{round_num}_results'] for round_num in range(1, len(evaluation_results) - 1)]\n",
        "response_lengths = [[len(r[\"response\"]) for r in round] for round in round_responses]\n",
        "\n",
        "print(\"\\nResponse length comparison (proxy for completeness):\")\n",
        "avg_lengths = [sum(lengths) / len(lengths) for lengths in response_lengths]\n",
        "for round_num, avg_len in enumerate(avg_lengths, start=1):\n",
        "    print(f\"Round {round_num}: {avg_len:.1f} chars\")\n",
        "\n",
        "if len(avg_lengths) > 1:\n",
        "    changes = [(avg_lengths[i] - avg_lengths[i-1]) / avg_lengths[i-1] * 100 for i in range(1, len(avg_lengths))]\n",
        "    for round_num, change in enumerate(changes, start=2):\n",
        "        print(f\"Change from Round {round_num-1} to Round {round_num}: {change:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F28gnFbbgUSU",
        "outputId": "1f7603d8-98e9-4911-c745-1d816bcf31a3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FEEDBACK IMPACT ANALYSIS ===\n",
            "\n",
            "Query 1: What is a neural network and how does it function?\n",
            "\n",
            "Analysis of feedback impact:\n",
            "After analyzing the responses, I can conclude that the feedback-enhanced RAG response is better than the standard RAG response in terms of relevance, accuracy, completeness, and clarity.\n",
            "\n",
            "**Relevance to the query:**\n",
            "The standard RAG response is relevant to the query, but it lacks a comprehensive definition of a neural network. The feedback-enhanced RAG response provides a more detailed explanation of what a neural network is, including its definition, function, and architecture.\n",
            "\n",
            "**Accuracy of information:**\n",
            "The standard RAG response contains some accurate information, but it is incomplete and lacks specificity. The feedback-enhanced RAG response provides a more accurate and detailed explanation of how neural networks function, including the process of adjusting weights between nodes.\n",
            "\n",
            "**Completeness:**\n",
            "The standard RAG response is incomplete, as it only provides a brief overview of neural networks without delving into the details of their function and architecture. The feedback-enhanced RAG response provides a more comprehensive explanation, covering the input process, convolutional layers, and the overall structure of neural networks.\n",
            "\n",
            "**Clarity and conciseness:**\n",
            "The standard RAG response is somewhat concise, but it is not entirely clear and concise. The feedback-enhanced RAG response is more concise and easier to understand, as it breaks down the explanation into clear steps and provides a more detailed description of the neural network's function.\n",
            "\n",
            "The feedback loop in the feedback-enhanced RAG response has improved the response quality in several ways:\n",
            "\n",
            "1. **Added specificity**: The feedback loop has provided more specific and detailed information about neural networks, making the response more accurate and informative.\n",
            "2. **Improved structure**: The feedback loop has helped to organize the response into clear steps, making it easier to understand the neural network's function and architecture.\n",
            "3. **Increased clarity**: The feedback loop has clarified the response, eliminating ambiguity and ensuring that the reader understands the concept of neural networks.\n",
            "4. **Enhanced relevance**: The feedback loop has ensured that the response is more relevant to the query, providing a comprehensive definition and explanation of neural networks.\n",
            "\n",
            "Overall, the feedback-enhanced RAG response is more informative, accurate, and clear than the standard RAG response, demonstrating the benefits of using a feedback loop to improve the quality of RAG responses.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Response length comparison (proxy for completeness):\n",
            "Round 1: 717.0 chars\n"
          ]
        }
      ]
    }
  ]
}