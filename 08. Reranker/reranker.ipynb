{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0e735a686474440a9bc13d5fe8f2ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_876e25b6347b49c2b7d0e162c455a25d",
              "IPY_MODEL_e1bff66e8dba4381ac3bd9f24d47e979",
              "IPY_MODEL_328bd075f1114e63b74e4adbfab560b7"
            ],
            "layout": "IPY_MODEL_8311ca8492b646568076998bf6bd495e"
          }
        },
        "876e25b6347b49c2b7d0e162c455a25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3719cf51d43146aa94fe50c8d4ebe9fc",
            "placeholder": "​",
            "style": "IPY_MODEL_fe19fd079c7f4c239ce754758238ecc8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e1bff66e8dba4381ac3bd9f24d47e979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fae6e214744a63ab3c678b8a75a177",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de79ea52e03b4012a07110909906b537",
            "value": 2
          }
        },
        "328bd075f1114e63b74e4adbfab560b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_497d515fcce74ba697ba1fa8b44e8924",
            "placeholder": "​",
            "style": "IPY_MODEL_6bc811bd1cc84d05bc114aaa8aead055",
            "value": " 2/2 [00:01&lt;00:00,  1.12it/s]"
          }
        },
        "8311ca8492b646568076998bf6bd495e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3719cf51d43146aa94fe50c8d4ebe9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe19fd079c7f4c239ce754758238ecc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23fae6e214744a63ab3c678b8a75a177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de79ea52e03b4012a07110909906b537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "497d515fcce74ba697ba1fa8b44e8924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc811bd1cc84d05bc114aaa8aead055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ll93OKIGdgXm"
      },
      "outputs": [],
      "source": [
        "!pip install -q pymupdf\n",
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import tqdm\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "AZupqpAzdsQV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"unsloth/Llama-3.2-3B-Instruct\")\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "embed_model = AutoModel.from_pretrained(\"BAAI/bge-base-en\")\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "b0e735a686474440a9bc13d5fe8f2ae6",
            "876e25b6347b49c2b7d0e162c455a25d",
            "e1bff66e8dba4381ac3bd9f24d47e979",
            "328bd075f1114e63b74e4adbfab560b7",
            "8311ca8492b646568076998bf6bd495e",
            "3719cf51d43146aa94fe50c8d4ebe9fc",
            "fe19fd079c7f4c239ce754758238ecc8",
            "23fae6e214744a63ab3c678b8a75a177",
            "de79ea52e03b4012a07110909906b537",
            "497d515fcce74ba697ba1fa8b44e8924",
            "6bc811bd1cc84d05bc114aaa8aead055"
          ]
        },
        "id": "ZQ338_2r74hn",
        "outputId": "f10ac0c9-450c-479d-cdd8-b5f9bb75b7d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0e735a686474440a9bc13d5fe8f2ae6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = \"\"\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text(\"text\")\n",
        "\n",
        "  return text\n",
        "\n",
        "def chunk_text(text, n = 1000, overlap = 200):\n",
        "  return [text[i:i+n] for i in range(0, len(text), n-overlap)]"
      ],
      "metadata": {
        "id": "_CqvalVUeL7o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVectorStore:\n",
        "  def __init__(self):\n",
        "    self.vectors = []\n",
        "    self.texts = []\n",
        "    self.metadata = []\n",
        "\n",
        "  def add_item(self, text, embedding, metadata = None):\n",
        "    self.vectors.append(embedding)\n",
        "    self.texts.append(text)\n",
        "    self.metadata.append(metadata or {})\n",
        "\n",
        "  def similarity_search(self, query_embedding, k = 5):\n",
        "    if not self.vectors:\n",
        "      return []\n",
        "\n",
        "    similarities = [(i, np.dot(query_embedding, vector) / np.linalg.norm(query_embedding) * np.linalg.norm(vector)) for i, vector in enumerate(self.vectors)]\n",
        "\n",
        "    similarities.sort(key = lambda x:x[1], reverse = True)\n",
        "\n",
        "    results = []\n",
        "    for i in range(min(k, len(similarities))):\n",
        "      idx, score = similarities[i]\n",
        "      results.append({\n",
        "          \"text\": self.texts[idx],\n",
        "          \"metadata\": self.metadata[idx],\n",
        "          \"similarity\": score\n",
        "      })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "4aDmyagae9CG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model.to(device)\n",
        "def embed(text):\n",
        "    is_single = isinstance(text, str)\n",
        "    if is_single:\n",
        "        text = [text]\n",
        "\n",
        "    inputs = embed_tokenizer(\n",
        "        text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = embed_model(**inputs)\n",
        "        cls_emb = output.last_hidden_state[:, 0, :]\n",
        "        emb_normalized = F.normalize(cls_emb, p=2, dim=1)\n",
        "\n",
        "    embeddings = emb_normalized.cpu().numpy()\n",
        "\n",
        "    return embeddings[0] if is_single else embeddings # (dim,) with str (n, dim) with list"
      ],
      "metadata": {
        "id": "1mHlvxmjhWRL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(pdf_path, chunk_size = 1000, chunk_overlap = 200):\n",
        "  print(\"Extracting text from PDF...\")\n",
        "  text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "  print(\"Chunking text...\")\n",
        "  text_chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
        "  print(f\"{len(text_chunks)} text chunks has been created\")\n",
        "\n",
        "  print(\"Create embeddings...\")\n",
        "  embeddings = [embed(chunk) for chunk in tqdm.tqdm(text_chunks)]\n",
        "\n",
        "  print(\"Creating vector store...\")\n",
        "  store = SimpleVectorStore()\n",
        "\n",
        "  for i, (chunk, embedding) in enumerate(zip(text_chunks, embeddings)):\n",
        "    store.add_item(\n",
        "        text = chunk,\n",
        "        embedding = embedding,\n",
        "        metadata = {\n",
        "            \"index\": i,\n",
        "            \"source\": pdf_path\n",
        "            }\n",
        "        )\n",
        "  return store"
      ],
      "metadata": {
        "id": "RSy38E56qzIb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(system_prompt, user_prompt): # work with unsloth/Llama-3.2-3B-Instruct\n",
        "    text = gen_tokenizer.apply_chat_template(\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            }\n",
        "        ],\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = False\n",
        "    )\n",
        "\n",
        "    model_inputs = gen_tokenizer([text], return_tensors = \"pt\").to(device)\n",
        "\n",
        "    generated_ids = gen_model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample = True\n",
        "    )\n",
        "\n",
        "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "\n",
        "    response =  gen_tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0].strip(\"assistant\\n\\n\")\n",
        "\n",
        "    # print(\"===========================================\")\n",
        "    # print(f\"resposne: \\n{response}\")\n",
        "    # print(\"===========================================\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "RSHbaf-z4y4W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_with_llm(query, results, top_n=3, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Reranks search results using LLM relevance scoring.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Initial search results\n",
        "        top_n (int): Number of results to return after reranking\n",
        "        model (str): Model to use for scoring\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Reranked results\n",
        "    \"\"\"\n",
        "    print(f\"Reranking {len(results)} documents...\")  # Print the number of documents to be reranked\n",
        "\n",
        "    scored_results = []  # Initialize an empty list to store scored results\n",
        "\n",
        "    # Define the system prompt for the LLM\n",
        "    system_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\n",
        "Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n",
        "\n",
        "Guidelines:\n",
        "- Score 0-2: Document is completely irrelevant\n",
        "- Score 3-5: Document has some relevant information but doesn't directly answer the query\n",
        "- Score 6-8: Document is relevant and partially answers the query\n",
        "- Score 9-10: Document is highly relevant and directly answers the query\n",
        "\n",
        "You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"\n",
        "\n",
        "    # Iterate through each result\n",
        "    for i, result in enumerate(results):\n",
        "        # Show progress every 5 documents\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Scoring document {i+1}/{len(results)}...\")\n",
        "\n",
        "        # Define the user prompt for the LLM\n",
        "        user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "Document:\n",
        "{result['text']}\n",
        "\n",
        "Rate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n",
        "\n",
        "        # # Get the LLM response\n",
        "        # response = client.chat.completions.create(\n",
        "        #     model=model,\n",
        "        #     temperature=0,\n",
        "        #     messages=[\n",
        "        #         {\"role\": \"system\", \"content\": system_prompt},\n",
        "        #         {\"role\": \"user\", \"content\": user_prompt}\n",
        "        #     ]\n",
        "        # )\n",
        "\n",
        "        # Extract the score from the LLM response\n",
        "        score_text = gen(system_prompt, user_prompt)\n",
        "\n",
        "        # Use regex to extract the numerical score\n",
        "        score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n",
        "        if score_match:\n",
        "            score = float(score_match.group(1))\n",
        "        else:\n",
        "            # If score extraction fails, use similarity score as fallback\n",
        "            print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n",
        "            score = result[\"similarity\"] * 10\n",
        "\n",
        "        # Append the scored result to the list\n",
        "        scored_results.append({\n",
        "            \"text\": result[\"text\"],\n",
        "            \"metadata\": result[\"metadata\"],\n",
        "            \"similarity\": result[\"similarity\"],\n",
        "            \"relevance_score\": score\n",
        "        })\n",
        "\n",
        "    # Sort results by relevance score in descending order\n",
        "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Return the top_n results\n",
        "    return reranked_results[:top_n]\n"
      ],
      "metadata": {
        "id": "Ed03Ik3e4qIs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_with_keywords(query, results, top_n=3):\n",
        "    \"\"\"\n",
        "    A simple alternative reranking method based on keyword matching and position.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        results (List[Dict]): Initial search results\n",
        "        top_n (int): Number of results to return after reranking\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Reranked results\n",
        "    \"\"\"\n",
        "    # Extract important keywords from the query\n",
        "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
        "\n",
        "    scored_results = []  # Initialize a list to store scored results\n",
        "\n",
        "    for result in results:\n",
        "        document_text = result[\"text\"].lower()  # Convert document text to lowercase\n",
        "\n",
        "        # Base score starts with vector similarity\n",
        "        base_score = result[\"similarity\"] * 0.5\n",
        "\n",
        "        # Initialize keyword score\n",
        "        keyword_score = 0\n",
        "        for keyword in keywords:\n",
        "            if keyword in document_text:\n",
        "                # Add points for each keyword found\n",
        "                keyword_score += 0.1\n",
        "\n",
        "                # Add more points if keyword appears near the beginning\n",
        "                first_position = document_text.find(keyword)\n",
        "                if first_position < len(document_text) / 4:  # In the first quarter of the text\n",
        "                    keyword_score += 0.1\n",
        "\n",
        "                # Add points for keyword frequency\n",
        "                frequency = document_text.count(keyword)\n",
        "                keyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n",
        "\n",
        "        # Calculate the final score by combining base score and keyword score\n",
        "        final_score = base_score + keyword_score\n",
        "\n",
        "        # Append the scored result to the list\n",
        "        scored_results.append({\n",
        "            \"text\": result[\"text\"],\n",
        "            \"metadata\": result[\"metadata\"],\n",
        "            \"similarity\": result[\"similarity\"],\n",
        "            \"relevance_score\": final_score\n",
        "        })\n",
        "\n",
        "    # Sort results by final relevance score in descending order\n",
        "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Return the top_n results\n",
        "    return reranked_results[:top_n]\n"
      ],
      "metadata": {
        "id": "xmCIihVV6Gq-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query, context, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Generates a response based on the query and context.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        context (str): Retrieved context\n",
        "        model (str): Model to use for response generation\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response\n",
        "    \"\"\"\n",
        "    # Define the system prompt to guide the AI's behavior\n",
        "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
        "\n",
        "    # Create the user prompt by combining the context and query\n",
        "    user_prompt = f\"\"\"\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a comprehensive answer based only on the context above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the response using the specified model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Return the generated response content\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "HJME6B1j6InJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"unsloth/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline incorporating reranking.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (SimpleVectorStore): Vector store\n",
        "        reranking_method (str): Method for reranking ('llm' or 'keywords')\n",
        "        top_n (int): Number of results to return after reranking\n",
        "        model (str): Model for response generation\n",
        "\n",
        "    Returns:\n",
        "        Dict: Results including query, context, and response\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = embed(query)\n",
        "\n",
        "    # Initial retrieval (get more than we need for reranking)\n",
        "    initial_results = vector_store.similarity_search(query_embedding, k=10)\n",
        "\n",
        "    # Apply reranking\n",
        "    if reranking_method == \"llm\":\n",
        "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n",
        "    elif reranking_method == \"keywords\":\n",
        "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
        "    else:\n",
        "        # No reranking, just use top results from initial retrieval\n",
        "        reranked_results = initial_results[:top_n]\n",
        "\n",
        "    # Combine context from reranked results\n",
        "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "\n",
        "    # Generate response based on context\n",
        "    response = generate_response(query, context, model)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"reranking_method\": reranking_method,\n",
        "        \"initial_results\": initial_results[:top_n],\n",
        "        \"reranked_results\": reranked_results,\n",
        "        \"context\": context,\n",
        "        \"response\": response\n",
        "    }\n"
      ],
      "metadata": {
        "id": "qXrsdkrX6brr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation data from a JSON file\n",
        "with open('val.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the first query from the validation data\n",
        "query = data[0]['question']\n",
        "\n",
        "# Extract the reference answer from the validation data\n",
        "reference_answer = data[0]['ideal_answer']\n",
        "\n",
        "# pdf_path\n",
        "pdf_path = \"AI_Information.pdf\""
      ],
      "metadata": {
        "id": "Po2nXL7i6mB2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process document\n",
        "vector_store = process_document(pdf_path)\n",
        "\n",
        "# Example query\n",
        "query = \"Does AI have the potential to transform the way we live and work?\"\n",
        "\n",
        "# Compare different methods\n",
        "print(\"Comparing retrieval methods...\")\n",
        "\n",
        "# 1. Standard retrieval (no reranking)\n",
        "print(\"\\n=== STANDARD RETRIEVAL ===\")\n",
        "standard_results = rag_with_reranking(query, vector_store, reranking_method=\"none\")\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"\\nResponse:\\n{standard_results['response']}\")\n",
        "\n",
        "# 2. LLM-based reranking\n",
        "print(\"\\n=== LLM-BASED RERANKING ===\")\n",
        "llm_results = rag_with_reranking(query, vector_store, reranking_method=\"llm\")\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"\\nResponse:\\n{llm_results['response']}\")\n",
        "\n",
        "# 3. Keyword-based reranking\n",
        "print(\"\\n=== KEYWORD-BASED RERANKING ===\")\n",
        "keyword_results = rag_with_reranking(query, vector_store, reranking_method=\"keywords\")\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"\\nResponse:\\n{keyword_results['response']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWbg9a8J6rW-",
        "outputId": "9a435f54-0ce0-48e1-d7eb-eb1995cc1522"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "42 text chunks has been created\n",
            "Create embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 53.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector store...\n",
            "Comparing retrieval methods...\n",
            "\n",
            "=== STANDARD RETRIEVAL ===\n",
            "\n",
            "Query: Does AI have the potential to transform the way we live and work?\n",
            "\n",
            "Response:\n",
            "Yes, AI has the potential to transform the way we live and work. According to the context, AI is increasingly being used to address various aspects of our lives, including social and environmental challenges, healthcare, finance, transportation, retail, manufacturing, and the future of work.\n",
            "\n",
            "In various industries, AI is enhancing trust and accountability, improving productivity, and augmenting human capabilities. It is used for applications such as medical diagnosis, personalized medicine, and robotic surgery, which can lead to breakthroughs in healthcare. In finance, AI is used for fraud detection, algorithmic trading, and risk management, enabling more efficient financial processes.\n",
            "\n",
            "Moreover, AI is revolutionizing transportation with self-driving cars and autonomous vehicles, and transforming the retail industry with personalized recommendations, inventory management, and customer service chatbots. In manufacturing, AI is being used to optimize production, predict market movements, and automate financial processes.\n",
            "\n",
            "However, the increasing capabilities of AI also raise concerns about job displacement, particularly in industries with repetitive or routine tasks. Nevertheless, the context suggests that AI also creates new opportunities and transforms existing roles, and that reskilling and upskilling initiatives are necessary to address the potential impacts of AI on the workforce.\n",
            "\n",
            "Overall, AI has the potential to significantly impact various aspects of our lives and the way we work, bringing about both benefits and challenges that need to be addressed.\n",
            "\n",
            "=== LLM-BASED RERANKING ===\n",
            "Reranking 10 documents...\n",
            "Scoring document 1/10...\n",
            "Scoring document 6/10...\n",
            "\n",
            "Query: Does AI have the potential to transform the way we live and work?\n",
            "\n",
            "Response:\n",
            "Based on the provided context, it appears that AI has the potential to significantly transform the way we live and work. The text highlights the various applications of AI across different industries, including healthcare, finance, transportation, retail, and manufacturing, which demonstrate its potential to automate tasks, provide insights, and support decision-making.\n",
            "\n",
            "The development and deployment of AI create new job roles, such as AI development, data science, AI ethics, and AI training, which require specialized skills and expertise. This suggests that AI is not only automating tasks but also creating new opportunities for employment.\n",
            "\n",
            "Moreover, the text acknowledges the potential risks of job displacement due to automation, but also emphasizes the need for reskilling and upskilling initiatives to equip workers with the skills needed to adapt to new roles and collaborate with AI systems.\n",
            "\n",
            "Overall, the context suggests that AI has the potential to transform the way we live and work, with both positive and negative consequences. While it may automate certain tasks and displace some jobs, it also creates new opportunities for employment, innovation, and growth.\n",
            "\n",
            "=== KEYWORD-BASED RERANKING ===\n",
            "\n",
            "Query: Does AI have the potential to transform the way we live and work?\n",
            "\n",
            "Response:\n",
            "Based on the provided context, the answer to the question \"Does AI have the potential to transform the way we live and work?\" is yes. \n",
            "\n",
            "The text highlights various aspects of AI's impact on society, including its integration with robotics, its potential to enhance productivity and efficiency, and its implications for job displacement and the future of work. It also touches on the importance of responsible development, transparency, explainability, and accountability in AI systems.\n",
            "\n",
            "The text mentions that AI has the potential to transform industries such as manufacturing, healthcare, logistics, and exploration, and that it can be used in various applications, including manufacturing, healthcare, logistics, and customer service. Additionally, it notes that AI can augment human capabilities, automate mundane tasks, and provide insights that support decision-making, indicating its potential to transform the way we live and work.\n",
            "\n",
            "However, the text also acknowledges the challenges and concerns associated with AI, such as job displacement, the need for reskilling and upskilling initiatives, and the importance of establishing clear guidelines and ethical frameworks for AI development and deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n",
        "    \"\"\"\n",
        "    Evaluates the quality of reranked results compared to standard results.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        standard_results (Dict): Results from standard retrieval\n",
        "        reranked_results (Dict): Results from reranked retrieval\n",
        "        reference_answer (str, optional): Reference answer for comparison\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation output\n",
        "    \"\"\"\n",
        "    # Define the system prompt for the AI evaluator\n",
        "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
        "    Compare the retrieved contexts and responses from two different retrieval methods.\n",
        "    Assess which one provides better context and a more accurate, comprehensive answer.\"\"\"\n",
        "\n",
        "    # Prepare the comparison text with truncated contexts and responses\n",
        "    comparison_text = f\"\"\"Query: {query}\n",
        "\n",
        "Standard Retrieval Context:\n",
        "{standard_results['context'][:1000]}... [truncated]\n",
        "\n",
        "Standard Retrieval Answer:\n",
        "{standard_results['response']}\n",
        "\n",
        "Reranked Retrieval Context:\n",
        "{reranked_results['context'][:1000]}... [truncated]\n",
        "\n",
        "Reranked Retrieval Answer:\n",
        "{reranked_results['response']}\"\"\"\n",
        "\n",
        "    # If a reference answer is provided, include it in the comparison text\n",
        "    if reference_answer:\n",
        "        comparison_text += f\"\"\"\n",
        "\n",
        "Reference Answer:\n",
        "{reference_answer}\"\"\"\n",
        "\n",
        "    # Create the user prompt for the AI evaluator\n",
        "    user_prompt = f\"\"\"\n",
        "{comparison_text}\n",
        "\n",
        "Please evaluate which retrieval method provided:\n",
        "1. More relevant context\n",
        "2. More accurate answer\n",
        "3. More comprehensive answer\n",
        "4. Better overall performance\n",
        "\n",
        "Provide a detailed analysis with specific examples.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate the evaluation response using the specified model\n",
        "    response = gen(system_prompt, user_prompt)\n",
        "\n",
        "    # Return the evaluation output\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "rWz20Y1t6t80"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the quality of reranked results compared to standard results\n",
        "evaluation = evaluate_reranking(\n",
        "    query=query,  # The user query\n",
        "    standard_results=standard_results,  # Results from standard retrieval\n",
        "    reranked_results=llm_results,  # Results from LLM-based reranking\n",
        "    reference_answer=reference_answer  # Reference answer for comparison\n",
        ")\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "print(evaluation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x-pogRS7IS1",
        "outputId": "6b851b8c-2d6a-4367-ec02-80809b4cb0dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "**Evaluation Criteria:**\n",
            "\n",
            "1. Relevance of context: How well does the context provide information relevant to the query?\n",
            "2. Accuracy of answer: How accurate is the answer in relation to the query?\n",
            "3. Comprehensive answer: Does the answer provide a thorough and detailed explanation?\n",
            "4. Overall performance: How well does the retrieval method perform in providing a coherent and relevant response?\n",
            "\n",
            "**Evaluation:**\n",
            "\n",
            "1. **Relevance of context:**\n",
            "   - Standard Retrieval Context: The context provides a clear overview of various applications of AI, including its potential to transform the way we live and work. It covers topics such as AI at the edge, quantum computing and AI, human-AI collaboration, and AI for social good. The context is relevant to the query and provides a broad understanding of AI's potential impact.\n",
            "   - Reranked Retrieval Context: The context is more focused on the development and deployment of AI, job roles, and ethical considerations. While it touches on AI's potential to transform the way we live and work, it does not provide as comprehensive an overview as the standard context.\n",
            "   - Reference Answer: The context is limited to explaining Explainable AI (XAI) and does not address the query directly.\n",
            "\n",
            "2. **Accuracy of answer:**\n",
            "   - Standard Retrieval Answer: The answer provides a detailed and accurate explanation of AI's potential to transform the way we live and work. It covers various applications, including healthcare, finance, transportation, and manufacturing, and acknowledges the potential risks and benefits of AI.\n",
            "   - Reranked Retrieval Answer: The answer is more concise and focused on the development and deployment of AI, job roles, and ethical considerations. While it provides some relevant information, it lacks the breadth and depth of the standard retrieval answer.\n",
            "   - Reference Answer: The answer is limited to explaining Explainable AI (XAI) and does not address the query directly.\n",
            "\n",
            "3. **Comprehensive answer:**\n",
            "   - Standard Retrieval Answer: The answer provides a thorough and detailed explanation of AI's potential to transform the way we live and work, covering various applications, benefits, and risks.\n",
            "   - Reranked Retrieval Answer: The answer is more concise but does not provide as comprehensive an overview as the standard retrieval answer.\n",
            "   - Reference Answer: The answer is limited to explaining Explainable AI (XAI) and does not address the query directly.\n",
            "\n",
            "4. **Overall performance:**\n",
            "   - Standard Retrieval Answer: The standard retrieval answer provides the most comprehensive and accurate response to the query, covering various aspects of AI's potential impact on our lives and work.\n",
            "   - Reranked Retrieval Answer: The reranked retrieval answer is less comprehensive and less accurate than the standard retrieval answer, but still provides some relevant information.\n",
            "   - Reference Answer: The reference answer is the least relevant and least accurate, as it does not address the query directly.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Based on the evaluation, the standard retrieval context provides the most relevant context, the most accurate answer, and the most comprehensive answer. The standard retrieval answer provides a thorough and detailed explanation of AI's potential to transform the way we live and work, covering various applications, benefits, and risks. The reranked retrieval answer is less comprehensive and less accurate, while the reference answer is the least relevant and least accurate.\n"
          ]
        }
      ]
    }
  ]
}